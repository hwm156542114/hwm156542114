{"meta":{"title":"MrHou","subtitle":"Northern Lights","description":"Nothing is impossible��Nerver give up ~~~","author":"Mr Hou","url":"http://hwm156542114.github.io","root":"/"},"pages":[{"title":"about","date":"2021-11-08T08:29:44.000Z","updated":"2021-11-09T03:02:59.067Z","comments":true,"path":"about/index.html","permalink":"http://hwm156542114.github.io/about/index.html","excerpt":"","text":"关于我是谁，双枪会给出答案"},{"title":"categories","date":"2021-11-09T03:00:07.000Z","updated":"2021-11-09T03:01:46.150Z","comments":true,"path":"categories/index.html","permalink":"http://hwm156542114.github.io/categories/index.html","excerpt":"","text":"我的分类"},{"title":"心灵树洞","date":"2021-11-11T03:12:45.000Z","updated":"2021-11-12T02:03:43.895Z","comments":true,"path":"bb/index.html","permalink":"http://hwm156542114.github.io/bb/index.html","excerpt":"","text":"世界那么精彩，为什么不多去看看呢？"},{"title":"contact","date":"2021-11-09T03:03:16.000Z","updated":"2021-11-11T02:56:38.351Z","comments":true,"path":"contact/index.html","permalink":"http://hwm156542114.github.io/contact/index.html","excerpt":"","text":""},{"title":"说说","date":"2021-11-08T08:29:44.000Z","updated":"2021-11-11T02:37:33.026Z","comments":true,"path":"artitalk/index.html","permalink":"http://hwm156542114.github.io/artitalk/index.html","excerpt":"","text":"new Artitalk({ appId: 'AHod8M40BdclaIDo61HCYn45-MdYXbMMI', // Your LeanCloud appId appKey: 'qJmw2dizG6syutR6g07N5Yy7', // Your LeanCloud appKey bgImg: 'https://gitee.com/cungudafa/source/raw/master/img/gif/Sitich/Sitich16.gif', color1: '#BBDEFB', color2: '#7bed9f', color3: '#ff6348' })"},{"title":"navigate","date":"2021-11-12T03:36:46.000Z","updated":"2021-11-12T06:04:52.751Z","comments":true,"path":"navigate/index.html","permalink":"http://hwm156542114.github.io/navigate/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-11-09T03:06:04.000Z","updated":"2021-11-09T03:06:48.763Z","comments":true,"path":"friends/index.html","permalink":"http://hwm156542114.github.io/friends/index.html","excerpt":"","text":"友情链接"},{"title":"tags","date":"2021-11-09T03:02:06.000Z","updated":"2021-11-09T03:02:39.585Z","comments":true,"path":"tags/index.html","permalink":"http://hwm156542114.github.io/tags/index.html","excerpt":"","text":"我的标签"},{"title":"galleries","date":"2021-11-12T02:18:36.000Z","updated":"2021-11-12T02:19:24.020Z","comments":true,"path":"galleries/index.html","permalink":"http://hwm156542114.github.io/galleries/index.html","excerpt":"","text":""},{"title":"hwm","date":"2021-11-12T02:51:50.000Z","updated":"2021-11-12T03:10:28.197Z","comments":true,"path":"galleries/hwm/index.html","permalink":"http://hwm156542114.github.io/galleries/hwm/index.html","excerpt":"","text":""},{"title":"banner","date":"2021-11-12T02:51:50.000Z","updated":"2021-11-12T03:19:13.342Z","comments":true,"path":"galleries/banner/index.html","permalink":"http://hwm156542114.github.io/galleries/banner/index.html","excerpt":"","text":""}],"posts":[{"title":"Redis","slug":"Redis","date":"2021-11-30T12:48:26.000Z","updated":"2021-12-08T02:10:59.285Z","comments":true,"path":"2021/11/30/redis/","link":"","permalink":"http://hwm156542114.github.io/2021/11/30/redis/","excerpt":"","text":"RedisRedis为什么快？​ Redis的数据是存储在内存上的，访问数据的时候可以直接访问内存即可，不需要通过cpu访问磁盘。因此可以知道redis的性能问题不在CPU上。 Redis为什么使用单线程？ redis中的单线程是针对网络IO模块和读写模块，除了这两个模块外，其他的持久化和集群模块都是多线程的 单线程就没有多线程那样的上下文切换带来的性能开销 在单线程中使用了多路复用I/O技术 Redis为什么不使用多线程？多线程的目的是什么？ ​ 通过并发的方式提供I/O利用率和CPU利用率 思考：既然知道了多线程的目的，那么思考为什么不用呢？ redis的数据是存储在内存上的，所以不需要通过CPU访问磁盘，redis的性能瓶颈不在cpu上，所以多线程提高CPU的利用率就不符合了 redis确实是一个I/O操作密集型的框架，要提升redis的性能肯定是要从IO进行下手的，但是提升IO的利用率不一定非要使用多线程，因为使用多线程会带来线程安全问题，还有线程切换带来的性能消耗。 Redis选择多路复用IO技术","categories":[{"name":"redis","slug":"redis","permalink":"http://hwm156542114.github.io/categories/redis/"}],"tags":[{"name":"redis简单介绍","slug":"redis简单介绍","permalink":"http://hwm156542114.github.io/tags/redis%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"}]},{"title":"每日算法","slug":"每日算法","date":"2021-11-26T13:51:07.000Z","updated":"2021-12-09T03:25:27.003Z","comments":true,"path":"2021/11/26/mei-ri-suan-fa/","link":"","permalink":"http://hwm156542114.github.io/2021/11/26/mei-ri-suan-fa/","excerpt":"","text":"Leetcode两数相加(2021-11-26)class Solution { public int[] twoSum(int[] nums, int target) { //使用hashmap进行两数相加 /** 思想：主要就是map里面的核心方法containsKey，这个方法 判断这个键是否存在，如果存在，那么就通过map的get方法获 取值也就是下标，返回，否则就将数据存到map中，进行下一次 查询判断 */ HashMap&lt;Integer,Integer&gt; map = new HashMap&lt;Integer,Integer&gt;(); for(int i =0;i&lt;nums.length;i++){ if(map.containsKey(target - nums[i])){ return new int[]{map.get(target-nums[i]),i}; } map.put(nums[i],i); } return new int[0]; } } 整数反转(2021-11-27)class Solution { public int reverse(int x) { /** 可以通过字符串进行，但是如果考虑性能问题 还是选择对这个整数进行取余的操作 思路：用取余的方式可以得到尾数，然后将这个 尾数记录，如果下一次再次进入循环，那么将尾 数进行移位并且加上取余的数 */ //标志位,long类型很重要，最后判断越界异常 long n =0; while(x !=0){ //将每次得到的尾数进行记录 n = n*10 +x %10; //每次都会减少一个尾数 x = x/10; } //判断是否有越界 return (int) n == n?(int)n:0; } } 两数相加(2021-11-29)/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ /** 核心思想： 首先判断两个链表是否为空以及是否有进位标志，如果有那么就需要进行增加节点操作， 增加节点首先判断两个链表当前是否有值，如果有则使用当前值，如果没有则使用0当做 当前值，然后对当前两个链表的值相加，如果有进位那么将进位也一起想加，得到的 值先进行除以10的操作，主要就是判断当前所得的值是否大于10，如果大于则记录当前 进位，如果不大于则为0，然后将这个值进行取余操作，存入到链表中，然后更新链表 指针，进行下一轮扫描执行操作 */ class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { //新建链表 ListNode l3 = new ListNode(0); //建立一个指针操作链表 ListNode root = l3; //进位标志 int c = 0; while(l1!=null||l2!=null||c!=0){ //记录当前两个链表的值 int l1val = l1!=null?l1.val:0; int l2val = l2!=null?l2.val:0; //相加(注意添加进位标志) int sumval = l1val + l2val + c; //重新计算进位标志 c = sumval/10; //将sumval添加到新建的链表中 root.next = new ListNode(sumval%10); root = root.next; //然后判断链表是否为空，进行下一次循环 if(l1!=null) l1 = l1.next; if(l2!=null) l2 = l2.next; } //返回结果 return l3.next; } } 无重复字符串的最长子串（2021-12-02）class Solution { public int lengthOfLongestSubstring(String s) { /** 核心思想：使用hashmap来解决此问题 主要是将字符串存到map中，在扫描的过程中判断每次存的字符串是否重复， 如果重复就将上一次出现这个值得位置+1，这样就能从新的位置开始扫描， */ //创建一个新数组 HashMap&lt;Character,Integer&gt; map = new HashMap&lt;&gt;(); //初始化最大长度 int max = 0; //初始化开始扫描的位置 int start = 0; //通过循环将数据添加进map中，并且进行扫描 for(int i = 0;i&lt;s.length();i++){ //将插入的值记录 char c = s.charAt(i); //首先判断map中是否存在要插入的值 if(map.containsKey(c)){ //如果存在，那么就需要更新开始扫描的位置 //将开始位置更新为当前重复元素的上一次出现位置的下一个位置 start = Math.max(start,map.get(c)+1); } //更新max的值 max = Math.max(max,i-start+1); //将数据添加进map中，注意这里如果key重复了，那么就会用最新的值覆盖之前的值 map.put(c,i); } return max; } } 用两个栈来实现队列（2021-12-8）class CQueue { //初始化两个栈 Stack&lt;Integer&gt; stack1; Stack&lt;Integer&gt; stack2; public CQueue() { stack1 = new Stack&lt;&gt;(); stack2 = new Stack&lt;&gt;(); } /** 阐述一下思想： 使用两个栈实现队列，首先要知道的是栈是先进后出，而队列是先进先出的 其次就是队列是队头移除，队尾插入 如何实现？ 在入队的时候，就是普通的入栈操作，但是出队就需要将栈的顺序反过来， 意思就是比如现在添加了几个元素1，2，3，4，5，在栈中的顺序是5，4，3，2，1 现在进行队列的删除操作，也就是将对头的1进行移除，那么此时就需要 借助第二个栈，将栈1的数据都弹出存到栈2，这样顺序就变成了1，2，3，4，5 移除操作就是删除栈2的栈顶元素 */ public void appendTail(int value) { stack1.push(value); } public int deleteHead() { /** 删除元素首先得判断栈2是否有元素，如果有就直接弹出 如果没有就需要将栈1的数据压到栈2，如果栈1的数据是 空,就返回-1，意思就是没有数据 */ if(!stack2.isEmpty()){ return stack2.pop(); } while(!stack1.isEmpty()){ stack2.push(stack1.pop()); } return stack2.isEmpty()?-1:stack2.pop(); } } /** * Your CQueue object will be instantiated and called as such: * CQueue obj = new CQueue(); * obj.appendTail(value); * int param_2 = obj.deleteHead(); */ 包含min函数的栈(2021-12-8)class MinStack { /** 思路： 因为栈的底层数据结构可以是数组，也可以是链表，或者是其他的 这里因为有最小函数的出现，所以采用链表比较好，因为在链表中 可以开辟空间存储头部节点以及最小值节点 */ //初始化一个头结点 private Node head; /** initialize your data structure here. */ public MinStack() { } public void push(int x) { //判断头结点是否为空，如果为空就进行初始化 if(head==null){ head = new Node(x,x,null); }else{ //不为空，那么就需要比较大小，记录最小值 head = new Node(x,Math.min(x,head.min),head); } } public void pop() { head = head.next; } public int top() { return head.val; } public int min() { return head.min; } } //定义节点信息，包含最小值 class Node{ int val; Node next; int min; public Node(int val,int min,Node next){ this.val = val; this.next = next; this.min = min; } } /** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(x); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.min(); */ 从尾到头打印链表/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ /** 思路： 1. 本来自己做的话想的是使用栈，然后取出用数组存储，但是这样 就使用了额外的内存 2. 借鉴思想，反正都是要循环两次解决问题，为什么要创建额外的 空间存储呢？直接使用倒序插入数组就好了 */ class Solution { public int[] reversePrint(ListNode head) { ListNode index = head; //记录链表的长度，以便创建新的数组长度来存储链表节点 int size = 0; while(index!=null){ //遍历链表，记录链表长度 size++; index = index.next; } int [] array = new int[size]; for(int i =size-1;i&gt;=0;i--){ array[i] = head.val; head = head.next; } return array; } }","categories":[{"name":"算法","slug":"算法","permalink":"http://hwm156542114.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"每日算法","slug":"每日算法","permalink":"http://hwm156542114.github.io/tags/%E6%AF%8F%E6%97%A5%E7%AE%97%E6%B3%95/"}]},{"title":"AQS源码分析","slug":"AQS源码分析","date":"2021-11-17T01:38:57.000Z","updated":"2021-12-08T02:07:45.207Z","comments":true,"path":"2021/11/17/aqs-yuan-ma-fen-xi/","link":"","permalink":"http://hwm156542114.github.io/2021/11/17/aqs-yuan-ma-fen-xi/","excerpt":"","text":"首先看一下继承结构 通过上面的图，可以知道，其实除了公共功能，右边所有的类都是对AQS的一个实现，所以了解AQS是我们了解这些方法的基础 结合源码进行工作流程分析首先为了不让我们在看源码的过程中晕头转向，首先得把握住AQS的核心点： 占锁-&gt;失败存储线程-&gt;阻塞存储线程-&gt;唤醒 首先进入到ReentrantLock中的lock方法 public void lock() { //这里是通过sync进行实现的 sync.lock(); } 继续进入lock中，会发现有公平锁和非公平锁的实现，这里选择非公平锁进行查看 这里有lock方法的主要实现 final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } //CAS内部 protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 分析：这里的方法主要是通过CAS判断state是否为0，如果为0，那么就将state更新为1，然后设置这个线程为占锁的线程，如果不为0，那么即占锁失败，这时候就会执行**acquire(1);**这个方法。 补充：CAS里面主要是通过unsafe类进行一个CAS的操作，因为unsafe是一个可以直接操作内存数据的方法，绕过了JMM内存可见性。 进入到acquire(1);方法中 public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 分析：这个方法主要做了三个事情 再次尝试获取锁：tryAcquire(arg) 加入到阻塞队列：acquireQueued(addWaiter(Node.EXCLUSIVE), arg) 自己中断：selfInterrupt() 下面我们来逐个分析这三个方法到底是怎么实现的 首先进入tryAcquire(arg)方法中 protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } //非公平锁对tryAcquire的实现方法 final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false; } 分析：通过tryAcquire(arg)方法可以看到，首先获取state的值，如果state的值为0，说明之前的线程已经释放锁，那么就通过CAS尝试占锁，成功则返回。这里进入第二个判断条件，如果state不等于0，但是当前线程和占锁的线程是同一个线程，那么就证明这是一个可重入的操作（ReentrantLock是一个可重入锁），那么就对state的值+1，完成可重入操作，并且返回。最后如果都不符合，那么就返回false，表示再次获取锁失败。 进入到acquireQueued(addWaiter(Node.EXCLUSIVE), arg)方法中 首先来看一下addWaiter(Node.EXCLUSIVE)这个方法是干嘛的 private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node; } 分析：这个方法主要就是将当前线程封装成一个节点，然后判断尾节点是否为空，如果不为空，那么就将当前节点通过CAS加入到队尾，然后返回。如果为空，就证明这个队列还是一个空队列，就会进行一个空队列的入队操作enq(node)，然后返回。 进入到enq方法中看看 private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 分析：这个方法主要通过无限循环进行入列操作，如果队列为空，那么就通过CAS设置为头结点，并且尾节点也指向头节点；如果不为空就通过CAS把节点加到队尾。 了解了addWaiter这个方法回到acquireQueued这个方法中 final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 分析：通过无限循环，检查当前节点的前面一个节点是否为头结点，如果是就尝试获取锁，获取成功之后就将当前节点设置为头结点，并且返回false表示当前节点无需阻塞shouldParkAfterFailedAcquire方法就是在阻塞当前线程之前，会检查当前队列中阻塞线程状态是否正常，如果正常那么就通过parkAndCheckInterrupt进行阻塞，并且返回true 进入到shouldParkAfterFailedAcquire(p, node)方法中 /** waitStatus value to indicate thread has cancelled */ static final int CANCELLED = 1; /** waitStatus value to indicate successor's thread needs unparking */ static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ static final int PROPAGATE = -3; private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 分析：这个方法主要就是判断当前节点的前面节点的状态，如果前面节点的状态是SIGNAL那么就说明前面节点是正常的，可以添加到当前节点的后面；如果前面节点的状态大于0，意思就是前面的节点已经放弃等待了，那么继续往前找节点，直到找到为不大于0状态的节点，然后将这个节点的状态设置成-1；如果都不满足，就返回false。 进入到unlock方法中 public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 分析：进入到unlock方法中，会执行一个release的方法，这个方法首先执行的是tryRelease尝试解锁，如果释放锁成功，那么就通过unparkSuccessor方法进行唤醒阻塞队列中的线程。 进入到TryRelease方法中 protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } 分析：首先获取state值减去release初始值，判断当前线程时候与获取到锁的线程一致，不一致抛出异常，一致就判断c的值是否为0，如果为0表示可以释放锁，那么就将占锁线程置为null，设置state的值，返回。 进入到unparkSuccessor(h)方法中 private void unparkSuccessor(Node node) { int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread); } 分析：首先判断头节点的状态，如果小于0那么就通过CAS将头结点的状态设为0。获取头节点的下一个节点，如果为空或者状态大于0，那么从队列的末尾往前遍历，找到一个节点的状态&lt;= 0的，然后记录这个节点，如果不为空，就通过unpark进行唤醒。 总结大致的流程就是：首先一个线程进入，如果此时共享资源没有被占用，那么就通过CAS将state设置为1，也就是将共享资源变为占用状态，然后执行对应的操作，最后释放锁。如果此时有线程已经占用了，那么这个线程就会进入阻塞队列，不过进入阻塞队列之前还会再次尝试获取，如果也获取失败了，那么检查阻塞队列中线程的状态，找到线程的状态为SIGNAL，然后添加到这个节点的后面。现在节点进入到了阻塞队列中，如果占锁线程执行完毕调用unlock方法，那么会通过tryrelease方法先进行释放锁操作，释放完成之后，会调用unparkSuccessor里面的LockSupport.unpark方法，进行下一个线程的唤醒。这时候进入到acquireQueued这个方法中的parkAndCheckInterrupt这个方法，检测到当前线程不是阻塞状态，那么就会通过自旋一直尝试获取锁。","categories":[{"name":"Java","slug":"Java","permalink":"http://hwm156542114.github.io/categories/Java/"}],"tags":[{"name":"AQS源码","slug":"AQS源码","permalink":"http://hwm156542114.github.io/tags/AQS%E6%BA%90%E7%A0%81/"}]},{"title":"Spring学习之路","slug":"Spring学习之路","date":"2021-11-16T13:25:56.000Z","updated":"2021-12-08T02:09:01.187Z","comments":true,"path":"2021/11/16/spring-xue-xi-zhi-lu/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/spring-xue-xi-zhi-lu/","excerpt":"","text":"Spring安装与应用 安装 直接下载https://repo.spring.io/release/org/springframework/spring/，点击我们想要下载的版本，进去之后下载第一个dist.zip 详细下载步骤： 首先我们要进入spring.io界面，然后找到spring Framework，然后点击右上角的GitHub，之后我们进入页面下滑到Access To Binaries ，那里有个查看spring文件，然后我们点击进去之后，点击小房子下面的文件，然后找到release-&gt;org-&gt;springframework-&gt;spring 点击右边会出现一个链接release/org/springframework/spring/，将这个链接追加到io后面就可以访问到下载页面 应用 我们首先创建一个简单的java类，然后引入jar包，然后加入库，然后创建对象 &gt;package Test; &gt;import Java.hellow; &gt;import org.junit.Test; &gt;import org.springframework.context.ApplicationContext; &gt;import org.springframework.context.support.ClassPathXmlApplicationContext; &gt;public class hellowTest { &gt;@Test &gt;public void Hellow(){ //加载spring配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(\"xml1.xml\"); //获取配置对象文件 hellow hellow = context.getBean(\"hellow\", hellow.class); System.out.println(hellow); hellow.Hello(); &gt;} &gt;} &gt;package Java; &gt;public class hellow { &gt;public void Hello(){ System.out.println(\"你好spring\"); &gt;} &gt;} IOC IOC底层原理 xml配置 工厂模式 反射 通过这三层，来系统的降低我们的耦合度 IOC接口 IOC思想基于IOC容器完成的，IOC容器底层就是对象工厂 spring提供IOC容器实现两种方式（两个接口） BeanFactory：IOC基本实现，是spring内部的使用接口，不提供开发人员进行使用（加载配置文件的时候不会创建对象，只有在获取对象的时候才去创建对象） ApplicationContext：BeanFactory的子接口，拥有更多更强大的功能，一般由开发人员进行使用（加载配置文件的时候就会将配置文件对象创建出来） ApplicationContext接口有实现类 FileSystemXmlApplicationContext ClassPathXmlApplicationContext IOC中基于xml依赖注入set方法的注入public class book{ private String name; private String author; public void setName(String name){ this.name=name; } public void setAuthor(String author){ this.author=author; } } &lt;Bean id = \"book\" class=\"Java.book\"&gt; &lt;properties name=\"name\" value=\"123\"&gt;&lt;/properties&gt; &lt;properties name=\"author\" value=\"456\"&gt;&lt;/properties&gt; &lt;/Bean&gt; public class test{ //加载配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(\"book.xml\"); //获取配置对象 Book book = context.getBean(\"book\",Book.class); } 构造方法的注入默认的是无参构造，所以我们需要用构造器重新构造这个对象 &lt;bean id=\"book\" class=\"Java.book\"&gt; &lt;constructor-arg name=\"name\" value=\"123\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=\"author\" value=\"456\"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 特殊字符的注入&lt;bean id=\"book\" class=\"Java.book\"&gt; &lt;property name=\"author\"&gt; &lt;null&gt;&lt;/null&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"book\" class=\"Java.book\"&gt; &lt;property name=\"author\"&gt; &lt;value&gt; &lt;![CDATA[&lt;&lt;南京&gt;&gt;]]]]&gt; &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt;s 对象的注入package dao; public class Userimpl implements User{ @Override public void add() { System.out.println(\"add...........\"); } } package service; public class UserserviceImpl implements Userservice { private User user; public void setUser(User user) { this.user = user; } @Override public void update() { System.out.println(\"update...........\"); user.add(); } } &lt;bean id=\"Userimpl\" class=\"dao.Userimpl\"&gt;&lt;/bean&gt; &lt;bean id=\"UserserviceImpl\" class=\"service.UserserviceImpl\"&gt; &lt;property name=\"user\" ref=\"Userimpl\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; //这里的name是我们在类中创建的名字 //这里的ref是我们bean标签的id值 级联属性的设置emp package Java; public class emp { private String ename; private Dept dept; public void setEname(String ename) { this.ename = ename; } public void setDept(Dept dept) { this.dept = dept; } public Dept getDept() { return dept; } public void sout(){ System.out.println(ename+\" \"+dept); } } Dept package Java; public class Dept { private String dname; public void setDname(String dname) { this.dname = dname; } @Override public String toString() { return \"Dept{\" + \"dname='\" + dname + '\\'' + '}'; } } Test public class DeptTest { @Test public void Test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"xml3.xml\"); emp emp = context.getBean(\"emp\", emp.class); emp.sout(); } &lt;bean id=\"dept\" class=\"Java.Dept\"&gt; &lt;property name=\"dname\" value=\"财务部\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"emp\" class=\"Java.emp\"&gt; &lt;!--普通属性设置--&gt; &lt;property name=\"ename\" value=\"123\"&gt;&lt;/property&gt; &lt;!--对象属性设置--&gt; &lt;property name=\"dept\" ref=\"dept\"&gt;&lt;/property&gt; &lt;!-- &lt;property name=\"dept\"&gt;--&gt; &lt;!-- &lt;bean id=\"dept2\" class=\"Java.Dept\"&gt;--&gt; &lt;!-- &lt;property name=\"dname\" value=\"456\"&gt;&lt;/property&gt;--&gt; &lt;!-- &lt;/bean&gt;--&gt; &lt;!-- &lt;/property&gt;--&gt; //这里要使用dept.dname必须要求有get方法 &lt;!-- &lt;property name=\"dept.dname\" ref=\"dept\"&gt;&lt;/property&gt;--&gt; &lt;/bean&gt; 数组、集合、map、set属性的注入package Java; import java.sql.Array; import java.util.Arrays; import java.util.List; import java.util.Map; import java.util.Set; public class array { private String [] course; private List&lt;String&gt; list; private Map&lt;String,String&gt; map; private Set&lt;String&gt; set; public void setCourse(String[] course) { this.course = course; } public void setList(List&lt;String&gt; list) { this.list = list; } public void setMap(Map&lt;String, String&gt; map) { this.map = map; } public void setSet(Set&lt;String&gt; set) { this.set = set; } public void sout(){ System.out.println(Arrays.toString(course)); System.out.println(list); System.out.println(map); System.out.println(set); } } public class arrayTest { @Test public void Test(){ ApplicationContext context = new ClassPathXmlApplicationContext(\"xml4.xml\"); array array = context.getBean(\"array\", array.class); array.sout(); } &lt;bean id=\"array\" class=\"Java.array\"&gt; &lt;property name=\"course\"&gt; &lt;array&gt; &lt;value&gt;123&lt;/value&gt; &lt;value&gt;456&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name=\"list\"&gt; &lt;list&gt; &lt;value&gt;list&lt;/value&gt; &lt;value&gt;list&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=\"map\"&gt; &lt;map&gt; &lt;entry key=\"JAVA\" value=\"java\"&gt;&lt;/entry&gt; &lt;entry key=\"PHP\" value=\"php\"&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=\"set\"&gt; &lt;set&gt; &lt;value&gt;set&lt;/value&gt; &lt;value&gt;abc&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;/bean&gt; 基于XML的自动装配&lt;!--基于XML的自动装配注入--&gt; &lt;!--XML有两种自动装配注入的方法 第一种：通过名称注入 ByName 第二种：通过类型注入 ByType --&gt; &lt;bean id=\"dept\" class=\"Java.Dept\"&gt;&lt;/bean&gt; &lt;bean id=\"emp\" class=\"Java.Dept\" autowire=\"byName\"&gt;&lt;/bean&gt; &lt;bean id=\"emp\" class=\"Java.Dept\" autowire=\"byType\"&gt;&lt;/bean&gt; IOC中基于注解的注入四种注解方式： @Component（一般用于bean） @Service（一般用于Service层） @Controller（一般用于web层） @Repository（一般用于dao层） 使用注解的方式： ​ 对象的创建： 1.首先需要引入aop的jar包 2.然后需要在xml文件中添加： xmlns:context=\"http://www.springframework.org/schema/context\" http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd 3.然后进行配置 &lt;!--基于注解的配置--&gt; &lt;context:component-scan base-package=\"service\"&gt;&lt;/context:component-scan&gt; 4.配置完之后，我们需要在相应的类上面添加@注解名称（在注解中，value值默认的是我们类名，首字母小写） 5.运行 ​ 属性的注入： @Autowired：根据类型进行注入 @Qualifier：根据名称进行注入 @Resource：可以类型注入，可以属性注入 @Value：普通注入 1.首先需要在两个类上面都添加对象创建的注解 2.然后再service中创建dao对象，不需要set方法 3.然后在dao对象上面添加属性注入的注解（ 使用@Autowired直接在属性上写 使用Qualifier这个注解，需要和@Autowired一起使用 使用Resource，直接添加到属性上就是根据类型注入，如果加上name属性就是根据名称进行注入 使用Value这个注解，在普通属性上添加，并且设置value值就是它的值） 4.（要注意我们的xml文件的配置中，一定要将我们需要自动装备的包都添加进去） 基于java类的自动装配@Configuration//作为配置类，替代xml配置文件 @ComponentScan(basePackages = \"Java\")//路径 public class Config { } 普通bean和工厂bean的区别 普通bean返回的类型只能是我们定义的类型 工厂bean返回的可以不是我们定义的类型 IOC的生命周期 执行无参构造，创建bean的实例 调用set方法设置属性值 在初始化之前执行的方法 执行初始化的方法 在初始化之后执行的方法 获取创建bean的实例对象 执行销毁方法 AOP概念：不通过修改源码的方式，在主干功能里面添加新的功能 底层原理：使用动态代理 有接口的情况下：创建接口实现类的代理对象，增强类的方法 没有接口的情况下：创建子类的代理对象，增强类的方法 AOP术语： 连接点：类里面那些方法被增强了 切入点：实际被增强的方法 通知（增强）：实际被增强的逻辑部分成为通知 1. 前置通知 2. 后置通知 3. 环绕通知 4. 异常通知 5. 最终通知 切面：动作，把通知应用到切入点的过程 AOP准备操作： 1. Spring框架一般都是基于AspectJ实现AOP操作的 1. AspectJ不是spring的组成部分，是独立的框架 基于AspectJ实现AOP操作 基于xml配置文件 基于注解方式 在项目中引入相关jar包 springsource.net.sf.cglib springsource.org.aopalliance springsource..org.aspectj.weaver aspectj.release 切入点表达式 作用：知道对哪个类里面的哪个方法进行增强 语法结构：execution(权限修饰符、返回类型、类全路径、方法名称（参数列表）) 例:execution(* com.dao.add.*) AOP操作： 基于注解方式开发 创建类： @Component public class Userimpl { public void add() { System.out.println(\"add...........\"); } } 创建增强类 @Component @Aspect public class UserProxy { //增强类 @Before(\"execution(* dao.Userimpl.add())\") public void before(){ System.out.println(\"before/........\"); } public void after(){ System.out.println(\"before/........\"); } } 创建配置文件 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;!--开启自动扫描 --&gt; &lt;context:component-scan base-package=\"dao\"&gt;&lt;/context:component-scan&gt; &lt;!--生成AspectJ对象 --&gt; &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; &lt;/beans&gt; 配置不同类型的通知 //增强类 //调用前 @Before(\"execution(* AspeectJ.User1.add())\") public void before(){ System.out.println(\"before/........\"); } //调用后 @After(\"execution(* AspeectJ.User1.add())\") public void after(){ System.out.println(\"after/........\"); } //返回值时 @AfterReturning(\"execution(* AspeectJ.User1.add())\") public void AfterReturning(){ System.out.println(\"AfterReturning/........\"); } //抛异常时 @AfterThrowing(\"execution(* AspeectJ.User1.add())\") public void AfterThrowing(){ System.out.println(\"AfterThrowing/........\"); } //环绕 @Around(\"execution(* AspeectJ.User1.add())\") public void Around(ProceedingJoinPoint joinPoint) throws Throwable { System.out.println(\"环绕前/........\"); joinPoint.proceed(); System.out.println(\"环绕后/........\"); } 方法抽取 //公共抽取 @Pointcut(value = \"execution(* AspeectJ.User1.add())\") public void Pointdemo(){ } @Before(value = \"Pointdemo()\") public void before(){ System.out.println(\"before/........\"); } //这里报错了，可能是因为jar包版本的错误，也可能是jdk版本的错误 设置优先级 在增强方法的类上面添加@Order（数字值越小，优先级越高） 基于xml配置文件开发 首先创建两个类，一个普通类，一个增强类 然后再写配置文件 &lt;!-- 配置类--&gt; &lt;bean id=\"user1\" class=\"AspeectJ.User1\"&gt;&lt;/bean&gt; &lt;bean id=\"userproxy\" class=\"AspeectJ.UserProxy\"&gt;&lt;/bean&gt; &lt;!--aop配置--&gt; &lt;aop:config&gt; &lt;!-- 配置切入点--&gt; &lt;aop:pointcut id=\"p\" expression=\"execution(* AspeectJ.User1.add())\"/&gt; &lt;!-- 配置切面--&gt; &lt;aop:aspect id=\"proxy\" ref=\"userproxy\"&gt; &lt;aop:before method=\"before\" pointcut-ref=\"p\"&gt;&lt;/aop:before&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; 完全开发，没有配置文件的开发 需要添加一个aop操作的类文件 package AspeectJ; @Configuration//变成配置类文件 @ComponentScan(basePackages = \"AspeectJ\")//组件扫描 @EnableAspectJAutoProxy(proxyTargetClass = true) public class AspectJConfig { } JDBCTemplate​ 概念：JDBCTemplate是spring对JDBC的封装，使用JDBCTemplate对数据库进行操作 ​ 准备工作： 引入相关依赖 . druid-1.0.9.jar 2. mysql-connector-java-5.1.6.jar 3. spring-jdbc-5.2.9.RELEASE.jar 4. spring-tx-5.2.9.RELEASE.jar 5. spring-orm-5.2.9.RELEASE.jar 2. 配置数据库连接池 &lt;bean id=\"datasource\" class=\"com.alibaba.druid.pool.DruidDataSource\" destroy-method=\"close\"&gt; &lt;property name=\"url\" value=\"jdbc:mysql:///user_db\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;/bean&gt; 3. 配置jdbc对象，注入DataSource &lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;!-- 注入DataSource--&gt; &lt;property name=\"dataSource\" ref=\"datasource\"&gt;&lt;/property&gt; &lt;/bean&gt; 4. 装配实现的dao以及service层 &lt;!--配置自动装配--&gt; &lt;context:component-scan base-package=\"service,dao\"&gt;&lt;/context:component-scan&gt; 总结：这里因为我的MySQL版本和驱动版本问题，不能进行数据库连接，无法进行下面的课程学习代码，但是JDBCTemplate在javaweb中都用过，所以这些都是复习性的过一遍 事务 什么是事务：事务是数据库操作的最基本单元，逻辑上一组操作，要么都成功，要么都失败 事务的四个特性（ACID） 原子性：操作过程不可分割，要么都成功，要么都失败 一致性：操作前和操作后总量是不变的 隔离性：多事务操作之间不会产生影响 持久性：提交数据后，会进行保存 事务操作 配置事务管理器 //加入tx命名空间 xmlns:tx=\"http://www.springframework.org/schema/tx\" http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd //创建事务操作 &lt;bean id=\"dataSourceTransactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!--注入DataSource--&gt; &lt;property name=\"dataSource\" ref=\"datasource\"&gt;&lt;/property&gt; &lt;/bean&gt; //开启事务注解 &lt;tx:annotation-driven transaction-manager=\"dataSourceTransactionManager\"&gt;&lt;/tx:annotation-driven&gt; 在类上或者方法上加注解@Transaction 事务管理参数配置 事务传播行为：7种 REQUIRED：如果一个方法本身有事务，在方法里调用另一个方法，那么就都使用这个事务 REQUIRED_NEW：一个方法调用另一个方法，不管有没有事务，都会创建一个新的事务 操作：@Transaction（propagation = propagation.REQUIRED 事务隔离性： 存在问题：脏读、不可重复读、虚读 脏读：一个未提交的事务读取到另一个未提交的事务 不可重复读：一个未提交的事务读取到另一个提交的事务 虚读：一个未提交的事务读取到另一个提交的添加事务 解决：通过隔离级别解决 read uncommited:读未提交 read commited:读已提交 repeatable read:可重复读 serializable:串行化 设置方式：@Transaction（isolation = isolation .serializable） 超时时间：timeout 事务需要在一定的时间内提交，如果不提交则回滚 默认值是-1，设置时间以秒为单位进行计算 readOnly：是否只读 默认设置为false 设置为只读，就不能进行增删改操作 rollbackFor：回滚 设置出现哪些异常进行事务回滚 morollbackFor:不回滚 设置哪些事务不回滚 spring5框架新功能 spring框架代码基于java8，运行时兼容jdk9，许多不建议使用的类和方法在代码库中删除 spring5框架自带了通用的日志封装 移除了log4j 整合了log4j2 使用： 引入jar包： log4j-api log4j-core log4j-slf4j slf4j-api 写xml配置文件 spring5框架核心容器支持@Nullable注解： @Nullable注解可以使用在方法、属性、参数上面，表示它们的返回值可以为空 spring5支持整合JUnit5 整合JUnit4 第一步引入spring-test依赖 创建测试类，使用注解方法 @RunWith（SpringJUnit4ClassRunner.class） @ContextConfiguration(\"classpath:bean.xml\") 整合JUnit5 ​ 第一步引入JUnit5的jar包 第二步创建测试类，使用注解完成 @ExtendWith(SpringExtension.calss) @ContextConfiguration(\"calsspath:bean.xml\") //改进之后 @SpringJUnitConfig(locations=\"classpath:bean.xml\") Spring5框架新功能：WebFlux springwenflux介绍 是spring5添加新的模块，用于web开发，功能和springMVC类似的，webflux适用当前一种比较流程响应式编程出现的框架 使用传统web框架，比如springMVC，这些基于servlet容器，webflux是一种一步非阻塞的框架，一步非阻塞的框架在servlet3.1以后才支持，核心是基于Reactor的相关API实现的 解释什么是异步非阻塞 异步和同步针对调用者：调用者发送请求，如果等着对方回应之后才去做其他事情就是同步，如果发送请求之后不等着对方回应就去做其他事情就是异步 阻塞和非阻塞对被调用者：被调用者收到请求之后，做完请求任务之后才给出反馈就是阻塞，受到请求之后马上给出反馈然后再去做事情就是非阻塞 Webflux特点： 非阻塞式：在有限的资源下，提高系统吞吐量和伸缩性，以reactor为基础实现响应式编程 函数式编程：spring5框架基于java8，webflux使用java8函数式编程方式实现路由请求 比较Springmvc 两个框架都可以使用注解方式，都运行在Tomcat等容器中 SpringMVC采用命令式编程，Webflux采用异步响应式编程 响应式编程（Reactor实现） 响应式编程操作中，Reactor是满足Reactive规范框架 Reactor有两个核心类，Mono和Flux，这两个类实现接口publiser，提供丰富操作符。flux对象实现发布者，返回N和元素；Mono实现发布者，返回0或者1个元素 flux和Mono都是数据流的发布者，使用Flux和Mono都可以发出三种数据信号：元素值、错误信号、成功信号，错误信号和完成信号都代表终止信号，终止信号用于告诉订阅者数据流结束了，错误信号终止数据流同时吧错误信息传递给订阅者 三种信号特点： 错误信号和完成信号都是终止信号，不能共存 如果没有发送任何元素值，而是直接发送错误或者完成信号，表示空数据流 如果没有错误信号，没有完成信号，表示是无线数据流 调用just或者其他方法只是声明了数据流，数据流并不会发出，只有订阅了才会发出，不订阅什么都不会发生 操作符 map：将元素映射成新元素 flatMap：将元素映射成流 将每个元素映射成流，然后将流都合并成一个大流 springwebflux执行流程和核心api springwebflux基于reactor，默认使用容器是Netty，Netty是高性能NIO框架，异步非阻塞框架 阻塞和非阻塞 BIO：阻塞 NIO：非阻塞 springwebflux执行过程和springMVC相似的 springwebflux核心控制器DispatchHandler，实现接口webHandler 接口WebHandler有一个方法 springwebflux里面DispatcherHandler，负责请求的处理： HandlerMapping：请求查询到处理的方法 HandlerAdapter：真正负责请求的处理 HandlerResultHandler：响应处理结果 springwebflux实现函数式变成，两个接口：RouterFunction（路由处理）和HandlerFunction（处理函数） 实践","categories":[{"name":"学Java","slug":"学Java","permalink":"http://hwm156542114.github.io/categories/%E5%AD%A6Java/"}],"tags":[{"name":"spring基础","slug":"spring基础","permalink":"http://hwm156542114.github.io/tags/spring%E5%9F%BA%E7%A1%80/"}]},{"title":"SpringCloud学习之路","slug":"SpringCloud学习之路","date":"2021-11-16T13:25:12.000Z","updated":"2021-12-08T02:08:41.565Z","comments":true,"path":"2021/11/16/springcloud-xue-xi-zhi-lu/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/springcloud-xue-xi-zhi-lu/","excerpt":"","text":"SpringCloud1.了解SpringCloud版本问题 首先Spring cloud的版本号是以英文来区分的，而Springboot是以数字来区分的 Spring Cloud的版本用英文来区分的原因是因为根据火车站名来定的 它们俩之间的版本约束 这里需要注意的是：并不是都用最新版的就是最好的，而是有一定的约束条件，根据上下图来选择最佳版本搭配使用 这里也有我们版本的最佳选择 升级引发的惨案 2.配置运行环境 首先搭建一个父工程的maven项目 然后只留一个pom.xml文件，并且引入以下信息 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;SpringCloudStudy&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;!--统一管理jar包版本--&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;lombok.version&gt;1.18.18&lt;/lombok.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;mysql.version&gt;8.0.18&lt;/mysql.version&gt; &lt;druid.version&gt;1.1.16&lt;/druid.version&gt; &lt;mybatis.spring.boot.version&gt;2.1.1&lt;/mybatis.spring.boot.version&gt; &lt;/properties&gt; &lt;!--子模块继承之后，提供作用：锁定版本+子module不用谢groupId和version--&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--spring boot 2.2.2--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--spring cloud Hoxton.SR1--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Hoxton.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--spring cloud 阿里巴巴--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mysql--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- druid--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;${druid.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--mybatis--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${mybatis.spring.boot.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--junit--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;${junit.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--log4j--&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;${log4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;${lombok.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!--bulid是这样的用springboot默认的build方式--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;addResources&gt;true&lt;/addResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 然后创建子工程的maven项目 子工程的pom.xml文件引入以下内容 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;SpringCloudStudy&lt;/artifactId&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;payment&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 子工程的resource目录下创建一个application.yml文件 server: port: 8001 spring: application: name: payment datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: org.gjt.mm.mysql.Driver url: jdbc:mysql://localhost:3306/user?serverTimezone=UTC username: root password: root mybatis: mapper-locations: classpath:mapper/*.xml type-aliases-package: springcloud.hwm.entity 建立数据库表 CREATE TABLE `payment` ( `id` BIGINT(20) not NULL auto_increment COMMENT 'ID', `serial` VARCHAR(200) DEFAULT'', primary key(`id`) )ENGINE=INNODB auto_increment=1 DEFAULT CHARSET=utf8; 编写service-dao-controller的一系列代码 3.Eureka 基础知识 什么是服务治理 ​ 在传统的RPC远程调用框架中，管理每个服务与服务之间依赖关系比较复杂，所以需要使用服务治理管理其依赖关系，并实现服务调用、负载均衡、容错等，实现服务注册与发现 什么是服务注册与发现 Eureka Server作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务使用Eureka客户端连接到Eureka Server并维持心跳连接。这样系统的维护人员就可以通过Eureka Server来监控系统中各个微服务是否正常运行。 在服务注册与发现中，有一个注册中心。当服务器启动的时候，会把自己服务器的信息注册到注册中心上，然后消费者、服务提供者就可以以别名的方式获取实际的服务通讯地址，然后再实现本地RPC调用远程RPC框架的思想：远程RPC框架中都会有一个注册中心存放服务地址相关信息 Eureka包含两个组件 Eureka Server：各个微服务节点通过配置启动后都再Eureka Server中进行注册 EurekaClient：是一个Java客户端，用于简化Eureka Server的交互。在应用启动后，会向Eureka Server发送心跳，Eureka Server通过心跳对其进行管理，如果在某个周期没有收到心跳，那么它就会将服务注册表中的这个服务节点移除 Eureka自我保护机制： 在某一时刻微服务不可用的时候（微服务本身是健康的，但是可能由于网络延迟等其他原因导致没能及时发送心跳数据包），Eureka不会立刻清理该微服务，而是对这个微服务的信息进行保存。 属于CAP里面的AP分支 Eureka的使用Eureka Server端的创建 首先得有得创建一个maven工程，这个工程作为我们的Eureka Server端 在pom.xml中添加下面的依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;!-这里就是导入Eureka Server的依赖 --&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;Common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写application.yml文件 # 设定端口号 server: port: 7001 eureka: instance: #设定服务端的主机名 hostname: eureka7001.com client: #是否注册自己 register-with-eureka: false fetch-registry: false #服务的URL service-url: defaultZone: http://eureka7003.com:7003/eureka/ 编写主启动类 @SpringBootApplication //开启Eureka Server的注解 @EnableEurekaServer public class EurekaMain { public static void main(String[] args) { SpringApplication.run(EurekaMain.class,args); } } 如果服务端创建多个，那么在yml文件中，最后的URL就是映射到其他服务端，有几个，就映射几个 Eureka Client端创建 在pom文件中添加下面依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; 编写yam文件 eureka: client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7003.com:7003/eureka instance: # 显示的是自定义的id地址 instance-id: payment8001 prefer-ip-address: true # 这里因为服务端不是一个，所以url不是一个 主启动类 @SpringBootApplication @EnableEurekaClient @EnableDiscoveryClient public class PaymentMain8001 { public static void main(String[] args) { SpringApplication.run(PaymentMain8001.class,args); } } 编写业务代码 这里注意：如果编写的是消费者的客户端，那么我们可以在自定义配置类中加下面得Bean对象 @Configuration public class SpringConfig { @Bean @LoadBalanced //实现负载均衡 public RestTemplate getRestTemplate(){ return new RestTemplate(); } } 然后在Controller中使用RestTemplate进行转发请求。此时，我们不需要知道请求的路径，因为此时我们的路径映射为 spring: application: name: order 请求直接写成http://order 4.Zookeeper的使用 首先要在Linux系统中，启动Zookeeper注册中心 在项目的pom文件中多添加一个依赖 然后编写yml文件 主启动类 在启动的时候，会报错，是因为Zookeeper依赖里面含有Zookeeper的旧版本，使之与我们在Linux系统安装的Zookeeper版本不匹配导致报错 解决办法： 思考：Zookeeper的服务节点是临时节点还是持久节点 ​ 答：是临时节点，它相比较于eureka，没有保护机制，而是在某一段时间内，如果客户端没有发送心跳数据包就直接删除。 5.Consul简介：​ Consul是一套开源的分布式服务发现和配置管理系统，又HashiCorp公司用Go语言开发 ​ 提供了微服务系统中的服务治理、配置中心、控制总线、等功能。功能可单独使用，也可以一起使用构建服务网络，总之Consul提供了一种完整的服务网络解决风格 ​ 优点：基于raft协议，简洁；支持健康检查，同时支持HTTP和DNS协议，支持跨数据中心的WAN集群 提供图形界面 跨平台支持Linux、Mac、Windows 功能 服务发现：提供HTTP和DNS两种发现方式 健康检测：支持多种方式：HTTP、TCP、Docker、Shell脚本定制化 KV存储：KEY Value的存储方式 多数据中心： Consul支持多数据中心 可视化Web界面 下载安装 先去Consul官网下载 完成之后打开压缩包，看到Consul.exe文件，在此打开cmd窗口 输入命令：consul agent -dev，运行成功 访问http://localhost:8500 使用 pom文件中加入以下依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt; &lt;/dependency&gt; 编写yml文件 server: port: 80 spring: application: name: consul-order cloud: consul: host: localhost port: 8500 discovery: service-name: ${spring.application.name} 主启动类 @SpringBootApplication @EnableDiscoveryClient public class Consul_order80 { public static void main(String[] args) { SpringApplication.run(Consul_order80.class,args); } } 编写业务代码，验证注册是否成功，以及是否能访问数据。 6.三大注册中心的异同点 7.Ribbon简介：​ ribbon是基于Netflix Ribbon实现的一套客户端负载均衡的工具 ​ Ribbon是Netflix发布的开源项目，主要功能是提供 客户端的软件负载均衡算法和服务调用 ​ 我们可以很容易的使用Ribbon实现自定义的负载均衡算法 负载均衡 负载均衡是什么？ 简单的说就是将用户的请求平摊的分配到多个服务器上，从而达到系统HA（高可用） 常见的负载均衡又软件Nginx、LVS、硬件F5 Ribbon本地负载均衡客户端 VS Nginx服务端负载均衡 Nginx是服务器负载均衡，客户端所有的请求都会交给nginx，然后由nginx实现转发请求。即负载均衡是由服务器实现的。 Ribbon本地负载均衡，在调用微服务接口的时候，会在注册中心上获取注册信息服务列表后缓存到JVM本地，从而在本地实现RPC远程服务调用技术 负载均衡分为两种：集中式LB、进程内LB 集中式LB：在服务的消费方和提供方之间使用独立的LB设施，由该设施负责把访问请求通过某种策略转发至服务的提供方 进程内LB：将LB逻辑集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器 Ribbon属于进程内LB，它只是一个类库，集成于消费方进程，消费方通过它来获取服务器提供方的地址。 Ribbon在工作时分为两步 第一步：先选择EurekaServer，它优先选择在同一个区域内负载较少的server 第二步：根据用户指定的策略，从server取到的服务注册列表中选择一个地址 Ribbon提供多种策略：轮询、随机、根据响应时间加权等。 Ribbon的使用 首先我们如果在项目中添加了 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; 那么我们就不需要再添加Ribbon的依赖，因为新版的Eureka依赖包中包含了Ribbon依赖，所以我们可以直接使用 实现负载均衡肯定是在集群的环境下进行的，所以我们在80消费者端口的Configuration的Rest Template上面加上@LoadBalancer注解 访问的地址直接填写spring application name 即可 Ribbon的负载均衡详解 负载均衡分为哪几种方式 负载均衡的算法： 负载均衡的接口与类实现 查看源码，首先进入到IRul接口中，然后CTRL+Alt+b显示其所有的实现类 RoundRobinRule就是轮询负载均衡的类 //这里是对服务地址的选择 public Server choose(ILoadBalancer lb, Object key) { if (lb == null) { //如果lb是空，会提示没有负载均衡 log.warn(\"no load balancer\"); return null; } else { Server server = null; int count = 0; while(true) { if (server == null &amp;&amp; count++ &lt; 10) { List&lt;Server&gt; reachableServers = lb.getReachableServers(); List&lt;Server&gt; allServers = lb.getAllServers(); int upCount = reachableServers.size();//这里是第几次请求数 int serverCount = allServers.size();//这里是服务总集群数 if (upCount != 0 &amp;&amp; serverCount != 0) { int nextServerIndex = this.incrementAndGetModulo(serverCount); server = (Server)allServers.get(nextServerIndex); if (server == null) { Thread.yield(); } else { if (server.isAlive() &amp;&amp; server.isReadyToServe()) { return server; } server = null; } continue; } log.warn(\"No up servers available from load balancer: \" + lb); return null; } if (count &gt;= 10) { log.warn(\"No available alive servers after 10 tries from load balancer: \" + lb); } return server; } } } private int incrementAndGetModulo(int modulo) { int current;//当前的服务端口号 int next;//下一个服务端口号 do { current = this.nextServerCyclicCounter.get(); next = (current + 1) % modulo; } while(!this.nextServerCyclicCounter.compareAndSet(current, next)); return next; } 手写Ribbon轮询负载均衡算法并使用过程图： 首先去掉在order工程，下面的Configuration类，里面自动注入RestTemplate的@LoadBalanced注解 编写LoadBalancer接口 @Component public interface LoadBalancer { //这里主要是获取所有的微服务端 ServiceInstance instances(List&lt;ServiceInstance&gt; serviceInstances); } 编写MyLB package springcloud.hwm.LB; @Component public class MyLB implements LoadBalancer { //必须有一个原子型整数,这里设置的初始值为0 private AtomicInteger atomicInteger = new AtomicInteger(0); public final int getAndIncreament() { int current;//当前的访问次数 int next;//下一次访问次数 do { current = this.atomicInteger.get();//当前的访问次数先从这个原子型整数这里获取； //2147483647这个数是整型的最大值，如果大于这个数就从0记起，如果不大于就等于当前访问次数+1 next = current &gt;= 2147483647 ? 0 : current + 1; }while (!this.atomicInteger.compareAndSet(current,next)); //这里的compareAndSet，使用的是CAS原理，CAS相比Synchronized，避免了锁的使用，总体性能比Synchronized高很多. //这里我们分析一下CAS的方法是如何进行使用的： //在这个循环条件下，假设有多个线程在同时执行这段代码，即使有一个线程在产生了新值之后， // 它还需要让它自己产生的新值与旧值比较之后才能决定要不要这个新值。也就是说，如果oldValue是10， // 而产生的新值是20，然后程序还没到while的时候，有其他线程修改了newValue值，那当这个线程到while判断的时候， // 会出现comparAndSet方法的预期值不跟实际值一样，导致方法返回false，直到没有其他线程干扰。这时就确定了新产生的值。 //那么根据以上的原理，只有当预期值和实际值相同的时候，方法返回的是True，那么这个时候while循环内是false，就会跳出循环 并且的到我们的预期值 return next; } @Override //这个就是获取服务端的所有信息,根据信息我们来返回我们想要的值 public ServiceInstance instances(List&lt;ServiceInstance&gt; serviceInstances) { //这里就是轮询算法的关键之处，也就是我们所说的 访问次数 % 服务集群数 = 实际调用服务的下标 int index = getAndIncreament() % serviceInstances.size(); //通过下标选择返回哪一个微服务端 return serviceInstances.get(index); } } 编写OrderController类 @GetMapping(\"/customer/payment/lb\") public String getLB(){ List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"payment\"); if (instances==null&amp;&amp;instances.size()&lt;=0){ return null; } ServiceInstance serviceInstance = loadBalancer.instances(instances); URI uri = serviceInstance.getUri(); log.info(\"当前的URL为+\"+uri+\"/payment/lb\"); return restTemplate.getForObject(uri+\"/payment/lb\",String.class); } 8.OpenFeign简介​ Feign是一个声明式的web Service客户端。使用Feign能让编写WebService客户端更加简单，它的使用方法是 定义一个服务接口，然后在上面添加注解。Feign也支持可拔插式的编码器和解码器。spring cloud对Feign进行了封装，使其支持了SpringMVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡 Feign能干什么​ Feign旨在使编写javaHttp客户端变得更容易 ​ 前面在使用Ribbon+RestTemplate时，利用Rest Template对http请求的封装处理，形成了一套模板化的调用方法。但是在实际开发过程中，由于对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以通常都会针对每个微服务自行封装一些客户端类来包装这些依赖服务的调用。所以，Feign在此基础上做了进一步封装，由它来帮助我们定义和实现依赖服务接口的定义。在Feign的实现下，我们只需要创建一个接口并使用注解的方式来配置它（以前是Dao接口上面标注Mapper注解，现在时一个微服务接口上面标注一个Feign注解即可），即可完成对服务提供方的接口绑定，简化了使用Spring cloud Ribbon时，自动封装服务调用客户端的开发量。 ​ Feign集成了Ribbon ​ 利用Ribbon维护了payment的服务列表信息，并且通过轮询实现了客户端的负载均衡。而与Ribbon不同的是，通过feign只需要定义服务绑定接口且以声明式的方法，优雅而简单的实现了服务调用 Feign与OpenFeign的区别 Feign的使用 创建一个消费者项目 编写pom文件 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;Common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写yam文件 server: port: 80 eureka: client: register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7003.com:7003/eureka 主启动类 @SpringBootApplication @EnableFeignClients //一定要添加这个注解 public class OrderFeignMain { public static void main(String[] args) { SpringApplication.run(OrderFeignMain.class,args); } } 编写service接口 @Component @FeignClient(\"payment\") public interface FeignService { @GetMapping(\"/payment/findById/{id}\") public CommonResult&lt;payment&gt; findById(@PathVariable(\"id\") long id); } 编写controller @RestController public class FeignController { @Resource private FeignService service; @GetMapping(\"/customer/findById/{id}\") public CommonResult&lt;payment&gt; findById(@PathVariable(\"id\") long id){ return service.findById(id); } } 注意：当报404错误的时候，一定要看好自己的地址是否正确的编写，细心检查（报错点） Feign的超时控制​ 项目启动，Feign客户端默认只等待1s，但是服务端需要超过1s，导致Feign客户端不想等待了，直接返回报错。为了避免这样的情况，有时候我们需要设置Feign客户端的超时控制。 ​ 案例： ​ 先假设超时： @GetMapping(\"/TimeOut\") public String paymentTimeOut(){ //这里设置延迟的时间为3秒 try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } return serverPort; } ​ 然后通过消费者去调用：会报错 ​ 这时候我需要通过yml进行配置 ​ 依然报错： ​ 原因：因为在消费者的pom文件中，导入了Eureka和Feign两个依赖包，他们下面都包含Ribbon依赖包，导致包冲突，并且不能直接通过Ribbon来设置成功 # 设置feign客户端超时时间 ribbon: #指的是建立连接所用的时间，适用于网络状况正常的情况下，两端连接所用的时间 ReadTimeOut: 5000 #指的是建立连接后从服务器读取到可用资源所用的时间 ConnectTimeOut: 5000 #这个是不成功的，即使启动也会报错 ​ 正确的修改方式： feign: client: config: default: #简历连接所用的时间，适用于网络状况正常的情况下，两端连接所需要的时间 ConnectTimeOut: 5000 #指建立连接后从服务端读取到可用资源所用的时间 ReadTimeOut: 10000 日志增强​ Feign提供了日志打印功能，可以通过配置来调整日志级别，从而了解Feign中Http请求的细节。说白了就是对Feign接口的调用情况进行监控和输出 ​ 使用： 开启日志增强功能： @Configuration public class Config { //开启详细日志 @Bean Logger.Level Log(){ return Logger.Level.FULL; } } 配置yml文件 logging: level: #feign日志以什么级别监控哪个接口 springcloud.hwm.service.FeignService: debug 日志级别： 9.Hystrix概念 Hystrix是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性 “断路器”本身是一种开关装置，当某个服务单元发生故障后，通过断路器的故障监控（类似于熔断保险丝），向调用方法返回一个符合预期的、可处理的备选响应（FallBack）,而不是长时间的等待或者抛出调用方无法处理的异常，这样就保证了服务调用方法的线程不会被长时间、不必要的占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。 服务降级： 服务器忙，请稍后再试，不让客户端等待并立刻返回一个友好提示，fallback 哪些情况会发出降级： 程序运行异常 超时 服务熔断触发服务降级 线程池、信号量打满也会导致服务降级 服务熔断： 类比保险丝，达到最大服务访问后直接拒绝访问，拉闸限电，然后调用服务降级方法并返回友好提示 服务的降级-&gt;进而熔断-&gt;恢复调用链路 服务限流： 秒杀高并发等操作，严禁一窝蜂的过来拥挤，排队有序进行 Hystrix的服务降级使用 首先创建一个消费者客户端（一般都是使用在消费者客户端） pom文件添加以下依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; yml文件的编写 server: port: 80 eureka: client: register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7003.com:7003/eureka # 这里设置之后，后面的超时测试老是报错 #原因：关键在于feign:hystrix:enabled: true的作用，官网解释“Feign将使用断路器包装所有方法”，也就是将@FeignClient标记的那个service接口下所有的方法进行了hystrix包装（类似于在这些方法上加了一个@HystrixCommand），这些方法会应用一个默认的超时时间为1s，所以你的service方法也有一个1s的超时时间，service1s就会报异常，controller立马进入备用方法，controller上那个3秒那超时时间就没有效果了。 feign: hystrix: enabled: true #解决： hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 3000 #然后ribbon的超时时间也需加上 ribbon: ReadTimeout: 5000 ConnectTimeout: 5000 主启动类的编写 @SpringBootApplication @EnableFeignClients @EnableHystrix public class FeignHystrixOrderMain { public static void main(String[] args) { SpringApplication.run(FeignHystrixOrderMain.class,args); } } service层的编写 @Service //这里的fallback就是利用一个实现类，完成所有方法的降级处理，这样可以程序耦合度降低，而不会把所有的降级方法都写在Servcie或者Controller类里面，造成代码膨胀且杂乱 @FeignClient(value = \"PAYMENT-HYSTRIX\",fallback = FeignHystrixServiceImpl.class) public interface FeignHystrixService { @GetMapping(\"/payment/hystrix/ok/{id}\") public String payment_ok(@PathVariable(\"id\")Integer id); @GetMapping(\"/payment/hystrix/TimeOut/{id}\") public String payment_TimeOut(@PathVariable(\"id\")Integer id); } Controller类的编写 @RestController @Slf4j //这个是定义全局的降级方法，前提是加了@HystrixCommand注解。 @DefaultProperties(defaultFallback = \"globolMethod\") public class FeignHystrixOrder80Controller { @Resource private FeignHystrixService feignHystrixService; @HystrixCommand @GetMapping(\"/customer/payment/hystrix/ok/{id}\") public String payment_ok(@PathVariable(\"id\")Integer id){ int a= 10/0; return feignHystrixService.payment_ok(id); } @GetMapping(\"/customer/payment/hystrix/TimeOut/{id}\") //这个是特定的降级方法 @HystrixCommand(fallbackMethod = \"TimeOutHandler\",commandProperties = { @HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\",value = \"5000\") }) public String payment_TimeOut(@PathVariable(\"id\")Integer id){ return feignHystrixService.payment_TimeOut(id); } public String TimeOutHandler(Integer id){ return \"系统繁忙，请稍后再试!!!!\"+id; } public String globolMethod(){ return \"系统出错\"; } 服务熔断​ 熔断机制概述： ​ 熔断机制是应对雪崩效应的一种微服务链路的保护机制。当扇出链路的某个微服务出错不可用或者响应时间太长，会进行服务降级，进而熔断该节点微服务的调用，快速返回错误响应信息，当检测到该节点微服务调用响应正常后，恢复调用链路 ​ 在Spring Cloud框架中，熔断机制通过Hystrix实现，Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5s内20次调用失败，就会启动熔断机制。熔断机制的注解@HystrixCommand。 熔断过程中的三大状态： 熔断的三个重要参数： 熔断过程： 熔断最后的恢复机制： Hystrix图形化监控​ 除了隔离依赖服务的调用以外，Hystrix还提供了准实时的调用监控，Hystrix会持续的继续所有通过Hystrix发起的请求的执行信息，并以统计报表和图形的形式展示给用户，包括每秒执行多少请求，多少失败。Netflix通过hystrix-metrics-event-steam项目实现了对以上指标的监控。Spring cloud也提供了HystrixDashboard的整合，对监控内容转化成可视化界面 ​ 如何使用： 首先，添加依赖 &lt;!-- 导入图形监控的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 这两个依赖必须存在于客户端--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; yml文件 server: port: 9001 主启动类 @SpringBootApplication @EnableHystrixDashboard public class DashboardMain { public static void main(String[] args) { SpringApplication.run(DashboardMain.class,args); } } 这里在客户端的启动类里面添加 如果不添加网页会报：Unable to connect to Command Metric Stream. /** * 此配置是为了服务监控而配置的，与服务容错本身无关，spring cloud升级后的坑 * Servlet RegistrationBean 因为springboot的默认路径不是/hystrix.stream， * 只要在自己的项目里配置下面得servlet就可以了 */ @Bean public ServletRegistrationBean getServlet() { HystrixMetricsStreamServlet streamServlet = new HystrixMetricsStreamServlet(); ServletRegistrationBean registrationBean = new ServletRegistrationBean(streamServlet); registrationBean.setLoadOnStartup(1); registrationBean.addUrlMappings(\"/hystrix.stream\"); registrationBean.setName(\"HystrixMetricsStreamServlet\"); return registrationBean; } 10.GateWay简介： SpringCloud GateWay是Spring Cloud的一个全新项目，基于Spring5.0+SpringBoot2.0和Project Reactor等技术开发的网关，它旨在为微服务架构提供一种简单有效的同意的API路由管理方式 SpringCloud GateWay作为SpringCloud生态系统中的网关，目标是替代Zuul，在SpringCloud2.0以上版本中，没有对新版本的Zuul2.0以上最新高性能版本进行集成，仍然还是使用Zuul1.x非Reactor模式的老版本。而为了提升网关的性能，SpringCloud Gateway是基于web Flux框架实现的，而web Flux框架底层则使用了高性能的Reactor模式通信框架Netty。 SpringCloud GateWay的目标提供统一的路由方式且基于Filter链的方式提供了网关基本的功能 为什么选择GateWay Netflix不太靠谱，zuul2.0一直跳票，迟迟不发布 一方面因为Zuul1.0已经进入了维护阶段，而且Gateway是SpringCloud团队研发的，是亲儿子产品，值得信赖。而且很多功能比起Zuul是非常简单便捷的 GatWay是基于异步非阻塞模型上进行开发的，性能方面不需要担心，虽然Netflix早就发布了最新的Zuul2.x，但Spring cloud貌似没有整合计划。而且Netflix相关组件都宣布维护期，不知前景如何 SpringCloud GateWay具有如下特性 基于Spring5.0+SpringBoot2.0和Project Reactor进行构建 动态路由：能够匹配任何请求属性 可以对路由指定Predicate（断言）和Filter（过滤器） 集成Hystrix的断路器功能 集成Spring cloud服务发现功能 易于编写的Predicate和Filter 请求限流功能 支持路径重写 SpringCloud GateWay于Zuul的区别 Zuul1.x是一个基于阻塞IO的API Gateway Zuul 1.x基于Servlet2.5使用阻塞架构它不支持任何长连接（如web Socket）Zuul的设计模式和Nginx较像，每次IO操作都是从工作线程中选择一个执行，请求线程被阻塞到工作线程完成，但是差别是Nginx用C++实现，Zuul用Java实现，而JVM本身会有第一次加载较慢的情况，使得Zuul的性能相对较差 Zuul2.x理念更先进，想基于Netty非阻塞和支持长连接，但Spring cloud目前还没有整合。Zuul2.x的性能较Zuul1.x有较大提升。在性能方面，根据官方提供的基准测试，SpringCloud GateWay的RPS（每秒请求数）是Zuul的1.6倍 SpringCloud GateWay建立在Spring5.0+SpringBoot2.0和Project Reactor之上，使用非阻塞API SpringCloud GateWay还支持web Socket，并且与Spring紧密集成拥有更好的开发体验 Zuul模型 Spring cloud中所集成的zuul版本，采用的是Tomcat容器，使用的是传统的Servlet IO处理模型 servlet由servlet container进行生命周期管理 container启动时构建servlet对象并调用servlet init进行初始化 container运行时，接受请求，并为每个请求分配一个线程（一般从线程池中获取空闲线程）然后调用Service container关闭时调用destory销毁 上述模型的缺点 ​ servlet是一个简单的网络IO模型，当请求进入servlet container时，就会为其绑定一个线程，在并发不高的场景下这种模型是适用的，但是一旦高并发（比如抽风用jemeter压），线程数量就会上涨，而线程资源代价是昂贵的（上下文切换，内存消耗大）严重影响请求的处理时间。在一些简单业务场景下，不希望为每个request分配一个线程，只需要1个或者几个线程就能应对极大并发的请求，这种业务场景下servlet模型没有优势 ​ 所以zuul1.x是基于servlet之上的一个阻塞式处理模型，即spring实现了处理所有request请求的一个servlet并由该servlet阻塞式处理，所以Zuul无法摆脱servlet模型的弊端 Gateway的三个核心概念 路由：路由时构建网关的基本模块，它由ID，目标URI，一系列的断言和过滤器组成，如果断言为true则匹配该路由 断言：开发人员可以匹配HTTP请求中的所有内容（请求头，请求参数），如果请求与断言相匹配则进行路由 过滤：指的是Spring框架中GatewayFilter的实例，使用过滤器，可以在请求被路由前或者之后对请求进行修改 客户端向Spring cloudGateWay发出请求，然后再Mapping中找到与请求相匹配的路由，将其发送到GateWayWebHandler Handler再通过指定的过滤器链将请求发送到实际的服务执行业务逻辑，然后返回 过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前或者之后执行业务逻辑 Filter在”pre”类型的过滤器可以做参数校验、权限校验、流量监控、日志输出、协议转换等， 在”post”类型的过滤器中可以做响应内容、响应头的修改、日志的输出，流量监控等有着非常重要的作用 Gateway的使用 创建一个项目 pom文件导入 &lt;dependencies&gt; &lt;!-- 首先得说明以下，必须把web和actuator这两个依赖除去---&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;Common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; yml文件 server: port: 9527 spring: application: name: gateway cloud: gateway: discovery: locator: enabled: true #开启从注册中心动态创建路由的功能，利用微服务名进行路由 routes: # 这下面不仅可以配置断言，还可以配置filter - id: payment_route #路由的id，没有固定规则但要求唯一，建议配合服务名 # uri: http://localhost:8001 uri: lb://PAYMENT predicates: - Path=/payment/findById/** #断言，路径相匹配的进行路由 - id: payment_route2 uri: lb://PAYMENT # 匹配后提供服务的路由 predicates: - Path=/payment/lb/** eureka: client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7003.com:7003/eureka instance: hostname: gateway-service 主启动类 @SpringBootApplication @EnableEurekaClient public class GatwayMain { public static void main(String[] args) { SpringApplication.run(GatwayMain.class,args); } } 配置gateway路由的第二种方法 @Configuration public class GatewayConfig { @Bean public RouteLocator customerRouteLocator2(RouteLocatorBuilder builder){ RouteLocatorBuilder.Builder routes =builder.routes(); routes.route(\"route_payment3\",r-&gt;r.path(\"/guoji\").uri(\"http://news.baidu.com/guoji\")).build(); return routes.build(); } } 配置全局Filter @Component @Slf4j public class MyFilter implements GlobalFilter, Ordered { @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { log.info(\"我的过滤器\"+ new Date()); String uname = exchange.getRequest().getQueryParams().getFirst(\"uname\"); if (uname==null){ log.info(\"用户名为null，非法用户\"); exchange.getResponse().setStatusCode(HttpStatus.NOT_ACCEPTABLE); return exchange.getResponse().setComplete(); } return chain.filter(exchange); } @Override public int getOrder() { return 0; } } 11.spring cloud Config 配置总控中心是什么？​ spring cloud config 为微服务框架中的微服务提供集中化的外部配置支持，配置服务器为各个不同微服务应用的所有环境提供了一个 中心化的外部配置 怎么用？​ spring cloud config分为 服务端和客户端两部分 ​ 服务端也称为 分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并为客户端提供获取配置信息，加密/解密信息等访问接口 ​ 客户端则是通过指定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息，配置服务器默认采用git来存储配置信息，这样就有助于对环境配置进行版本管理，并且可以通过git客户端工具来方便的管理和访问配置内容。 能干嘛？1. 集中管理配置文件 2. 不同环境不同配置，动态化的配置更新，分环境部署。比如dev/test/prod/beta/release 3. 运行期间动态调整配置，不再需要在每个服务部署的机器上编写配置文件，服务会向配置中心统一拉去配置自己的信息 4. 当配置发生变动时，服务不需要重启即可感知到配置文件的变化，并应用新的配置 5. 将配置信息以REST接口的形式暴露 操作使用 新建两个工程，一个是config服务端，一个是config客户端 &lt;!-这里是服务端导入的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-这里是客户端导入的依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; yml文件编写 #这里是服务端需要编写的yml文件 server: port: 3344 spring: application: name: config-center cloud: config: server: git: uri: https://gitee.com/hou-wenming/springcloud-config.git #上面的git仓库名字 search-paths: - springcloud-config # skip-ssl-validation: true # username: 156542114@qq.com # password: hwm2000916.. default-label: master eureka: client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7003.com:7003/eureka instance: hostname: config-center #这里是客户端编写的yml文件 #命名必须是bootstrap.yml 否则无法获取中心仓库的内容 server: port: 3355 spring: application: name: config-client #config客户端配置 cloud: config: label: master # 分支名称 name: config # 配置文件名称 profile: dev # 读取后缀名称 uri: http://localhost:3344 # 配置中心地址 # discovery: # enabled: true eureka: client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7003.com:7003/eureka instance: hostname: config-client # 暴漏监控端点 management: endpoints: web: exposure: include: \"*\" 主启动类 //服务端 @SpringBootApplication @EnableConfigServer public class ConfigCenterMain { public static void main(String[] args) { SpringApplication.run(ConfigCenterMain.class,args); } } //客户端 @SpringBootApplication @EnableEurekaClient public class ConfigClientMain { public static void main(String[] args) { SpringApplication.run(ConfigClientMain.class,args); } } controller类 @RestController @RefreshScope//这个是解决配置中心内容修改，不需要重启的问题 public class ConfigClientController { @Value(\"${server.info}\") private String configInfo; @GetMapping(\"/configInfo\") public String getConfig(){ return configInfo; } } 当我们在配置中心修改了文件的时候，这时候服务端的数据可以实时更新，但是客户端不行，只能重启获取数据（如果服务端庞大，重启将会崩毁） 解决办法：暴漏监控端点 # 暴漏监控端点 management: endpoints: web: exposure: include: \"*\" //在Controller类上面加上这个注解 @RefreshScope 最后在cmd命令行中发送一个POST请求 curl -X POST “http://localhost:3355/actuator/refresh\" 12.bus总线前提：​ bug总线解决了上面我们的spring cloud config最后因为在远程配置中心修改文件的时候，我们的客户端不能实时的更新配置，而是需要重启才行，然后解决方法是发送一个post请求解决问题，但是我们每次更新都需要发送一个请求未免太麻烦了，这时候bus总线就体现了它的使用价值 简介：​ spring cloud bus 配合 spring cloud config 使用可以实现配置的动态刷新 ​ spring cloud bus是用来将分布式系统的节点与轻量级消息系统连接起来的框架，它整合了Java的事件处理机制和消息中间件的功能。spring cloud bus目前支持RabbitMQ和Kafka 这个图是bus通过向一个客户端进行发送，然后这个客户端再向其他客户端进行转发 ​ spring cloud bus能管理和传播分布式系统间的消息，就像一个分布式执行器，可用于广播状态更改、时间推送等，也可以当作微服务间的通信通道 为什么被称为操作总线什么是总线​ 在微服务架构的系统中，通常会使用轻量级的消息代理来构建一个共用的消息主题，并让系统中所有微服务实例都连接上来，由于 该主题中产生的消息会被所有实例监听和消费，所以称它为消息总线。在总线上的各个实例，都可以方便的广播一些需要让其他连接在该主题上的实例都知道的消息 基本原理​ configClient实例都监听MQ中同一个topic（默认是springcloudbus）。当一个服务刷新数据的时候，它会把这个信息放入到topic中，这样其他监听同一topic的服务就能得到通知，然后去更新自身的配置 实际操作使用 配置pom文件 //使用总线，必须在客户端和服务端添加这个依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt; &lt;/dependency&gt; 配置yam文件 #这个是配置中心，需要用mq的支持 # rabbitmq的相关配置 rabbitmq: host: localhost port: 5672 username: guest password: guest management: endpoints: web: exposure: include: 'bus-refresh' 后面的配置和前面的配置中心一样 如何去操作： 首先我们得启动我们的eureka注册中心，以及配置中心，和配置客户端 然后修改一下我们的中心配置文件 在cmd中发送这个请求：curl -X POST “http://localhost:3344/actuator/bus-refresh\" 登陆rabbit MQ进行广播 成功！ 也可以针对性的进行广播 在cmd中发送这个请求：curl -X POST “http://localhost:3344/actuator/bus-refresh：config-client:3355\" 13.springcloud stream消息中间件简介：​ 一句话：屏蔽底层消息中间件的差异，降低切换成本，统一消息的编程模型 ​ 官方定义springcloud stream是一个构建消息驱动微服务的框架 ​ 应用程序通过inputs或者outputs来与springcloud stream对象交互，通过配置来bingding（绑定），而springcloud stream的binder对象负责与消息中间件交互，所以我们只需要弄清楚如何与springcloud stream交互就可以方便使用消息驱动的方式 ​ 通过spring integration来连接消息代理中间件以实现消息事件驱动，springcloud stream为一些供应商的消息中间件产品提供了个性化的自动化配置实现，引用了发布订阅、消费组、分区的三个核心概念 Stream凭什么可以统一底层差异​ 通过向应用程序暴露统一的Channel通道，使得应用程序不需要再考虑各个不同的消息中间件实现 ​ 通过定义绑定器Binder作为中间层，实现了应用程序与消息中间件细节之间的隔离 设计思想 标准MQ 生产者/消费者之间靠消息媒介传递信息内容 Message 消息必须走特定的通道 消息通道Message Channel 消息通道里的消息如何被消费呢，谁负责收发处理 消息通道MessageChannel的子接口SubscribleChannel，由MessageHandler消息处理器所订阅 为什么用cloudStream？ 这些中间件的差异性导致我们实际项目开发给我们造成了一定的困扰，我们如果用了两个消息队列的其中一种，后面的业务需求，我想往另外一种消息队列进行迁移，这时候无疑就是一个灾难性的，一大堆东西都要重新推倒重新做，因为它跟我们的系统耦合了，这时候spring cloud stream 给我们提供了一种解耦合的方式 Stream中的消息通信方式遵循了发布-订阅模式 Binder：很方便的连接中间件，屏蔽差异 Channel：通道，是队列的一种抽象，在消息通讯系统中就是实现存储和转发的媒介，通过channel对队列进行配置 Source和SInk：简单的可理解为参照对象是SpringCloudStream自身，从Stream发布消息就是输出，接收消息就是输入 SpringCloudStream标准流程套路 如何使用？ 创建stream流的服务端和客户端 导入pom文件 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;Common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--导入stream rabbit依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编写yml文件 server: port: 9902 spring: application: name: stream-customer cloud: stream: binders: # 在此处配置要绑定的rabbitmq的服务信息 defaultRabbit: # 表示定义的名称，用于bingding整合 type: rabbit #消息组件类型 environment: #设置rabbitmq的相关的环境配置 spring: rabbitmq: host: localhost port: 5672 username: guest password: guest bindings: #服务的整合处理 input: #这个名字是一个通道的名称 destination: studyExchange #表示要使用的Exchange名称定义 content-type: application/json #设置消息类型，本次为json，文本则设置text/plain defaultbinder: defaultRabbit #设置要绑定的消息服务的具体设置 group: A eureka: client: register-with-eureka: true fetch-registry: true service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7003.com:7003/eureka instance: lease-renewal-interval-in-seconds: 2 #设置心跳的时间间隔（默认是30s） lease-expiration-duration-in-seconds: 5 #如果现在超过了5秒的间隔（默认是90s） 主启动类 提供方的业务类代码 //接口 public interface streamrabbitmqService { public String send(); } //实现类 @EnableBinding(Source.class) public class StreamrabbitmqServiceImpl implements streamrabbitmqService { @Resource private MessageChannel output; @Override public String send() { String serial = UUID.randomUUID().toString(); output.send(MessageBuilder.withPayload(serial).build()); System.out.println(\"*********\"+serial); return null; } } //controller @RestController public class streamrabbitmqController { @Resource private streamrabbitmqService service; @GetMapping(\"/sendMessage\") public String sendMessage(){ return service.send(); } } 消费方的业务代码 @Component @EnableBinding(Sink.class) public class StreamMQCustomerController { @Value(\"${server.port}\") private String serverPort; @StreamListener(Sink.INPUT) public void input(Message&lt;String&gt; message){ System.out.println(\"消费者9902-----&gt;接受到的消息：\"+message.getPayload()+\"\\t +port:\"+serverPort); } } 以上的全部完成之后，启动Eureka注册中心，以及消费者和提供者，还有RabbitMQ消息转发中心 然后进行测试，测试地址:http://localhost:9901/sendMessage 当我们访问的时候，提供方会进行消息的发送，然后消费方会收到信息，并在控制台显示，这时候我们的RabbitMQ显示台就会显示我们的信息发送的频率 最后如果我们的消费方有多个，那么就会发生重复消费的问题，如何解决? 我们可以在配置文件中加入 grop（组）的概念 这样如果不同的组会继续重复消费，而相同的组则会竞争轮询的进行消费 但是如果我们消费方这时候断线了，并且其中一个消费者移除了grop这个概念，那么再上线的时候，移除了grop的会收不到未接受的信息，而没有移除的则会收到未接受的信息 14.sleuth +Zipkin 15.Cloud Alibaba有哪些功能： 服务限流降级：默认支持Servlet、Feign、Rest Template、Dubbo和Rocket MQ限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级Metrics监控 服务注册与发现：适配SpringCloud服务注册与发现标准，默认集成了Ribbon的支持 分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新 消息驱动能力：基于SpringCloud Stream为微服务应用构建消息驱动能力 阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务、支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。 分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于Cron表达式）任务调度服务。同时提供分布式的任务执行模型。如网格任务、网格任务支持海量子任务均匀分配到所有Worker上执行。 1. Nacos简介：​ 一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台 ​ 就是注册中心+配置中心的组合（相当于前面所讲的Eureka+ribbon+bus所继承为一体的管理平台） 使用：1. 首先启动nacos2.编写服务提供项目 pom &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; yaml #服务端口 server: port: 9001 #服务名 spring: application: name: nacos-payment # 注册进nacos的服务地址 cloud: nacos: discovery: server-addr: localhost:8848 #配置nacos地址 #暴露端口 management: endpoints: web: exposure: include: \"*\" 主启动类 @SpringBootApplication @EnableDiscoveryClient public class AlibabaPaymentMain9001 { public static void main(String[] args) { SpringApplication.run(AlibabaPaymentMain9001.class,args); } } 业务逻辑 @RestController public class PaymentController { @Value(\"${server.port}\") private String serverport; @GetMapping(\"/nacos/{id}\") public String getPayment(@PathVariable(\"id\") Integer id){ return \"hello nacos,serverport:\"+serverport+\" id:\"+id; } } 运行 3. 编写配置中心 pom &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; yam server: port: 3377 spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: localhost:8848 config: server-addr: localhost:8848 file-extension: yaml # group: test 这里是分组的名字 # namespace: 123456 这里是命名空间的名字 spring: profiles: active: dev #这里是测试开发环境 # active: test 主启动类 @SpringBootApplication @EnableDiscoveryClient public class configAlibabaMain { public static void main(String[] args) { SpringApplication.run(configAlibabaMain.class,args); } } 服务类 @RestController @RefreshScope //动态刷新功能 public class ConfigController { @Value(\"${config.info}\") private String Info; @GetMapping(\"/config/Info\") public String getInfo(){ return Info; } } 启动运行 4. 编写客户端 pom文件 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; yml文件 server: port: 83 spring: application: name: nacos-order cloud: nacos: discovery: server-addr: localhost:8848 server-url: nacos-user-service: http://nacos-payment 主启动类 @SpringBootApplication @EnableDiscoveryClient public class OrderMian83 { public static void main(String[] args) { SpringApplication.run(OrderMian83.class,args); } } 配置类 @Configuration public class Config { @Bean @LoadBalanced public RestTemplate getRestTemplate(){ return new RestTemplate(); } } 服务类 @RestController public class OrderController { @Resource private RestTemplate restTemplate; @Value(\"${server-url.nacos-user-service}\") private String serviceURL; @GetMapping(\"/customer/nacos/{id}\") public String AlibabaNacos(@PathVariable(\"id\") String id){ return restTemplate.getForObject(serviceURL+\"/nacos/\"+id,String.class); } } 5. 实现集群配置以及持久化（重点） 总体架构实现 1 nginx+3 nacos + 1 mysql 首先在虚拟机上部署我们的nginx服务器，然后配置nacos服务，最后将nacos自带的嵌入式数据库换成mysql 在本机运行，实现多个注册中心同时运行 2.Sentinel Sentinel持久化 首先在客户端添加以下的配置文件 然后在nacos中配置中心配置以下信息 3. Seata简介： 什么是Seata： Seata是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata将为用户提供了AT、TCC、SAG和XA事务模式，为用户打造一站式的分布式解决方案。 Seata术语 XID TC-事务协调者： 维护全局和分支事务的状态，驱动全局事务提交或者回滚 TM-事务管理器、 定义全局事务的范围：开始全局事务、提交或回滚全局事务 RM-资源管理器 管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚 TM向TC申请开启一个全局事务，全部事务创建成功并生成一个全局唯一的XID XID在微服务调用链路的上下文中传播 RM向TC注册分支事务，将其纳入XID对应全局事务的管辖 TM向TC发起针对XID的全局提交或者回滚决议 TC调度XID下管辖的全部分支事务完成提交或回滚请求 详细过程： TM开启分布式事务（TM向TC注册全局事务记录） 按业务场景、编排数据库、服务等事务内资源（RM向TC汇报资源准备状态） TM结束分布式事务，事务一阶段能结束（TM通知TC提交／回滚分布式事务） TC汇总事务信息，决定分布式事务是提交还是回滚 TC通知所有RM提交／回滚资源，事务二阶段结束 其中的事务阶段细节 在一阶段，Seata会拦截业务SQL 解析SQL语义，找到业务SQL要更新的业务数据，在业务数据被更新前，将其保存成before image 执行业务SQL更新业务数据，在业务数据更新之后 将其保存成after image，最后生成行锁 以上操作全部在一个数据库事务内完成，这样保证了一阶段操作的原子性 二阶段回滚： 二阶段如果是回滚的话，Seata就需要回滚一阶段已经执行的业务SQL，还原业务数据 回滚方式就是用before image还原业务数据；但是还原之前要首先校验脏写，对比数据库当前业务数据和after image 如果两份数据完全一致就说明没有脏写，可以还原业务数据，如果不一致就说明有脏写，出现脏写就需要转人工处理","categories":[{"name":"Java","slug":"Java","permalink":"http://hwm156542114.github.io/categories/Java/"}],"tags":[{"name":"springcloud","slug":"springcloud","permalink":"http://hwm156542114.github.io/tags/springcloud/"}]},{"title":"ArrayList源码分析","slug":"ArrayList源码分析","date":"2021-11-16T13:24:13.000Z","updated":"2021-12-08T02:09:09.408Z","comments":true,"path":"2021/11/16/arraylist-yuan-ma-fen-xi/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/arraylist-yuan-ma-fen-xi/","excerpt":"","text":"ArrayList源码分析ArrayList类的继承和接口 首先看一下ArrayList这个类的继承和实现关系 public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 从上面可以看出它继承了AbstractList，实现了RandomAccess, Cloneable, java.io.Serializable这几个接口 那么来分析一下这几个的作用吧 AbstractList 通过看这个抽象类的源码可以知道，这个就是ArrayList的骨架，所以ArrayList会通过AbstractList来构建自己的骨架和方法 RandomAccess 这个接口提供了随机访问功能。RandmoAccess是java中用来被List实现，为List提供快速访问功能的。在ArrayList中，我们即可以通过元素的序号快速获取元素对象；这就是快速随机访问。在实践中，如果集合实现了这个接口，那么就建议采取随机访问，这样速度会更快，如果没有实现这个接口，建议采取顺序访问；因为有无这个接口然后选择访问的方式会对访问速度有很大的影响。 Cloneable这个接口主要是提供了ArrayList可以被拷贝的功能 java.io.Serializable 这个接口主要提供了ArrayList的序列化功能，因为实现了序列化，所以在实践中，如果有大量数据需要进行存储或者取出，那么就可以考虑使用ArrayList集合进行操作。 ArrayList是如何初始化的？ 先声明接下来会出现的参数 //初始化容量为10 private static final int DEFAULT_CAPACITY = 10; //定义一个空数组 private static final Object[] EMPTY_ELEMENTDATA = {}; //定义一个默认容量为空的空数组 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; //定义一个数组（这个也就是ArrayList底层使用的数组） transient Object[] elementData; // non-private to simplify nested class access //ArrayList中元素所占数组的大小 private int size; 然后从它的三个构造方法说起： ArrayList list1 = new ArrayList&lt;&gt;(); ArrayList list2 = new ArrayList&lt;&gt;(12); ArrayList list3= new ArrayList&lt;&gt;(list1); 第一个无参构造 进行debug来看看怎么初始化的 //可以看到，无参构造初始化，进入的就是他自身的无参构造器 public ArrayList() { //这是将本身的数组设置为空数组 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } 第二个有参构造 //有参构造就会进入到这个构造方法中 //initialCapacity这个就是传过来的初始化容量参数 public ArrayList(int initialCapacity) { //判断如果初始化容量的参数大于0，那么就将新创建的数组（这个数组的容量就是前面传过来的初始化容量大小）赋给本地的数组 if (initialCapacity &gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { //如果初始化容量大小等于零，那么就将本地数组置为空数组 this.elementData = EMPTY_ELEMENTDATA; } else { //如果上面两种条件都不满足，那么就会报出非法容量异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); } } 第三个有参构造 //进入到这个构造方法中，参数传递是集合 public ArrayList(Collection&lt;? extends E&gt; c) { //首先将传过来的集合变成数组，然后将这个数组赋给本地数组 elementData = c.toArray();//有关这个方法的调用在下面代码中指出 //这里将size赋值，然后判断本地数组的长度 是否等于0 if ((size = elementData.length) != 0) { // 如果不等于0，再进行判断本地数组的类和对象数组的类是否相同 if (elementData.getClass() != Object[].class) //如果不相同，那么就会将本地数组的类型转换成对象数组类型 elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // 如果上述条件都不符合，那么就会将本地数组置为空 this.elementData = EMPTY_ELEMENTDATA; } } //这是上面的toArray方法 public Object[] toArray() { //而这个方法的底层实现是将数据拷贝到一个新数组，然后将这个新数组进行返回 return Arrays.copyOf(elementData, size); } //original就是传过来的数据，newLength就是传过来的集合长度 public static &lt;T&gt; T[] copyOf(T[] original, int newLength) { return (T[]) copyOf(original, newLength, original.getClass()); } //实际上调用的就是这个方法！！！！！ public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) { @SuppressWarnings(\"unchecked\") //这里进行三元运算判断，判断新类型是否和对象数组类型相同，这里不管判断是正确还是不正确，都会创建一个新数组，然后把数据放到这个新数组里面，进行返回 T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); //这里是进行数据拷贝，Math.min(original.length, newLength)这个是比较传过来的数据长度和传过来的长度大小。 System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; } 通过上面的构造方法，可以知道ArrayList是如何初始化的 ArrayList是怎么实现增删改查的？ 下面再通过它的增删改查来进一步探索 从增加开始说起，直接上代码//就从这个代码开始debug吧 ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); //首先进入的是这个添加方法中，这是boolean类型的方法，但是它返回的永远是ture public boolean add(E e) { //这个方法主要是判断是否是第一次添加，然后数组是否初始化过，然后再进行后面的扩容判断。 ensureCapacityInternal(size + 1); // Increments modCount!! //在将元素添加进数组之前，需要判断这个数组的容量是否能够让这个元素添加进去 //将size作为下标，存储传进来的元素 elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { //因为是第一次添加操作，所以传过来的minCapacity最小值是1，也就是前面的size + 1 //这里进行判断，本地数组是否为空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { //如果是空数组，那么就将这个最小容量设置为默认容量，也就是10 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } //进行扩容判断 ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { //计数用的 modCount++; //判断预扩展的值与当前数组的容量大小 if (minCapacity - elementData.length &gt; 0) //如果大于，说明数组容量不够，会进行扩容操作 grow(minCapacity); } private void grow(int minCapacity) { // oldCapacity将本地数组长度记录下来 int oldCapacity = elementData.length; //&gt;&gt;是右移，也就是除以2的几次幂 //&lt;&lt;是左移，也就是乘以2的几次幂 //newCapacity=旧的容量+将旧的容量右移后的结果 //得到的结果就是newCapacity是原来旧容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //判断如果新的容量减去最小容量小于0 if (newCapacity - minCapacity &lt; 0) //那么新容量就还是等于最小容量（意味着没有扩容） newCapacity = minCapacity; //如果新的容量大于最大数组容量 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //那么就会 newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } private static int hugeCapacity(int minCapacity) { //如果最小容量小于0，内存溢出异常 if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); //如果上述条件不满足，进行三元运算，如果最小容量大于最大数组容量，那么就会将这个更大的值返回，否则就返回最大数组容量 return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } 将思路捋清：这个代码主要是在添加元素之前将确保一下数组容量的大小，以便后面可以正常的元素插入。在这个确保容量的时候，首先如果是第一次添加进来的，那么因为我们初始化集合是用的无参构造方法，所以先将这个数组的大小设置为默认大小。设置完之后，再判断添加的元素之后的长度是否大于当前数组的容量，如果大于那么就会进入到扩容中，然后将旧的容量扩充为原来的1.5倍（这里主要是针对第一次进来时的情况：如果扩充的容量比最小容量还小，那么就标志这扩容失败，容量还是采用最小容量，如果容量大于最大数组容量那么就会再进行相关的判断….） 思考思考思考！！！！！ 到了这里，其实我还是有很多的疑问，虽然源码看明白了，但是为什么是这么个流程呢？ 带着问题，将源码更通透的理解一下： 为什么是否扩容的判断条件如此难懂？minCapacity - elementData.length &gt; 0 ？？？？ 为什么最小容量大于元素存储的容量还要进行扩容，这里最小容量大于存储容量，那就证明有空间存储啊，为啥还要进行扩容，反而最小容量如果小于那就什么操作也不做？？？？下面纯属个人理解： 首先这里有一个错误的理解：受第一次添加数据的影响，我把minCapacity当成了一个定值，我一直把这个值当做是10来处理，其实这个最小容量是根据每次添加数据动态更新的值；elementData.length我把它理解成当前数组中存储元素的长度，其实是初始化数组长度的值!!!因此，我无法理解这个扩容的判断条件。 正确思路：将上述的错误思想改正之后，豁然开朗，解决思路就是我来了一个一万次循环将元素添加进集合当中，那么这个时候，你会发现这个minCapacity是个什么东西呢，它其实就是在我们添加元素之前，将数组容量预扩展一个存放元素的空间，然后用这个预扩展的值减去当前数组的长度，如果这个长度大于数组长度，那么就说明数组长度不够，这时候就会调用grow方法，将数组扩容为原来的1.5倍。 如果上面的描述还是不懂，那么现在假设现在循环到了第十一次，到调用这个添加方法的时候，首先判断容量是否够，传过去的参数就是minCapacity=11，而这个时候elementData.length就是当时初始化的长度，也就是10，所以说现在是不是就必须得将数组扩容呢？那么就进入到grow方法当中，一顿操作下来，那容量不就变成了15嘛，然后不就可以继续存元素了吗？？？ 经过上面的一顿分析之后，下面再看看另外的几个添加方法： public void add(int index, E element) 在指定位置添加元素 ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); //添加到指定位置 list1.add(1,\"abc\"); //进入到这个方法当中 public void add(int index, E element) { //校验传过来的下标是否合格 rangeCheckForAdd(index); //校验数组容量是否设置过，然后判断是否需要扩容 ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy(Object src, int srcPos,Object dest, int destPos,int length); //src表示源数组，srcPos表示源数组要复制的起始位置，desc表示目标数组,destPos在目标数组中开始赋值的位置,length表示要复制的长度。 //主要就是这段copy代码，这段代码是怎么实现的呢？？？ //首先数据源是elementData{aaa,bbb,ccc}，也就是原数组，要复制的起始位置就是index=1(bbb)，目标数组还是elementData{aaa,bbb,ccc}，开始赋值的位置就是2(ccc)开始，要复制的长度为3-1=2；那么根据上述条件，我们知道复制完之后数组的数据变成了elementData{aaa,bbb,bbb,ccc} System.arraycopy(elementData, index, elementData, index + 1, size - index); //然后将索引的位置变成要添加的元素值（在指定的地方添加元素） elementData[index] = element; size++; } private void rangeCheckForAdd(int index) { //是否大于数组长度，是否小于0，如果是就会抛出索引越界异常 if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } public boolean addAll(Collection&lt;? extends E&gt; c) 添加集合元素 ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); ArrayList list3= new ArrayList&lt;&gt;(list1); list3.addAll(list1); //进入到这个方法中来 public boolean addAll(Collection&lt;? extends E&gt; c) { //首先将集合变成一个对象数组 Object[] a = c.toArray(); //记录数组的长度 int numNew = a.length; //校验数组的容量，判断是否扩容 ensureCapacityInternal(size + numNew); // Increments modCount //又来到的熟悉的地方，同样的，我们根据上一个方法来进行分析 //数据源：a{aaa,bbb,ccc} 复制起始位置0 目标数组elementData{} 目标数组的位置0 复制的长度3 //那么最终得到的结果就是复制过来的数组elementData{aaa,bbb,ccc} System.arraycopy(a, 0, elementData, size, numNew); //数组的长度变成原本的长度加上新添加数据的长度 size += numNew; return numNew != 0; } public boolean addAll(int index, Collection&lt;? extends E&gt; c) 将集合添加到指定位置上 ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); ArrayList list3= new ArrayList&lt;&gt;(); list3.add(\"ddd\"); list3.add(\"eee\"); list3.addAll(1,list1); //进入到这个方法中 public boolean addAll(int index, Collection&lt;? extends E&gt; c) { //判断下标是否越界 rangeCheckForAdd(index); //将集合先变成对象数组 Object[] a = c.toArray(); //将数组的长度取出 int numNew = a.length; //校验是否有初始容量，以及是否需要扩容 ensureCapacityInternal(size + numNew); // Increments modCount //记录要移动的步数 int numMoved = size - index; //如果大于0 if (numMoved &gt; 0) //首先是进行第一次拷贝，这次拷贝主要是将目标数组扩充为可以容纳新数组的数组 //数据源elementData{ddd,eee} 复制的起始位置1 目标数组elementData 在目标数组赋值的位置1+3 复制的长度1 复制完成之后的数组变成elementData{ddd,eee,null,null,eee} System.arraycopy(elementData, index, elementData, index + numNew, numMoved); //第二次拷贝才是将真正的数据添加进数组中 System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; } 修改方法set上代码 ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); list1.set(1,\"ddd\"); public E set(int index, E element) { //判断索引下标是否越界 rangeCheck(index); //将旧的值取出 E oldValue = elementData(index); //将新的值修改 elementData[index] = element; //返回旧的值 return oldValue; } 获取方法getArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); list1.get(2); public E get(int index) { //判断索引下标是否越界 rangeCheck(index); //返回当前索引的值 return elementData(index); } toString方法源码解析ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); list1.toString(); //首先得明确一点，这个方法不是ArrayList的而是AbstractList中的方法 public String toString() { //主要使用的迭代器 Iterator&lt;E&gt; it = iterator(); //判断如果为空，就直接返回空串 if (! it.hasNext()) return \"[]\"; //如果不为空，那么用StringBuilder进行字符串拼接 StringBuilder sb = new StringBuilder(); //先将结构搭建好 sb.append('['); //然后用循环将里面的数据一一拼接到字符串上 for (;;) { E e = it.next(); //这里通过三元运算，判断y sb.append(e == this ? \"(this Collection)\" : e); if (! it.hasNext()) //如果没有数据了，就进行结尾 return sb.append(']').toString(); sb.append(',').append(' '); } } 迭代器的源码 这里我会通过三个不同的案例，来分析一下迭代器具体的源码，然后引申出相关的问题 第一个例子：通过迭代器遍历，使用集合自带的方法删除目标元素ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); Iterator it = list1.iterator(); while(it.hasNext()){ String s = (String) it.next(); if (s.equals(\"aaa\")){ list1.remove(s); } } 将这个代码运行之后，你会发现代码报错 Exception in thread \"main\" java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901) at java.util.ArrayList$Itr.next(ArrayList.java:851) at jihe.test.main(test.java:17) 为什么会报错呢？？？带着问题，查看源码！！！ //先进入这个迭代器的方法中，而后会发现，一个内部类继承iterator重写了迭代器方法 public Iterator&lt;E&gt; iterator() { return new Itr(); } //主要来看一下这个类的具体实现 private class Itr implements Iterator&lt;E&gt; { int cursor; // 这里是光标位置 int lastRet = -1; // 返回最后一个元素的索引 //将实际修改次数赋值给预期修改次数（报错的关键语句） int expectedModCount = modCount; //判断光标位置是否到达末尾 public boolean hasNext() { return cursor != size; } @SuppressWarnings(\"unchecked\") public E next() { //检查实际修改次数和预期修改次数是否一样，如果不一样那么就会报出刚刚的错误ConcurrentModificationException（报错的关键方法） checkForComodification(); //将光标的值赋值给i int i = cursor; //size：就是数组的长度 //然后用i和size进行比较，如果光标的值大于size的值，那么就会抛出异常NoSuchElementException if (i &gt;= size) throw new NoSuchElementException(); //将集合数组赋值给elementData Object[] elementData = ArrayList.this.elementData; //如果光标的值大于数组的长度抛出异常ConcurrentModificationException if (i &gt;= elementData.length) throw new ConcurrentModificationException(); //光标自增 cursor = i + 1; //将i的值赋值给lastRet，并且将当前遍历到的数据进行返回 return (E) elementData[lastRet = i]; } final void checkForComodification() { //判断实际修改次数和语气修改次数是否相等 if (modCount != expectedModCount) //不想等就直接报出ConcurrentModificationException异常 throw new ConcurrentModificationException(); } //上面的遍历大致就是那么个思路，然后根据程序的执行，我们会进行比较判断，如果当前遍历的值与要删除的值相同，那么就调用ArrayList自带的移除方法 public boolean remove(Object o) { //首先判断传过来的这个值是否为空 if (o == null) { //如果为空，那么就会依次遍历，将数组中的空值移除 for (int index = 0; index &lt; size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { //如果不为空，那么就会进行遍历 for (int index = 0; index &lt; size; index++) //找到与传过来的值相匹配的元素地址，然后进行移除操作 if (o.equals(elementData[index])) { //移除 fastRemove(index); return true; } } return false; } //在这个移除操作中 private void fastRemove(int index) { //首先会把实际修改次数加1 modCount++; //然后计算移除的位置 int numMoved = size - index - 1; if (numMoved &gt; 0) //将移除元素后面的元素进行拷贝 System.arraycopy(elementData, index+1, elementData, index, numMoved); //将元素置为空 elementData[--size] = null; // clear to let GC do its work } 那么通过上面的案例，可能还是没有明白为什么会报错呢，程序都执行到了结尾了，数据也移除了，那么为什么还会报错呢？？ 请继续往下看，程序确实已经完成了移除操作，但是，程序还没有执行完，继续往下走，会再次进入迭代器的遍历当中，当遍历了一个元素之后，再次调用next方法，就可以看到next方法中首先出现的checkForComodification();，那么核心就在这个方法当中了，还记得之前在添加数据的时候，modCount会记录修改的次数，之前添加了三个元素，也就是modCount=3，然后将modCount的值赋给了expectedModCount，那么expectedModCount的值是不是也等于3，但是在刚刚移除元素的过程中，是不是有modCount++出现，那么这时候modCount是不是等于4，这样是不是一切都明了了，此时modCount != expectedModCount成立，就会报出刚刚的错误了。 if (modCount != expectedModCount) throw new ConcurrentModificationException(); 第二个例子：和第一次是一样的代码，注意不同的是这次移除的是倒数第二个元素ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); Iterator it = list1.iterator(); while(it.hasNext()){ String s = (String) it.next(); if (s.equals(\"bbb\")){ list1.remove(s); } } 前面的操作流程都是一样的，但是这次运行结果竟然不报错！！！！？？ 为什么不报错呢？？？ 这属于是特殊情况，为什么这么说呢，根据刚刚的代码，再来分析一下思路，如果删除的是倒数第二个元素，那么在下一次迭代器遍历中，首先看到的判断条件是不是public boolean hasNext(){return cursor != size;}，实际上就是这里出了问题，现在想一下，集合中一共有三个元素，在进行了移除操作，就剩下两个元素，而且在移除操作最后一步elementData[–size] = null，首先是将size进行–操作，这样就说明此时的size大小为2，而光标在上一次的遍历中是不是cursor = i + 1有这个操作，这时候cursor是不是也变成了2，那么hasNext这个判断条件就变成了false，既然变成了false，就会直接跳出遍历，程序并没有报错！！ 第三个例子：和第一次一样的代码，不同的是这一次使用的是迭代器自带的移除方法ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); Iterator it = list1.iterator(); while(it.hasNext()){ String s = (String) it.next(); if (s.equals(\"ccc\")){ //注意，这里是迭代器自带的方法进行元素的删除 it.remove(); } } 前面遍历操作的流程大致都是一样的，主要看一下这个迭代器自带的删除方法有什么异同 下面进行源码分析 public void remove() { //来看一下lastRet是什么？ //之前在前面的代码中，是不是在遍历的next方法中出现，这个lastRet指向的就是，当前遍历的元素，也可以说是遍历到的最后一个元素（不是遍历完，而是遍历到当前的元素，当前元素的下表作为lastRet的值）。这个lastRet的值就是需要remove的那个元素的下标 //首先判断这个下标是否小于0，如果是就会抛出IllegalStateException if (lastRet &lt; 0) throw new IllegalStateException(); //检查预期修改次数和实际修改次数是否相同 checkForComodification(); try { //调用集合本地的remove方法，将lastRet这个下标指向的值删除 ArrayList.this.remove(lastRet); //将当前遍历的最后一个值得下标赋值给光标 cursor = lastRet; //将lastRet置为-1 lastRet = -1; //将实际修改的次数赋值给预期修改次数 expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } } public E remove(int index) { //校验下表是否越界 rangeCheck(index); //实际修改次数加1 modCount++; //记录旧值 E oldValue = elementData(index); //要移动的步数 int numMoved = size - index - 1; if (numMoved &gt; 0) //进行数组拷贝 System.arraycopy(elementData, index+1, elementData, index, numMoved); //将指定的值移除 elementData[--size] = null; // clear to let GC do its work //将旧值返回 return oldValue; } private void rangeCheck(int index) { if (index &gt;= size) //如果索引下标大于集合大小，就会报错 throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } 总结：可以看到，通过迭代器的方法进行删除的时候，大致思路是一样的，而且底层的删除还是用的ArrayList自带的集合进行删除的，不过不同的是，迭代器的删除方法中**expectedModCount = modCount;**多出了这样的一个步骤，所以迭代器删除是不会报错的。 clear、contains、isEmpty源码 clear方法： //简单明了 public void clear() { //实际修改次数自增 modCount++; // 通过遍历，将所有的值置为空，以便垃圾回收机制将它们进行回收 for (int i = 0; i &lt; size; i++) elementData[i] = null; //将数组的大小也置为0 size = 0; } IsEmpty方法： //判断数组大小是否为0 public boolean isEmpty() { return size == 0; } contains方法： ArrayList list1 = new ArrayList&lt;&gt;(); list1.add(\"aaa\"); list1.add(\"bbb\"); list1.add(\"ccc\"); list1.contains(\"abc\"); //首先进入这个方法中，返回indexOf(o) &gt;= 0比值是否大于0，如果大于就是包含相应的值，如果小于0就是不包含 public boolean contains(Object o) { return indexOf(o) &gt;= 0; } //核心方法 public int indexOf(Object o) { //判断传过来的对象是否为空 if (o == null) { //遍历数组，返回为空的对象下标 for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; } else { //遍历数组，返回与传过来对象匹配的下标 for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; } //如果都没有找到，就返回-1 return -1; } 最终可以通过判断**list1.contains(“abc”);**的返回值是ture还是false来判断是否包含所传入的值。 考虑以下几个问题 ArrayList是如何扩容的? ArrayList频繁扩容导致添加性能几句下降，如何处理？只想到了使用初始容量的构造方法，不过浪费空间 ArrayList插入或者删除元素一定比LinkedList慢吗？不一定，这个就要看是否针对某个位置进行相应的插入和删除操作，如果不针对位置进行插入和删除，那么肯定是LinkedList比较快的，如果针对位置，那么它们俩的性能是差不多的。 ArrayList是线程安全的吗？ 不是线程安全的，那么又如果解决这个不安全，或者手写一个不安全的案例看看？ 如何解决线程不安全？ 使用Vector、使用synchronize、以及Collections.synchronizedList()、lock（） 如何将某个ArrayList复制到另一个ArrayList中构造方法、clone、添加 多线程下保证正常读写？copyOnWriteArrayList ArrayList和LinkedList的区别？","categories":[{"name":"Java","slug":"Java","permalink":"http://hwm156542114.github.io/categories/Java/"}],"tags":[{"name":"ArrayList源码","slug":"ArrayList源码","permalink":"http://hwm156542114.github.io/tags/ArrayList%E6%BA%90%E7%A0%81/"}]},{"title":"Spring源码分析","slug":"Spring源码分析","date":"2021-11-16T13:23:11.000Z","updated":"2021-12-08T02:09:35.991Z","comments":true,"path":"2021/11/16/spring-yuan-ma-fen-xi/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/spring-yuan-ma-fen-xi/","excerpt":"","text":"加载Bean 封装资源文件 对Resource使用EncodedResource进行封装 获取输入流，在Resource中获取对应的输入流 进入核心部分：数据准备阶段 获取对XML文件的验证模式 DTD：文本类型定义–&gt;验证XML文档格式是否正确 XSD：XML schemas Defination –&gt;验证XML文档格式是否正确 有了上面的两种xml文档的校验模式，spring会在进行校验模式的时候判断如果手动指定了验证模式（自定义的），那么就使用自定义的模式进行校验，如果没有指定则按照默认的验证校验模式。如果使用默认的校验模式，那么就会根据xml文件判断使用的是DTD还是XSD验证模式 加载XML文件，并得到对应的Document对象 EntityResolver：因为每次验证都会根据xml文件里面的声明去网上找相应的校验文件，这样会很费时间，因此使用这个方法将需要校验的文件直接保存在本地，在需要使用的时候直接从本地进行加载校验，而不需要从网上下载。 DTDResolver和SchemasResolver负责xml文件的解析 根据返回的Document注册Bean信息 doRegisterBeanDefinations这个方法就是解析的核心部分，这里在做解析的时候，前后使用了模板方法设计模式针对对Bean解析前后的相应处理 profile属性：这个属性主要是指定了开发、部署环境，用这个属性可以指定想要的开发环境 解析注册BeanDefination，首先通过node.getnamespaceURI获取命名空间，然后通过对比spring中固定的命名空间，如果相同则使用默认的方法进行解析，如果不同则使用用户自定义的方法进行解析 默认标签的解析这里会分四种情况，分别对import、alias、bean和beans做出不同的处理 Bean标签的解析及注册 首先委派BeanDefinationDelegate类的parseBeanDefinationElement方法进行元素解析，返回BeanDefinationHolder类型的实例，经过这个方法，bdHolder已经包含配置文件中配置的各种属性，例如：class、name、id、alias之类的属性 进入到parseBeanDefinationElement方法 提取元素中的id和name属性 进一步解析其他所有属性，并统一封装成GenericBeanDefination类型实例中 创建用于属性承载的BeanDefination：BeanDefination是一个接口，spring中有三种实现（RootBeanDefination、ChildBeanDefination、GenericBeanDefination） 解析各种属性（parseBeanDefinationAttributes），存到BeanDefination中 解析子元素（construct-args、property…） 如果检测到bean没有指定的beanName，那么使用默认规则为此Bean生成BeanName（通过beanClassName生成名字） 将获取到的信息封装到BeanDefinationHolder中 当返回的bdHolder不为空的时候，如存在默认标签的子节点下还有自定义的属性，还需要再次对自定义标签进行解析 寻找自定义标签并根据自定义标签寻找命名空间处理器，进行解析 解析完成之后，需要对解析后的bdHolder进行注册，注册操作委派给BeanDefinationReaderUtils的registerBeanDefination方法 通过BeanName注册 对AbstractBeanDefination校验methodOverrides属性 对BeanName已经注册过的情况，如果不允许Bean覆盖，那么就会抛出异常，否则覆盖 加入map缓存 清除解析之前留下的beanName缓存 通过别名注册 alias与beanName相同不需要处理，删除原有的alias 若aliasName已经使用，那么进行覆盖操作 循环检查 注册alias 最后发出响应事件，通知相关监视器，bean加载完成 alias标签解析别名注册，就是将当前的Bean起多个别名，以适用于各种不同的场景 解析过程与Bean类似。。 impor标签解析 获取resource属性所表示的路径 解析路径中的系统属性 判断location是绝对路径还是相对路径 如果是绝对路径则递归调用bean的解析过程，执行另一次的解析 如果是相对路径，则计算出绝对路径进行解析 通知监视器完成 嵌入式Beans标签的解析。。。 自定义标签的解析获取标签的命名空间getNamespaceURI 提取自定义标签处理器通过Namespacehandler进行提取，这里就提到了之前自定义的处理器，如果命名空间与命名空间处理器有映射关系，那么就会从缓存中获取映射关系，如果不存在于缓存中，那么就使用自定义的处理器进行初始化后，存到缓存中 获取已经配置的handler映射 根据命名空间找到对应的信息 已经做过解析的情况直接从缓存中读取 没有做过解析，则返回类路径，使用反射将类路径转化为类 初始化类，调用自定义的NamespaceHandler的初始化方法 记录在缓存中 标签解析得到解析器后，就会将工作委派给解析器去进行解析 Bean的加载 转换对应的BeanName 去除FactoryBean的修饰符 取指定的alias所表示的最终BeanName 尝试从缓存中加载单例单例在spring的同一个容器中只会被创建一次，后续再获取bean直接从单例缓存中获取。这里只是尝试获取，首先尝试从缓存中加载，如果加载不成功再尝试去singeletonFactory中加载。因为在创建单例bean的时候，会存在依赖注入问题，因此为了避免依赖注入，在spring中创建bean的原则是不等bean创建完成就会将创建bean的ObjectFactory提早曝光加入到缓存中，一旦下一个bean创建时候需要依赖上一个bean，直接使用Objectfactory bean的实例化如果缓存中得到了bean的原始状态，那么就需要对bean进行实例化，从而得到需要的bean 原型模型的依赖检查只有在单例模式下才会进行循环依赖检查 检查parentBeanFactory如果缓存没有数据的话，就转到父类工厂去加载 将存储XML文件的GenericBeanDefination转换为RootBeanDefination后续操作都是针对RootBeanDefination，因此需要转换，如果BeanName是子bean的话，那么就会同时合并父类相关的属性 寻找依赖 如果某些属性用到了其他属性，其他属性依赖其他的Bean，那么此时就需要递归查找加载依赖的bean 针对不同的scope进行bean的创建针对不同的scope进行不同类型的初始化 类型转换 循环依赖 什么是循环依赖？ 就是两个或两个以上的bean，在方法中互相调用对方，然后他们之间的调用关系形成了一个环，这样的情况叫做循环依赖。 spring如何解决循环依赖的？ spring中循环以来包含构造器循环依赖和setter方法循环依赖 构造器循环依赖 通过构造器产生的循环依赖是无法解决的，只能抛出异常 因为在创建的过程中，比如有A、B、C三个bean对象，这三个对象互相调用对方，在创建A的时候，会在创建池中有标识A创建，但是创建过程中发现里面还有B创建，这时候就会先去创建B这个对象，然后在创建池中标识B的创建，但是B中又包含C的调用，这时候就需要创建C，然后在创建池中标识C的创建，但是这个时候C里面调用了A，这时候又会去创建A，但是在创建池中已经有A创建的标识，因此会直接抛出错误。（如果对象创建完成，则会将创建池中的标识清除掉） setter循环依赖（只能解决单例模式下的循环依赖） 首先在创建A的时候，会先根据无参构造器，先创建一个bean，并且暴露一个ObjectFactory，用于返回一个提前曝光的一个创建中的bean，这样即使有循环依赖，但是因为创建的无参构造bean，即使在创建池中遇到这个依赖的bean，可以通过无参构造的bean进行返回，从而完成创建，并且不造成循环依赖 创建Bean创建bean的实例 如果工厂方法不为空，那么使用工厂方法初始化策略 因为构造方法有很多参数，不同的参数需要不同的构造函数或对应的工厂方法 第2步如果进行了解析，那么就直接使用解析好的构造方法，不需要再次锁定 构造函数注入 默认构造函数构造 如果没有解析则需要根据参数解析构造器 构造函数注入 默认构造函数构造 构造函数注入 构造函数参数的确定 根据explicArgs参数判断 缓存中获取 配置文件获取 构造函数确定 根据确定的构造函数转换对应的参数类型 构造函数不确定性的验证 根据实例化策略以及得到的构造函数和构造函数的参数来实例化bean 不带参数的构造方法 实例化策略 使用过的设计模式 模板设计模式：在解析Bean的时候用过这个方法，主要是如果对解析Bean前后有相应的操作就会继承那两个方法进行操作。","categories":[{"name":"Java","slug":"Java","permalink":"http://hwm156542114.github.io/categories/Java/"}],"tags":[{"name":"spring源码解读","slug":"spring源码解读","permalink":"http://hwm156542114.github.io/tags/spring%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"}]},{"title":"Mysql优化","slug":"Mysql优化","date":"2021-11-16T13:23:11.000Z","updated":"2021-12-08T02:06:52.195Z","comments":true,"path":"2021/11/16/mysql-you-hua/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/mysql-you-hua/","excerpt":"","text":"MySQL优化Mysql的结构体系配置文件分析（基于Windows环境） 主配置文件：‪my.ini 二进制日志log-bin：存在于Mysql\\Data\\DESKTOP-bin.000377 错误日志log-error：存在于Mysql\\Data\\DESKTOP.err 查询日志log 数据文件 两系统 frm文件：存放表结构 myd文件：存放表数据 myi文件：存放表索引 Mysql架构图 1.客户端：各种语言都提供了连接mysql数据库的方法，比如jdbc、php、go等，可根据选择 的后端开发语言选择相应的方法或框架连接mysql 2.server层：包括连接器、查询缓存、分析器、优化器、执行器等，涵盖mysql的大多数核心服务功能，以及所有的内置函数（例如日期、世家、数 学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 3.存储引擎层（可插拔式）：负责数据的存储和提取，是真正与底层物理文件打交道的组件。 数据本质是存储在磁盘上的，通过特定的存储引擎对数据进行有组织的存放并根据业务需要对数据进行提取。存储引擎的架构模式是插件式的，支持Innodb，MyIASM、Memory等多个存储引擎。现在最常用的存储引擎是Innodb，它从mysql5.5.5版本开始成为了默认存储引擎。 4.物理文件层：存储数据库真正的表数据、日志等。物理文件包括：redolog、undolog、binlog、errorlog、querylog、slowlog、data、index等 执行流程 客户端要与数据库建立连接，需要通过连接器进行TCP握手以及身份校验，连接成功之后就可以操作数据库了 进行一条语句的查询，首先是查询缓存中是否存在，如果存在直接返回，如果不存在就通过分析器分析语法是否正确 语法分析完成之后就通过优化器进行优化，首先得判断是否有索引，有的话判断使用什么索引；其次就是对执行顺序进行优化 通过执行器进行SQL语句的执行 主流存储存储引擎的了解与区分 对比 MyISAM Innodb 主外键 不支持 支持 事务 不支持 支持 行锁与表锁 只支持表锁 支持行锁和表锁 缓存 只缓存索引，不缓存数据 索引和数据都缓存，内存要求高，内存大小影响性能 表空间 小 大 关注点 性能 事务 默认安装 Y Y 性能问题慢的原因？ 查询语句写的太烂 索引失效（单值索引、复合索引） 关联查询太多join（设计缺陷或不得已的需求） 服务器调优及各个参数设置 常见的join连接你知道几种？一共七中： 内连接：select * from tableA a inner join tableB b on a.id = b.id 左连接：select * from tbaleA a left join tableB b on a.id = b.id 右连接：select * from tbaleA a right join tableB b on a.id = b.id 去除公共部分的左连接：select * from tbaleA a left join tableB b on a.id = b.id where b.id is null; 去除公共部分的右连接：select * from tbaleA a rignt join tableB b on a.id = b.id where a.id is null; 全连接：select * from tbaleA a left join tableB b on a.id = b.id union select * from tbaleA a right join tableB b on a.id = b.id 去除公共部分的全连接：select * from tbaleA a left join tableB b on a.id = b.id where b.id is null union select * from tbaleA a right join tableB b on a.id = b.id where a.id is null Mysql索引结构B+索引数据结构：B+树的非叶子节点全部存储的是索引，而叶子节点存储的全部都是数据，这样相较于B树，B+树的每个节点可以存储更多的索引，树高会更低，而且B+树的叶子节点是由指针进行连接的，这样就可以实现范围查找 优点： 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。 因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。 主索引主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找，这个过程也被称作回表。 Hash索引哈希索引能以 O(1) 时间进行查找，但是失去了有序性： 无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找。 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 有序数组解决问题：hash索引是无序的，而且只支持等值查询的情况，那么如何解决呢？ 有序数组：不仅可以根据下标直接查询到数据，而且因为它是有序的，所以可以范围查找，但是同样具有缺点，那就是增加和删除的时候会改变当前的数据结构，会讲当前节点的后面节点全部左右移动，这样成本非常高。（这里又引申出hash索引可以增加删除方便，但是不能排序以及范围查询） 适用场景：历史数据，比如：历史订单，账单之类的信息 平衡二叉树平衡二叉树是有序的，所以支持范围查询，而且它的查询和更新复杂度为O(log(N))，但是同样是不支持来做索引的，因为平衡二叉树在数据很多的情况下，树会很高，这样就会造成查询的成本会很高 B树B树比平衡二叉树更优，因为在B树的一个节点可以存储多个数据，这样就可以使树更加的矮，但是B树相比较于B+树还是略显不足的 问题 B树的一个节点存储多大的数据最好呢？ 一页的大小，或者是页的整数倍最好，因为每次磁盘的读取都是读取一页的数据，如果小于一页就造成就会造成资源浪费，如果多余一页但是不是整数倍，也会多读一页，同样会造成资源浪费，因此选择读取的数据为页的整数倍最好 什么是回表？ 回表就是进行一个SQL语句查询的时候，这个语句例如：select * from user where name = ‘123’，这个表中存在ID主键索引和name字段索引，那么执行这个SQL语句的时候会先通过name索引找到值为123的Id，然后进行回表，通过id查询这条数据的信息 有什么方法解决回表呢？ 索引覆盖，意思就是将查询的返回结果换成ID，因为查询name索引的时候就会获取到ID，这样就不需要通过回表操作再次查询数据 什么是最左匹配原则 例如现在有四个索引，a,b,c,d,那么这个时候我的查询条件是a = 1,b =2,c &gt;3,d= 4，查询的结果列中只会显示a，b，c，因为c是范围查询，d就排不了序了 Mysql索引类型主键索引索引列中的值必须是唯一的，不允许有空值。 普通索引MySQL中基本索引类型，没有什么限制，允许在定义索引的列中插入重复值和空值。 唯一索引索引列中的值必须是唯一的，但是允许为空值。 全文索引只能在文本类型CHAR,VARCHAR,TEXT类型字段上创建全文索引。字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引。MyISAM和InnoDB中都可以使用全文索引。 空间索引MySQL在5.7之后的版本支持了空间索引，而且支持OpenGIS几何数据模型。MySQL在空间索引这方面遵循OpenGIS几何数据模型规则。 前缀索引在文本类型如CHAR,VARCHAR,TEXT类列上创建索引时，可以指定索引列的长度，但是数值类型不能指定。 其他（按照索引列数量分类） 单列索引 组合索引 组合索引的使用，需要遵循最左前缀匹配原则（最左匹配原则）。一般情况下在条件允许的情况下使用组合索引替代多个单列索引使用。 索引优化分析Explainexplain的作用参考：(https://www.cnblogs.com/yycc/p/7338894.html)、(https://blog.csdn.net/lvhaizhen/article/details/90763799) 通过explain+sql语句可以知道如下内容： 表的读取顺序。（对应id）：id越大越先执行，id一样顺序执行 数据读取操作的操作类型。（对应select_type）：普通查询、联合查询、子查询等复杂的查询 SIMPLE：简单的select查询，查询中不包含子查询或union查询。 PRIMARY：查询中若包含任何复杂的子部分，最外层查询为PRIMARY，也就是最后加载的就是PRIMARY。 SUBQUERY：在select或where列表中包含了子查询，就为被标记为SUBQUERY。 DERIVED：在from列表中包含的子查询会被标记为DERIVED(衍生)，MySQL会递归执行这些子查询，将结果放在临时表中。 UNION：若第二个select出现在union后，则被标记为UNION，若union包含在from子句的子查询中，外层select将被标记为DERIVED。 UNION RESULT：从union表获取结果的select。 type：表示查询所使用的访问类型，type的值主要有八种，该值表示查询的sql语句好坏，从最好到最差依次为：system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;ALL。 哪些索引可以使用。（对应possible_keys） 哪些索引被实际使用。（对应key） 表直接的引用。（对应ref）：const（常量）、外键 每张表有多少行被优化器查询。（对应rows） Extra Using filesort：Using filesort表明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。mysql中无法利用索引完成的排序操作称为“文件排序”。出现Using filesort就非常危险了，在数据量非常大的时候几乎“九死一生”。出现Using filesort尽快优化sql语句。 Using temporary 使用了临时表保存中间结果，常见于排序order by和分组查询group by。非常危险，“十死无生”，急需优化。 将tb_emp中name的索引先删除，出现如下图结果，非常烂，Using filesort和Using temporary，“十死无生”。 Using index 表明相应的select操作中使用了覆盖索引，避免访问表的额外数据行，效率不错。 如果同时出现了Using where，表明索引被用来执行索引键值的查找。（where deptid=1） 如果没有同时出现Using where，表明索引用来读取数据而非执行查找动作。 索引优化案例单表优化 通过这个案例，可以看出有两点需要优化，type为ALL是最坏的情况，Extra中出现filesort九死一生的情况，因此需要建立适当的索引进行优化 如何建立索引呢？ 观察：首先从查询条件入手，查询条件中存在，category_id、comments、views 现在考虑将这三个字段建立一个联合索引ccv 建完索引之后，进行查询，发现type已经优化成了range，但是filesort依然存在 为什么还是存在呢？ 看查询条件，因为我们建立的索引包含三个字段，但是在查询条件中出现comments&gt;1，这里是范围查询，那么在这个索引后面出现的views就相当于索引失效，无法通过comments找到views这个索引。因此对这个索引无法进行索引排序，这样就导致还是需要借助外部索引进行排序，也就是filesort。 如何解决？ 通过上述索引，因为中间的comments出现范围查询，导致索引失效，那么现在绕过comments来建立索引是否可以成功呢？ 建立联合索引category_id,views 问题解决！！！ 两张表优化两张表如何优化呢？ 首先考虑在查询两张表的时候，我们会使用左连接和右连接，那么在进行连接的时候对哪个表建立索引效果会更好呢？ 思考 左连接的特性是什么？ 左表全部都有，而右表只包含共有部分，反之就是右连接特性 根据这个特性思考，此时需要对这两个表建立索引，如果是左连接，对左表添加索引，那么是不是左表所有的数据都会使用，这时候type就会变成index，而对右表建立索引，只会查询存在对应关系的列，那么右表的type就是ref，这样是不是就知道两张表的时候对哪个表建立索引呢。 三张表优化join语句优化 尽可能减少join语句中的嵌套的循环总次数：“永远用小结果驱动大结果” 优先优化嵌套的内层循环 保证join语句中被驱动表上join条件字段建立了索引 当无法保证被驱动表的join条件字段被索引且内存资源充足的前提下，不要吝啬joinBuffer的设置 覆盖索引联合索引最左匹配原则索引下推唯一索引和普通索引的选择当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。 在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作，通过这种方式就能保证这个数据逻辑的正确性。 需要说明的是，虽然名字叫作change buffer，实际上它是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。 将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。 除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。 显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率 唯一索引一般一直使用内存操作数据，而普通索引会使用change buffer change buffer的使用场景因为merge的时候是真正进行数据更新的时刻，而change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好，这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价，所以，对于这种业务模式来说，change buffer反而起到了副作用。 查询截取分析查询优化优化原则：小表驱动大表当B表的数据小于A表的时候用in优于exists： select * from A where id in (select id from B) 等价于： for select id from B for select * from A where A.id = B.id 当A表的数据小于B表，用exists优于in： select * from A where exists (select 1 from B where B.id = A.id) 等价于: for select * from A for select * from B where B.id = A.id 对于order by 的优化前提：尽量使用index方式排序，避免使用FileSort进行排序 尽可能在索引列上完成排序操作，遵照索引建的最左匹配原则 如果不在索引列上，filesort有两种算法进行排序 双路排序：就是进行两次IO操作，得到最终数据。 流程：从磁盘读取排序字段，在sortbuffer中进行排序，然后再从磁盘取出其他字段信息 单路排序：主要针对的是sort_buffer进行的，如果此时取出的数据小于sortbuffer，那么只需要一次IO操作就能完成排序，但是如果现在数据量非常大，那么就需要多次的IO并且还会创建tmp文件进行多路合并操作，这样的话比双路排序更加的耗费资源。 针对上述所说的单路排序是在MySQL4之后所更新的，所以如果此时会进行多路合并操作，那么就需要扩大sortbuffer的大小，来提升性能。 最终优化方案 尽量遵守最左匹配原则 对于升序降序要一致 慢查询优化批量数据脚本Show Profile全局查询日志事务隔离级别与MVCCMVCC（Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用读已提交（READ COMMITTD）、可重复读（REPEATABLE READ）这两种隔离级别的事务在执行普通的SELECT操作时访问记录的版本链的过程，这样子可以使不同事务的读-写、写-读操作并发执行，从而提升系统性能。 这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，数据的可重复读其实就是ReadView的重复使用。 MVCC解决数据丢失问题MVCC，多版本的并发控制，Multi-Version Concurrency Control。 使用版本来控制并发情况下的数据问题，在B事务开始修改账户且事务未提交时，当A事务需要读取账户余额时，此时会读取到B事务修改操作之前的账户余额的副本数据，但是如果A事务需要修改账户余额数据就必须要等待B事务提交事务。MVCC使得数据库读不会对数据加锁，普通的SELECT请求不会加锁，提高了数据库的并发处理能力。借助MVCC，数据库可以实现READ COMMITTED，REPEATABLE READ等隔离级别，用户可以查看当前数据的前一个或者前几个历史版本，保证了ACID中I的特性（隔离性)。 Innodb是如何实现MVCC的undo log 不同的事务隔离级别产生的read view 的时期也不相同 Innodb是怎么存储MVCC数据的？InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现的。一个保存了行的事务ID（DB_TRX_ID），一个保存了行的回滚指针（DB_ROLL_PT）。每开始一个新的事务，都会自动递增产 生一个新的事务id。事务开始时刻的会把事务id放到当前事务影响的行事务id中，当查询时需要用当前事务id和每行记录的事务id进行比较。 Mysql语句的执行顺序开始-&gt;FROM子句-&gt;WHERE子句-&gt;GROUP BY子句-&gt;HAVING子句-&gt;ORDER BY子句-&gt;SELECT子句-&gt;LIMIT子句-&gt;最终结果 例子： select 考生姓名, max(总成绩) as max总成绩 from tb_Grade where 考生姓名 is not null group by 考生姓名 having max(总成绩) &gt; 600 order by max总成绩 在上面的示例中 SQL 语句的执行顺序如下: (1). 首先执行 FROM 子句, 从 tb_Grade 表组装数据源的数据 (2). 执行 WHERE 子句, 筛选 tb_Grade 表中所有数据不为 NULL 的数据 (3). 执行 GROUP BY 子句, 把 tb_Grade 表按 “学生姓名” 列进行分组(注：这一步开始才可以使用select中的别名，他返回的是一个游标，而不是一个表，所以在where中不可以使用select中的别名，而having却可以使用，感谢网友 zyt1369 提出这个问题) (4). 计算 max() 聚集函数, 按 “总成绩” 求出总成绩中最大的一些数值 (5). 执行 HAVING 子句, 筛选课程的总成绩大于 600 分的. (7). 执行 ORDER BY 子句, 把最后的结果按 “Max 成绩” 进行排序. 常见的索引失效场景 使用函数导致索引失效 错误的例子：select * from test where round(id)=10;如何解决？create index test_id_fbi_idx on test(round(id));然后 select * from test where round(id)=10; 这时函数索引起作用了 1,&lt;&gt; 2,单独的&gt;,&lt;,(有时会用到，有时不会) 对索引进行计算操作 错误的例子：select * from test where id-1=9; 或者在建立索引的时候用的是varchar但是查询的时候使用number类型 错误的例子：select * from test where tu_mdn=13333333333;正确的例子：select * from test where tu_mdn=’13333333333’; like “%_” 百分号在前. 解决：使用覆盖索引进行解决 单独引用复合索引里非第一位置的索引列.（最左匹配原则） 少用or，用or连接会导致索引失效 使用is null ，is not null 无法使用索引 使用！=或者&lt;&gt;时候，索引失效 使用范围查询，导致索引失效·","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://hwm156542114.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL优化","slug":"MySQL优化","permalink":"http://hwm156542114.github.io/tags/MySQL%E4%BC%98%E5%8C%96/"}]},{"title":"单例模式","slug":"单例模式","date":"2021-11-16T13:21:33.000Z","updated":"2021-12-08T02:09:23.861Z","comments":true,"path":"2021/11/16/dan-li-mo-shi/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/dan-li-mo-shi/","excerpt":"","text":"单例模式特点： 在JVM中，单例模式能保证对象实例只有一个 构造器必须是私有的，外部无法通过构造器创建对象实例 没有公开的set方法，外部不能通过set方法进行对象的创建 对外公开提供一个get方法，通过get方法可以获取对象实例 优点： 某些类创建非常繁琐，使用单例模式避免对象的频繁创建，而造成性能损耗 省去了new操作符，减轻系统内存使用频率，减轻GC的压力 避免对资源的重复占用 饿汉式单例模式public class Singleton{ private static Singleton instance = new Singleton(); private Singleton(){}; public static Singleton getInstance(){ return instance; } } 为什么说是饿汉式呢？ ​ 因为它在类加载的时候就直接把实例创建了出来，后面的方法直接进行返回即可 应用场景： ​ 适用于热点数据，在启动JVM的时候，就将热点数据创建好，这样也可以避免预热阶段，直接就可以使用了。 为什么是线程安全的？ ​ 因为类加载的方式是按需加载，且只加载一次，因此在访问单例对象的时候，其实单例对象的实例已经创建好了。因此饿汉式天生就是线程安全的。 懒汉式单例模式public class Singleton{ private static Singleton instance = null; private Singleton(){} public static Singleton getInstance(){ if(instance==null){ instance = new Singleton(); } return instance; } } 懒汉式单例模式是非线程安全的 为什么叫懒汉式呢？ ​ 因为类加载的时候，并不会直接将这个实例创建出来，而是在需要使用的时候进行创建 使用场景 ​ 适用于那些不热门的数据，因为只有在需要使用的时候进行调用并创建，这样就不会造成不必要的空间浪费 为什么是非线程安全的呢？ ​ 这里假设有两个线程，第一个线程和第二个线程现在都进行了if的判断，进入到了对象的创建，那么这两个线程就会创建两个对象。 那么针对这个懒汉式的非线程安全使用下面加锁的方式进行解决 懒汉式加锁public class Singleton{ private static Singleton instance = null; private Singleton(){} public static synchronized Singleton getInstance(){ if(instance ==null){ instance = new Singleton(); } return instance; } } 这样直接在获取对象实例的方法上加锁，虽然保证了线程的安全，但是这样加锁性能太低了，那么如何解决呢？？？ 懒汉式加锁进阶public class Singleton{ private static Singleton instance =null; private Singleton(){} public static Singleton getInstance(){ if(instance ==null){ synchronized(Singleton.class){ if(instance ==null){ instance = new Singleton(); } } } return instance; } } 这样把锁加在方法内，而且进行了一次判断，这样性能比上一个性能有提高，但是JVM对代码的运行会有运行优化，也就是在运行期间会进行指令重排操作，意思就是比如现在一个线程已经创建好了实例，而且JVM内存中也开辟了空间给这个实例，但是还没有初始化完成，而这个时候另一个线程判断instance不为空，那么直接返回进行调用，发现还没有实例化而出现错误，那么又如何解决这个问题呢？？？ 懒汉式加锁进阶加强(volatile)public class Singleton{ private static volatile Singleton instance = null; private Singleton(){} public static Singleton getInstance(){ if(instance ==null){ synchronized(Singleton.class){ if(instance==null){ instance = new Singleton(); } } } return instance; } } volatile主要是防止代码运行时的指令重排，可以保证，在创建实例完成，并且初始化之后，另外的线程才能对实例进行使用，那么这个已经很完美了，但是还有比这个更好的方式进行实现吗？？ 基于静态内部类实现的单例模式public class Singleton{ private Singleton(){} private static class SingletonFactory{ private static Singleton instance = new Singleton(); } public static Singleton getInstance(){ return SingletonFactory.instance; } } 基于枚举的单例模式class enum Singleton{ Instance; } 为什么使用单例模式而不使用静态类呢? 单例模式有懒汉模式，可以按需进行资源加载，而静态类会直接加载进内存中 静态类每次创建使用之后，都会被GC掉，而单例对象不会被GC","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://hwm156542114.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"单例模式","slug":"单例模式","permalink":"http://hwm156542114.github.io/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"}]},{"title":"项目复习","slug":"项目复习","date":"2021-11-16T13:20:18.000Z","updated":"2021-12-08T02:06:23.627Z","comments":true,"path":"2021/11/16/xiang-mu-fu-xi/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/xiang-mu-fu-xi/","excerpt":"","text":"项目总体流程通过Nginx搭建域名 首先使用Nginx搭建了一个域名访问环境。 为什么需要要使用nginx搭建这个域名访问呢？ 首先从正向代理和反向代理来看： 正向代理：就是客户端在访问某些网址的时候，通过代理服务器进行访问，这样就隐藏了自己的信息进行访问，保证客户端信息的安全 反向代理：就是搭建项目环境的时候，通过一个代理服务器，屏蔽内网的服务器信息，对外保暴露的是代理服务器的信息，这样别人通过访问代理服务器，然后代理服务器转发请求到内网服务器上进行访问 为什么使用Nginx搭配网关进行搭建？因为如果直接使用Nginx路由到端口上，那么如果在集群模式下，一个服务对应多个端口，就需要配置多个端口信息，这样很麻烦，所以通过网关进行路由，这样即使在集群环境下搭建的微服务，我们只需要路由到网管，然后让网关进行服务的调用即可 如何搭建的？ 首先安装Nginx，然后修改Nginx的配置文件，因为要映射到本机器上的网关上，所以首先配置一个上游服务器地址请求转发到网关上，配置了这个地址之后，再配置Nginx的代理地址，这个代理地址指向的是上游服务器的地址，那么通过这个代理地址，我们每次访问Nginx，都会访问它的80默认端口，然后Nginx会通过代理地址转发到上游服务器地址，再通过上游服务器地址访问网关。 遇到的问题：一开始在阿里云上部署的Nginx，然后通过本地的域名访问阿里云上的Nginx，发现Nginx一直无法访问本地服务器，一直报服务器错误，然后网上查了一下说，本地服务器需要备案才能访问内网，然后我去阿里云上看了一下需要备案，还要搞好多东西，我心里想着就是做一个项目，没想到这么麻烦，就没有备案，在网上查了好多方法，然后看到一个可以内网穿透的软件，通过那个软件可以代理一个内网穿透的地址，这个地址可以访问内网，然后我就花了十来块钱买了一个月，本来心里想着这下没问题了吧，但是后面使用动静分离那一块，一直出现静态资源无法访问，最终，没办法，就在本地安装了一个Nginx，好了一切问题都解决了，但是这个代理好像没什么太大意义！！！ 通过认证服务完成了社交登陆功能（验证码，接口防刷） 首先是注册功能，这里使用了阿里云的验证码进行验证，验证成功之后就可以进行注册了，但是因为验证码的条数是要钱的，所以如果一直被别人刷的话，那验证码的条数就浪费了，因此做了一个接口防刷的功能 如何实现接口防刷的？ 这里使用到了Redis，每次发送短信验证码请求的时候，就会将这个手机号和验证码绑定在一起，然后一起存到redis中，并且设置过期时间为一分钟，如果下次相同的手机号再次发送短信验证，首先会将这个手机号获取，然后通过手机号查询redis中是否存在该手机号，如果存在，那么就直接返回错误信息，如果没有，那么就输入验证码进行注册功能。 除了使用这个方法进行防刷，你还了解其他的吗？ 对单个手机号进行请求限制，如果这个手机号在规定时间段里发送的次数过多，就可以限制这个手机号此时间段内不能再发请求了 对单个Ip进行限制，这样虽然可以防止在一个ip地址下，多个手机号被刷的次数，但是同样的，因为一个小区可能都使用的一个ip地址，如果此时别人也在使用注册功能，那么将是一个很大的问题 网关进行控制，把当前时间段，同一个手机号多次恶意注册的请求拦截 如何实现密码的加密的？ 密码加密主要是保证用户信息的安全性，即使用户信息不小心暴露出去，也不能通过账号和验证码进行登录。实现密码的加密方式主要是使用MD5进行加密的，因为在网上看到了很多对MD5进行破解的软件，因此这里我使用了MD5盐值加密，使用这个会随机的在密码的基础上加一个字符串，这样就能保证密码永远对应的都是不同MD5值，有更高的安全性 社交登录功能 什么是OAuth2.0 OAuth2.0就是开放授权，意思就是用户可以通过给第三方授权，然后获取自己的相关信息，而不需要将用户的账户和密码提供给第三方网站。 说说OAuth2.0的工作原理？ 如何实现社交登陆的？ 这里首先是在微博的开放平台注册了一个个人应用，然后这个应用对应的域名就是本机服务的域名，注册成功。我们可以通过第三方的授权认证登录微博账户，然后通过这个微博注册的应用返回给我们该用户的相关信息，并且在登录的时候会判断这个用户信息是否存在，如果不存在则需要存储当前新用户，如果存在那么就登陆成功。 存储的数据具体流程 前端用户登录了第三方提供的登录地址，然后会返回一个code，然后通过code换取一个token，然后因为token过一段时间会变，所以得给token设置过期时间，然后不变的是uid，通过uid来识别不同的用户信息。 Session共享问题session的原理？ 首先用户第一次访问服务器进行登录，登陆成功之后，就会将用户信息保存在session中，然后session会将sessionId返回给浏览器，浏览器就会将这个sessionId存在cookie中，这样，下一次访问页面的时候会带上cookie进行访问。浏览器关闭，清除回话cookie，那么下一次再进来，就需要重新登录 session如果在集群环境下会出现什么问题？ 如果是在同一个域名，但是不同服务的情况下： 在这种情况下，如果浏览器第一次访问当了第一个服务器，那么会在该服务器存储session，并且返回sessionId存储在浏览器中，但是如果浏览器下一次访问的时候，通过负载均衡直接转到另一个服务器上，那么这个时候带上之前的cookie显然是不能进行访问的。 如果是在不同域名，不同服务下，session是不能共享的 如何解决这个session不能共享的问题呢？？？ 通过Spring Session解决了session不能共享的问题 是怎么解决的呢？ 首先导入SpringSession的依赖，然后这里在配置文件中指定存储类型为redis，然后写一个session配置类，主要用于放大作用域和解决序列化问题。解决的流程就是，将登录的用户的session通过redis进行保存，这样以后用户在访问页面的时候，不管访问哪个服务器，服务器都可以直接从redis中找出对应的session与之匹配，然后进行访问 SpringSession的核心原理是什么? ​ 它的核心原理有两个重要的点： 第一个：给容器中添加了一个SessionRepository的组件，这个组件主要的作用是让redis操作session，相当于redis的Dao 第二个：SessionRepositoryFilter，存储过滤器，这个过滤器在创建的时候就会自动的从容器中获取SessionRepository，最重要的就是它里面的doFilter方法，这个方法将原生的request和responce都包装成了Wrapper类型的request和response，然后后续我们的操作中获取session中的信息，都是从SessionRepository中获取而不是在原生的服务器中获取。 装饰者模式（责任链模式） 使用Redis做缓存提高性能 使用缓存存在哪些问题呢？ 缓存穿透 缓存击穿 缓存雪崩 分布式锁？怎么实现的？实现？先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。 当然，因为在设置锁和释放锁的过程中，可能此时服务崩溃，或者其他的一些原因导致系统无法运行，这时候可能无法释放锁，或者业务逻辑执行完成，但是锁没有释放，那么这时候就需要使用redis提供的lua脚本进行实现。 为什么使用分布式锁？解决了什么问题？在集群环境下，如果使用的都是本地锁，那么如果很多请求进来，每个服务都查询redis，里面都没有数据，那么他们就会针对每个服务都发送查询数据库的命令，这样就造成了很多不必要的查询，因此需要使用分布式锁，所有的请求进来，会首先抢占分布式锁，如果有一个线程抢占成功，那么就通过这一个线程来查询数据库，然后将查询到的数据存到redis中，这样就避免了多个线程，多次查询数据库的操作，造成不必要的资源浪费。 看门狗机制？如果我们设置了锁的过期时间，那么程序不管是否运行完成，时间一过，锁就过期了，那么别的线程就会抢占锁。但是如果我们不设置过期时间，那么就会触发看门狗机制，看门狗会默认的设置为30s，而且会有一个自动续期的功能，只要时间超过默认时间的三分之一，就会进行一次续期操作 接口幂等性 什么是接口幂等？ 接口幂等就是，比如我们在进行一个提交操作的时候，这时候如果因为网络延迟，我们点了多次提交，然后就会出现多次重复提交的问题，这样可能我们自身只需要一次提交，但是却提交了很多次，这就是接口幂等 哪些情况需要防止接口幂等 用户多次点击提交按钮 用户页面回退，再次点击提交 微服务互相调用，因为网络或者别的原因调用失败，使用feign的失败重试机制 其他业务… 怎么解决接口幂等性？ Token（令牌）机制 用户在提交请求的时候，首先会输入验证码，然后请求会将验证码一起携带传到服务器，服务器会根据传过来的验证码，和存在于redis中的验证码作比较，如果存在，那么就表示请求是第一次进来，如果不存在则表示请求是重复请求。 存在哪些危险性呢？ 验证完成之后，是先删除token再执行业务逻辑，还是先执行业务逻辑再删除呢？ 先删除token，可能业务还没有执行完成，但是服务器中断，导致这次请求调用失败 后删除token，可能业务完成了，但是token没来的及删除，服务器中断，导致下一次请求又会重新执行一次业务 最好是先删除token，这样即使服务调用失败，也不会造成什么损失 如何解决呢？ Token获取、比较、删除必须是原子性的，所以如果使用令牌机制，那么就可以选择redis中的lua脚本进行原子性操作，这样就不会出现上述问题了。 各种锁机制 数据库唯一索引约束 redis防重 全局请求唯一ID 多线程异步编排 初始化线程有哪几种方式？ 有四种方式 继承Thread类 实现Runnable接口 实现callable接口 线程池 说说Runnable和Callable的区别 1）Runnable提供run方法，无法通过throws抛出异常，所有CheckedException必须在run方法内部处理。Callable提供call方法，直接抛出Exception异常。 2）Runnable的run方法无返回值，Callable的call方法提供返回值用来表示任务运行的结果 3）Runnable可以作为Thread构造器的参数，通过开启新的线程来执行，也可以通过线程池来执行。而Callable只能通过线程池执行。 Callable任务通过线程池的submit方法提交。且submit方法返回Future对象，通过Future的get方法可以获得具体的计算结果。而且get是个阻塞的方法，如果任务未执行完，则一直等待。 Future和FutureTask的区别 对于Calleble来说，Future和FutureTask均可以用来获取任务执行结果，不过Future是个接口，FutureTask是Future的具体实现，而且FutureTask还间接实现了Runnable接口，也就是说FutureTask可以作为Runnable任务提交给线程池。 什么是异步任务？什么情况下使用异步任务？ 不等任务执行完，直接执行下一个任务。 ​ 按照上面程序的执行顺序，如果是同步任务的话，会依次按照ABC进行执行，但是AC运行的比较快，如果等待B完成之后再完成AC，那么就会使整个程序运行的非常慢，因此使用异步任务，不用等B执行完成，直接运行AC,从而提高效率 线程池的七大参数知道吗？ 核心线程数：就是线程池创建的时候，里面就有的线程数，称为核心线程数 最大线程数：如果此时阻塞队列满了，核心线程全部都在运行，那么就会开始最大线程数来执行任务 存活时间：当开启了最大线程来完成了自己的任务的时候，并且任务执行完了，此时除了核心线程外，其他的额外线程就会在规定的时间里进行销毁 时间单位：存活时间的单位 阻塞队列：当线程都在运行，此时还有任务进来的时候，任务不会被直接，而是加入到阻塞队列中 线程工厂：指定创建线程的工厂 饱和策略：当开启了最大线程数，且阻塞队列也满了，但是现在还有任务正在加入，那么就会执行饱和策略，默认的饱和策略就是拒绝策略 Abort策略：默认策略，新任务提交时直接抛出未检查的异常RejectedExecutionException，该异常可由调用者捕获。 CallerRuns策略：为调节机制，既不抛弃任务也不抛出异常，而是将某些任务回退到调用者。不会在线程池的线程中执行新的任务，而是在调用exector的线程中运行新的任务。 Discard策略：新提交的任务被抛弃。 DiscardOldest策略：队列的是“队头”的任务，然后尝试提交新的任务。（不适合工作队列为优先队列场景） 为什么使用线程池？ 在业务中， 你是怎么使用多线程异步任务来完成的？ 首先来看一下主要的方法 具体实现 分布式事务 分布式事务会存在那些问题？ 本地事务回滚，而远程服务不回滚 怎么解决？ CAP定理 面临问题现在互联网的集群规模越来越大，如果保证一致性，那么就要舍弃可用性，但是因为网络原因，或者节点故障而导致服务调用失败是常态，所以如果要保证可用性，就只能舍弃C而保证AP BASE理论 BASE理论是对CAP的一种延伸，意思就是即使无法做到强一致性，也可以做到弱一致性（最终一致性） 使用Seata解决分布式事务 消息队列 消息队列的应用场景 异步处理：比如一个注册功能，在注册的时候会首先将注册信息写入数据库，然后后面可能还需要发送邮件，发送信息等操作，这一系列操作都完成之后，才可以算真正的注册成功，但是这样非常的耗时，因此可以采用消息队列进行异步处理，也就是将用户的注册请求保存在消息队列中，然后直接让消息队列进行后续的异步处理逻辑 应用解耦：比如现在有两个微服务，他们之间需要一起调用实现一组操作，这个时候，可以通过消息中间件将他们分开，也就是解耦，让后面的服务不再依赖前面的服务，前面服务只需要将请求和数据存到消息队列中，而后面的服务只需要取出这些请求和数据进行相应的处理即可 流量控制：例如现在一个系统是百万级并发的秒杀系统，那么这时候可能会出现每秒百万的请求进来，这时候即使服务器能承受的了这么多的请求，但是等待处理的话，后续还有请求进来，将会一直阻塞，然后系统会崩溃，这时候就可以通过消息队列将这些请求全部存储起来，这样将请求的后续操作慢慢进行，就不会导致服务器崩溃的情况了。 消息队列的定义 RabbitMQ的概念： 首先会有一个生产者来生产消息 消息是由消息头和消息体组成的，这其中消息头中有一个重要属性：Route-Key：路由键 交换机用来接收生产者生产的消息的，并将消息路由给服务器中的队列 Broker消息中间键的服务器 Queue消息队列，用于存储消息的 VHost虚拟主机，主机之间互相隔离，互不影响 Consumer消费者 具体工作流程： ​ 首先生产者生产信息，然后将这些信息发送到消息服务器中，服务器会根据消息找到指定的交换机，然后交换机会通过路由键找到指定的队列，通过交换机与队列的绑定关系，将信息发送到队列中，然后由消费者通过与服务器建立连接，通过信道进行信息的传输 交换机类型 消息确认机制 p-&gt;b e-&gt;q q-&gt;c 延时队列 使用场景 下单成功之后，但是一直没有完成支付，那么在规定的时间内，如果一直没完成，那么时间结束，就会自动关单，并且将锁定的库存释放","categories":[{"name":"项目","slug":"项目","permalink":"http://hwm156542114.github.io/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"电商项目","slug":"电商项目","permalink":"http://hwm156542114.github.io/tags/%E7%94%B5%E5%95%86%E9%A1%B9%E7%9B%AE/"}]},{"title":"JAVA复习面试","slug":"JAVA复习面试","date":"2021-11-16T13:18:59.000Z","updated":"2021-12-08T02:06:39.889Z","comments":true,"path":"2021/11/16/java-fu-xi-mian-shi/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/java-fu-xi-mian-shi/","excerpt":"","text":"Java基础复习 Java和c++的区别： 首先是都是面向对象的编程语言 其次Java是没有指针，对内存管理更加安全 Java是单继承的，c++是多继承的，但是Java的接口是多继承的 Java有自动的垃圾回收机制，无需手动释放内存 Java只支持方法的重载，但是c++支持方法重载和操作符的重载 import java 和javax有什么区别： ​ 没有本质的区别，因为当时javax是java api的扩展包，随着时间的推移，javax的api包，逐渐称为java扩展api的一部分，但是合并javax包到Java包中太麻烦，所以将javax作为Java标准api的一部分 字符型常量和字符串常量的区别： 一个是单引号引用，一个是双引号引用，长度可以为0 字符型常量相当于一个ascll值，可以参与运算，但是字符串常量代表的是一个地址值(存放在内存中的地址值) 字符型常量的长度是2个字节，另一个占若干个字符 java中的注释有几种： 单行注释 多行注释 文本注释 continue、break、和 return 的区别是什么？ 跳出当前循环，下面的循环继续进行 跳出整个循环体 返回 ： 返回一个特定的值 直接返回，相当于方法的结束 == 和equals的区别： ==对于基本数据类型是值的比较，对于引用类型，比较的是引用比较，equals是引用比较，但是有的类重写了equals方法，例如String，Interger将引用比较变成了值比较 为什么重写equals就必须重写hashcode？ ​ 因为如果两个对象的hashcode的值相等，但是两个对象不是相等的，那么他们存储就会存在问题，所以重写equals也必须重写hashcode的算法来实现 例子：现在我们new两个对象，这两个对象都是自定义的相同的属性，如果这里我们不重写equals方法和hashcode方法，那么我们会发现计算这两个hashcode的值是不同的，但是实际上属性是相同的，那么存储到hashmap中，就会发现存储了两个一样的对象，因此在重写了equals方法之后，必须重写hashcode的计算方法，否则会出现存储的元素重复情况。 介绍一下hashcode ​ hashcode（）这个方法是返回一个int类型的值，这个值对应的就是对象存储在散列表中的位置 为什么要有hashcode ​ 因为在散列表中存储对象的时候，如果没有hashcode，那么就以hashset为例子，我们需要对每个地址中的对象都进行equals比较，这样耗费太多的时间与性能，所以用hashcode就很好的解决了这个问题，我们首先在存储对象的时候，可以先根据这个对象所算出的散列值在散列表中查找，如果已经存在那么就不用再进行重复插入了，这个值已经存在，如果没有那么就可以进行插入。 为什么两个对象有相同的hashcode值，但是他们却不相等 ​ 因为这个涉及到hash算法的问题，越简单的算法就代表着重复的可能性会越高，所以不可避免的就是hashcode可能一样，但是对象不一样，以hashset为例子，在存储对象的时候，如果两个对象的hashcode值是相同的，这时候会用equals进行比较，如果相同那么就不会进行插入，如果不相同就会散列到其他位置 Java中有哪几种数据类型： ​ 有八种数据类型： ​ 数字类型：byte、short、int、long、float、double ​ 字符类型：char ​ 布尔类型：Boolean 自动装箱与拆箱 ​ 装箱：将基本数据类型包装成对应的引用类型 ​ 实质上：装箱就是用了valueof（）方法 ​ 拆箱：将引用类型转换为基本数据类型 ​ 实质上：拆箱就是用了***value（）方法 什么是方法的返回值 ​ 就是一个方法执行后，得到的结果 方法有哪几种类型 无参无返回值 无参有返回值 有参无返回值 有参有返回值 在静态方法内调用一个非静态成员为什么是违法的 ​ 因为静态方法是属于类的，在类加载的时候，就会分配内存给静态方法，因此静态方法可以用过类来调用，但是非静态成员是属于对象的，只有当对象创建的时候，非静态成员才会被创建，因此在类加载的时候调用内存中不存在的非静态成员是违法的。 静态方法和实例方法有什么不同 ​ 这里有两处不同： 首先是方法的调用方式不同： 静态方法可以通过类名.方法名或者对象.方法名进行调用，但是实例方法只能是对象.方法名，但是为了不混淆，静态方法还是使用类型.方法名进行调用 成员变量访问限制： 静态方法只能访问静态变量，但是实例方法都可以访问 Java是按值传递的，不是按引用传递的 一个方法不能改变基本类型参数的值 一个方法可以改变对象的状态，但是不能让对象参数引用另一个对象 重写和重载： 重载： 方法的重载一般是发生在一个类中，重载的方法必须有着相同的方法名，但是参数类型的数量、顺序、方法返回值和访问修饰符都可以不同 编译器在进行重载方法的匹配时，叫做重载解析 重写： 发生在父类和子类中，方法重写返回值类型、方法名以及参数都必须一致 这里返回值类型可以小于等于父类的返回值类型 抛出的异常可以小于等于返回值类型 访问修饰符要大于等于父类的访问修饰符 深拷贝和浅拷贝： 浅拷贝：就是增加了一个新的指针，指向了原来的内存地址，当原地址发生变化，新的指针也跟着变化 深拷贝：也是增加了一个新指针，但是指向了一个新的内存地址，当原地址发生改变，新指针不会发生变化 面向对象和面向过程的区别 面向对象：面向对象没有面向过程的性能高，但是由于面向对象有着封装、继承、多态这些特性，所以有着易维护、易扩展、易复用的特点 面向过程：面向过程性能比面向对象高，但是类调用需要实例化，开销比较大。 成员变量和局部变量的区别： 成员变量可以被访问修饰符所修饰，但是局部变量不能，不过这里特殊情况就是他们都能被final所修饰 成员变量随着对象创建而产生，局部变量随着方法调用而消失 成员变量在对象实例化的时候会被赋给类型的默认值，但是局部变量不行 如果有static修饰符，那么成员变量时属于类的，但是如果成员变量没有static修饰符，就是数据实例化对象的。对象存储在堆中，而局部变量存储在栈中 创建一个对象用new运算符 对象实例存在与堆内存中，对象引用存在与栈内存中 对象的相等和指向他们引用的相等有什么区别 ​ 对象的相等指的是对象的值相等，而对象引用的相等时指向他们内存的地址相等 一个类的构造方法有什么作用？如果没有构造方法程序是否能正确执行，为什么 完成类的初始化工作 没有构造方法程序时可以执行的，因为即使类中没有声明构造方法，也会有默认的无参构造方法，如果有我们创建的构造方法，那么就必须使用我们自己创建的构造方法来初始化一个对象、 构造方法有哪些特点，可以被override吗？ 构造方法的名字必须和类名相同 构造方法没有返回值，不能用void声明构造函数 构造方法自动执行，无需调用 构造方法不能被override，但是可以被重载，一个类中可以有多个构造方法 面向对象的三大特征： 封装：就是将对象中的属性，全部隐藏在对象内部，外部对象不能直接操作对象内部的属性，只能通过对象内部提供的方法来操作属性 继承：通过已有的类，创建出新类，也就是子类继承父类，子类拥有父类的所有属性和方法，并且子类可以重写父类的方法，以及可以定义自己的属性及方法，这里子类拥有父类的所有属性，但是不能直接访问父类的属性，只是拥有 多态：就是继承了父类或者接口的类，以多种不同的形态进行展示 String为什么是不可变的？ ​ 因为String类中，是以final来定义字符串存储的数组，所以是不可变的 ​ Java9之后，就改变成byte数组来进行存储字符串 StringBuffer和StringBuilder以及String的区别： StringBuffer和StringBuilder都继承了abstractStringBuilder类，在abstractStringBuilder中，保存字符串的修饰符并没有final字段进行限制 String中的对象是不可变的，所以可以理解为常量，线程安全，StringBuffer中的方法都加了同步锁，线程也是安全的，但是StringBuilder没有同步锁，所以线程是不安全的，但是StringBuilder的性能比StringBuffer的性能高处10％-15％ 综上所述： 操作少量数据用String 单线程下，操作大量数据使用StringBuilder 多线程下，操作大量数据使用StringBuffer 何为反射？ 反射就是在运行期间，我们拥有执行类的方法和分析类的能力 反射可以获取类的所有属性和方法，并且可以调用执行方法 反射机制的优缺点： 优点：可以让代码更加灵活，为各种框架提供开箱即用的便利性 缺点：让我们在运行期间拥有分析类和执行类的能力，这也造成了安全问题，性能也会有一定的影响 什么事序列化，什么是反序列化？ 序列化：就是将要持久化的对象转换成机器能识别的二进制字节流的过程 反序列化：就是将二进制字节流转换成我们的对象 final关键字总结： final关键字意为最终的，不可修改的，可以用来修饰类，方法以及变量 用来修饰类：那么类不能被继承，而且这个类的所有成员方法都被隐式的用final修饰了 用来修饰方法：那么这个方法不能被重写 用来修饰变量：如果是基本类型，那么初始化后就不能被修改，如果是引用类型，那么在初始化之后，就不能指向别的对象 代理模式静态代理​ 类中的所有接口都需要实现，一旦接口发生了改变，那么代理对象的方法也必须重写，因此会很麻烦 动态代理​ 动态代理，不需要实现所有的接口，可以针对我们需要代理的对象，然后实现其方法 对比：动态代理比静态代理更加灵活，静态代理在编译的时候就将接口，实现类以及代理类变成了.class文件，而动态代理是在运行期将代理类，接口实现变成二进制字节码加载进jvm中 集合 集合存放单一元素：Collection接口 collection接口List ArrayList arrayList：object[]数组 怎么进行初始化的？ 如果是默认无参构造，那么就会置为一个空数组 如果是有参，参数是大小，那么就会根据这个大小进行初始化数组的大小 如果参数是集合，首先判断传过来的集合大小是否为空，空则置为空数组，不为空就进行数组拷贝，数组大小就是集合传过来的数据大小 怎么进行增删改查的？（扩容根据数据添加来讲解） 增：首先判断数组是否为空，如果为空就将数组设为默认的大小10，然后进入到扩容判断，扩容判断是根据当前数组的大小和元素添加进数组之后的大小进行比较，如果大于那么就需要进行扩容，创建一个新数组，数组长度是原来数组的1.5倍，然后进行数组拷贝。 删：根据下标进行删除 改：set方法，首先判断下标是否越界，然后将旧值记录，将新值写入，返回旧值 查：通过下标进行查找 如何解决线程安全？线程不安全表现？ 线程不安全表现： 输出值为null; 数组越界异常; 某些线程没有输出值; public static void main(String[] args) throws InterruptedException {List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i =1; i&lt;=30 ; i++) { new Thread(() -&gt; { list.add(\"a\"); list.add(\"b\"); list.add(\"c\"); list.add(\"d\"); System.out.println(list.toString()); }).start(); } } 解决办法：Vector、Collections.synchronizedList()、CopyOnWriteArrayList Vector在方法上添加了synchronize的锁，对代码进行加锁，力度大，所以代码执行效率低下 CopyOnWrite容器即写时复制的容器。往一个容器添加元索的时候，不直接往当前容器Object[]添加，而是先将当前容器Object[]进行Copy,复制出一个新的容器object[] newElements,然后往新的容器object[] newElements 里添加元素，添加完元素之后，再将原容器的引用指向新的容器setArray(newElements);。 这样做的好处是可以CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite 容器也是一种读写分离的思想，读和写不同的容器 频繁扩容带来的影响？如何解决？ 添加效率低，可以通过初始化的时候设置容量大小，不过这样浪费空间。以空间换时间 LinkedList：jdk1.6之前是循环链表，之后变成了双向链表 如何进行增删改查的？ 增：可以通过头插法和尾插法进行（中间插入：通过获取当前要插入节点的前面一个节点，然后通过指针进行添加），主要设置前驱节点后后驱节点的指向 删：删除头结点或者尾节点 改：首先查找要修改的节点，进行遍历找到，通过旧值返回，新值覆盖的操作进行修改。 查：通过下标索引进行查找，主要是遍历整个链表进行数据的返回 线程安全？如何解决？什么情况下出现不安全？ 线程是不安全的 解决：Collections.synchronizedList()、ConcurrentLinkedQueue Vector：object[]数组 线程安全！ set HashSet 底层用的hashMap存储数据 LinkHashSet：其实是hash set的子类，不过底层用的是LinkHashMap进行存储元素的 TreeSet：红黑树（自平衡的二叉树） LinkedHashSet TreeSet Queue queue PriorityQueue：object[]数组 ArraryQueue：object[]数组+双指针 dequeue：双端队列 怎么实现双端队列的? Map接口三大接口： hashTable hashMap SortedMap HashMap详解jdk1.7的hashmap： hashmap的构造方法，基本上都是初始化一些基本参数，比如默认的容量为16，默认的负载因子是0.75，还有阈值是通过计算得出的 为什么初始化的数组都是2的幂次 方便与运算 因为我们计算出来了hash值之后，还需要根据hash值计算出对应的数组下标，而这里的计算都是通过与运算进行的，所以都需要转换成二进制进行计算，如果此时数组的长度不是2的幂次，那么得到的与运算只是数组下标的部分值。 均匀分布 为2的幂次，length-1 为奇数，奇数的二进制最后一位是 1，这样便保证了 hash &amp;(length-1) 的最后一位可能为 0，也可能为 1（这取决于 h 的值），即 &amp; 运算后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性。 而如果 length 为奇数的话，很明显 length-1 为偶数，它的最后一位是 0，这样 hash &amp; (length-1) 的最后一位肯定为 0，即只能为偶数，这样任何 hash 值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间 为什么计算hashcode需要右移以及进行异或运算？ 用与运算计算完之后，会发现我们都是用低四位进行运算的，而高四位并没有进行运算，因此再进行了与运算之后，又将获取到的结果进行异或运算，这样高四位也得到了使用，这样就使散列更加均匀，减少了hash冲突的问题。 当我们存入的对象键是相同的，会进行怎么样的操作 会将原来的值进行覆盖 扩容机制： 在1.7的扩容机制是先判断是否扩容，再进行添加的，而在1.8之后，就是先插入后进行扩容的判断。 怎么进行扩容的？ ​ 1.7的扩容机制，首先判断size的大小是否超出阈值，如果超出了就会进行扩容，首先将数组的大小扩容到原来的两倍，也就是新创建一个数组，这个数组是原来的两倍，然后遍历这个数组中的链表，通过hashcode进行计算，是否需要rehash，如果得到的数组下标不变，那么就直接将这个链表转移到新的数组上，如果下标发生变化，就需要重新计算下标的值，然后将链表转移过去。 插入：jdk1.7的HashMap的插入，实际上就是根据key然后算出一个hashcode，然后根据hashcode的值来找到相应的地址，然后将对象插入进去，当然在插入的过程中是很有可能发生hash冲突的，因此解决hash冲突我们需要将此节点下加上链表，使用链表进行冲突元素的存储，而且存储冲突元素是根据头插法进行插入的。将冲突元素插入到头部之后，还需要将头节点移动到数组下表位置，否则遍历的时候找不到头结点的元素。 为什么使用头插法呢？ 因为头插法效率比尾插法高，因为使用头插法可以直接将当前要插入的元素的引用指向头结点就可以的，但是如果使用尾插法，那么我们还需要遍历整个链表，直到找到链表的尾部才能进行插入 get方法：首先是通过key值计算hash，然后通过hash计算出数组的下标，然后找到这个数组下表对应的key值所对应的元素 jdk1.8的hashMap· 为什么1.8中新加了红黑树代替了链表？ 因为对链表的添加很方便，但是遍历就会很麻烦，而红黑树对于添加和遍历都是差不多的，因此将红黑树代替了链表 为什么链表默认值大于等于8的时候变成红黑树，而链表默认值小于等于6的时候是链表，这两个临界值为什么不一样 因为当我们在这个临界值左右频繁的做增删操作的时候，如果这个临界值是一样的，那么就会导致频繁的在链表和红黑树之间进行转换，这样会严重的影响map的效率。 put方法： 首先根据key来计算hashcode，然后根据hashcode计算数组下标的位置，这里的计算也是同1.7一样，用与和异或来计算出来的，计算出来之后，就将这个元素插入进去，不过这里是尾插法，不再是1.7的头插法了；因为我们需要判断链表的长度，所以无论如何我们都需要对链表进行遍历，因此这里使用尾插法来插入元素，在插入过程中，依然会对这个键进行判断，如果重复则会进行覆盖。 线程安全问题Hashmap是线程不安全的 为什么不安全？ 插入的时候不安全，如果此时有两个线程进来进行插入操作，插入的对象计算出来的下标是一样的，那么如果此时线程一进行插入操作之后，紧接着线程二也进行插入操作，那么就会出现线程二插入的数据覆盖了线程一所插入的数据 针对1.7扩容会出现死循环问题，因为1.7中使用的是头插法，比如现在两个线程都检测到hashmap应该进行扩容操作，那么线程会同时进行扩容，因为是头插法，会将之前的数据倒序，这样如果第一个线程记录了头结点以及头结点的next节点，但是此时线程一被挂起，线程二进行执行，并且完成了扩容操作，那么线程一此时指向的节点就是扩容后链表的尾节点，那么进行遍历会将尾节点的next指针指向前一个节点，这样就形成了一个环 解决办法？ Hashtable：对put和get方法直接加了synchronized互斥锁，效率很低，不长用 ConcurrentHashMap Collections.synchronizedMap：也是加了synchronized LinkedHashMap详解: LinkedHashMap的底层实现因为它是继承HashMap，所以也是通过数组、链表和红黑树完成的，当然区别在于LinkedHashMap增加了两个指针，用于双向链表的维护。 添加元素操作 LinkedHashMap的插入操作其实是重写了HashMap的插入操作，只不过因为LinkedHashMap中有双向链表的操作，而HashMap中没有，因此在重写的方法中，添加了对双向链表的操作，首先将插入的元素new了一个新的Entry对象，然后将这个新的对象插入到链表中，然后通过后置的LinkNodeLast方法将两个指针指向链表的前置和后置 删除元素的操作 删除元素的操作，在LinkedHashMap中，其实也是重写了HashMap的remove方法，只不过这里还重写了一个AfterNodeRemove方法，这个方法就是在删除了节点之后的操作 主要的步骤： 首先定位要删除的节点位置 然后删除这个节点 之后删除双向链表的指向 访问顺序的维护 LinkedHashMap是按插入顺序维护链表的，不过我们可以在初始化的时候指定accessOrder为ture。这样就是使他按访问顺序维护链表，当我们访问一个节点的时候，就会将这个节点移到末尾。 ConcurrentHashmap详解：(https://blog.csdn.net/zycxnanwang/article/details/105424734)(https://blog.csdn.net/wangnanwlw/article/details/111587507)1.7ConcurrentHashmap数据结构：Segment数组加链表 怎么保证线程安全的？ 主要是通过获取Segment分段锁来保证线程安全的。 get操作 1.7的时候，get操作并没有加锁，因为它所有的共享变量都定义成volatile类型，保证了变量在线程间的可见性 put操作 1.7的时候，首先获取segment锁，然后判断是否扩容，再进行添加操作 1.8ConcurrentHashmap数据结构：数组+链表+红黑树 怎么保证线程安全的？ 初始化数组的时候怎么保证线程安全的？ 在JDK1.8中，初始化ConcurrentHashMap的时候这个Node[]数组是还未初始化的，会等到第一次put方法调用时才初始化，线程如果要进行初始化，首先会通过CAS操作将标志位置为-1，别的线程同时进来进行初始化的时候，如果标志位不为0，那么就会等待，进来的线程就会完成初始化操作，这样就保证了只有一个线程完成初始化工作。 put操作怎么保证线程安全的?putValue函数，首先调用spread函数，计算hash值，之后进入一个自旋循环过程，直到插入或替换成功，才会返回。如果table未被初始化，则调用initTable进行初始化。之后判断hash映射的位置是否为null,如果为null,直接通过CAS自旋操作，插入元素成功，则直接返回，如果映射的位置值为MOVED(-1),则直接去协助扩容，排除以上条件后，尝试对链头Node节点f加锁，加锁成功后，链表通过尾插遍历，进行插入或替换。红黑树通过查询遍历，进行插入或替换。之后如果当前链表节点数量大于阈值，则调用treeifyBin函数，转换为红黑树最后通过调用addCount,执行CAS操作，更新数组大小，并且判断是否需要进行扩容 扩容怎么保证线程安全？ 构建一个nextTable，大小为table的两倍。 把table的数据复制到nextTable中。 get怎么保证线程安全？ 判断table是否为空，如果为空，直接返回null； 首先计算hash值，定位到该table索引位置，如果是首节点符合就返回； 如果遇到扩容的时候，会调用标志正在扩容节点ForwardingNode的find方法，查找该节点，匹配就返回 hash值为负值表示正在扩容，这个时候查的是ForwardingNode的find方法来定位到nextTable（扩容新数组） eh=-1，说明该节点是一个ForwardingNode，正在迁移，此时调用ForwardingNode的find方法去nextTable里找。 eh=-2，说明该节点是一个TreeBin，此时调用TreeBin的find方法遍历红黑树，由于红黑树有可能正在旋转变色，所以find里会有读写锁。 以上都不符合的话，就往下遍历节点，匹配就返回，否则最后就返回null eh&gt;=0，说明该节点下挂的是一个链表，直接遍历该链表即可。 通过遍历链表或则树结构找到对应的节点，返回value值。 HashTable详解 底层数据结构：数组+链表 通过对方法加synchronize保证了线程安全，性能很差 初始化默认的容量为11，扩容机制就是2n+1 TreeMap底层实现：底层数据结构：红黑树 弄清楚了红黑树，基本上TreeMap就没有什么秘密了 红黑树详解： 定义：红黑树是一个自平衡的二叉树，一种高效的查找树，可以在O(logN）时间内完成增删查等操作。 性质： 节点是红色或者黑色 根是黑色 所有叶子都是黑色 每个红色节点必须有两个黑色子节点，并且不能出现连续的两个红色节点 从任意节点到叶子节点，包含的黑色节点都是相同的 插入： 删除： List、set、queue、Map的区别？ List、set、queue、map的区别： List：存储的数据是有序的，可重复的 set：存储的数据是无序的，不可重复的 queue：按照特定的顺序进行存储，数据可以重复 map：通过键值对进行存储，key只能是唯一的，value可以是重复的，他们都是无序的 并发基础 什么是进程？ 比如在电脑上运行一个程序，进程就是这个程序运行的基本单位，程序的一次运行就代表着进程的创建，运行到消亡的过程，因此进程是动态的。‘ 什么是线程线程是比进程更小的执行单位，一个进程中可以包含多个线程，与进程不同的是，同类的多个线程共享进程区的堆和方法区资源，而每个线程又拥有自己的程序计数器、虚拟机栈、本地方法栈。线程又被称为轻量级进程 对象的创建过程： 类加载检查：虚拟机遇到了new指令的时候，首先会检查这个指令的参数是否能在常量池中定位到这个类的引用，然后检查这个符号引用代表的类是否存在，如果不存在才会进行类加载过程 分配内存：在类加载完成之后，虚拟机就会分配一块堆的内存给这个对象。分配的方式有两种，这取决于Java堆是否规整，Java堆是否规整又取决于垃圾收集器是否带有压缩整理功能 指针碰撞：适用于堆内存规整的情况，用过的内存全部分配到一边，没用过的内存全部分配到另一边，并且中间有一个分界指针 空闲列表：适用于对内存不规整的情况，虚拟机通过维护一个列表来分配内存，它会找一个足够大的堆内存划分给对象实例 初始化零值 设置对象头 执行init方法 对象的访问定位 句柄通过句柄访问的话，首先在Java堆中就会分配一块内存用于存储句柄池，句柄池中存储着到对象实例和对象类型的指针；实例池存在于Java堆中，而对象类型则存储在方法区中 直接指针如果使用直接指针，那么reference指向的直接就是对象实例的数据，这时候我们就需要考虑如何存放数据类型的相关信息 程序计数器为什么是私有的？​ 主要是因为线程切换后，能正确的恢复到执行位置 ​ 程序计数器的作用： 1. 字节码解释器通过改变程序计数器来执行指令，从而实现代码的流程控制 2. 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而能在线程切换回来的时候知道上次运行到哪了 虚拟机栈和本地内存栈为什么是私有的？虚拟机栈：因为每个Java方法在执行的同时都会创建一个栈针用于存储局部变量表，操作数栈，常量池引用等信息，从方法的执行到完成对应着，栈针在虚拟机栈中的入栈和出栈操作 本地内存栈：和虚拟机栈的作用是相同的，区别在于本地内存栈是为Native方法服务的，而虚拟机栈是为Java方法服务的。 简单的介绍一下堆和方法区首先这两个都是线程的共享资源，堆是进程中最大的内存，主要用于存储新创建的对象，而方法区则用于存储已被加载的类信息、常量、静态变量等信息 并发和并行的区别并发：同一时间内多个任务同时进行 并行：单位时间内，多个任务同时进行 为什么要使用多线程呢？从计算机底层原理解释：线程也称之为轻量级进程，是程序的最小执行单位，线程的切换和调用的成本远远小于进程。然后现代的多核CPU的出现意味着多个线程可以同时运行，减少上下文切换的开销。 从互联网发展来看：现在的系统几乎都是要支持百万级甚至千万级的并发量，而多线程并发编程正是支持这些高并发的系统的基础，充分利用多线程就机制可以有效的提升高并发系统的性能 使用多线程可能带来什么问题内存泄露、死锁、线程不安全 线程的生命周期和状态初始化-&gt;运行-&gt;阻塞-&gt;等待-&gt;超时等待-&gt;终止 什么是上下文切换?上下文切换意思就是，保存当前线程运行的条件和状态，当下次切换到这个线程的时候，可以恢复上次的状态进行运行。 什么时候会发生上下文切换呢？ 主动让出了cpu。例如调用了sleep（）、wait（）方法 当前线程的时间片用完 调用阻塞类型的系统中断 什么是线程死锁？如何避免死锁死锁：多个线程同时被阻塞，他们中的一个或者多个都在等待某个资源的释放，因为这些线程被无限期的阻塞，导致程序无法正常终止。 产生死锁的必要条件： 互斥条件：该资源任意时刻只能被一个线程占用 请求与保持 不剥夺 循环等待 如何预防死锁呢？（破坏产生死锁的必要条件） 破坏请求与保持条件 破坏不剥夺条件 破坏循环等待条件 如何避免死锁？ 就是在分配资源的时候借助算法（银行家算法）来对资源分配进行计算，使其进入安全状态 安全状态：系统能够按照某种推进顺序，为每个进程分配所需的资源，直到进程所需的资源全部完成分配。 Java中是如何解决死锁问题的？ 出现死锁一般都是获取锁的顺序不一致，容易导致死锁状况 解决：可以让线程获取锁的顺序一致，可以减少死锁的情况 加锁时限 在尝试获取锁的时候加一个超时时间，这也就意味着在尝试获取锁的过程中若超过了这个时限该线程则放弃对该锁请求。若一个线程没有在给定的时限内成功获得所有需要的锁，则会进行回退并释放所有已经获得的锁，然后等待一段随机的时间再重试。这段随机的等待时间让其它线程有机会尝试获取相同的这些锁，并且让该应用在没有获得锁的时候可以继续运行 sleep和wait方法的异同 sleep没有释放锁，wait释放锁 它们俩都可以暂停线程，但是wait通常用于线程的交互/通信，sleep通常用于线程的暂停 sleep方法暂停之后可以让线程自动苏醒，但是wait方法不会，除非在wait方法上设置一个超时时间，那么时间超时后就会自动苏醒，但是如果不设置超时时间，那么就必须调用notify或者notifyall方法让线程苏醒 为什么不能直接调用run方法，而是需要调用start来执行run方法因为如果直接使用run方法，那么就会将这个线程当做为main方法下的线程来执行，并不是以线程的方式来执行。而使用start方法运行run方法的时候，首先线程会先进入就绪状态，当有时间片分配到当前线程的时候，才会运行run方法，这是线程的完整的执行流程。 对synchronize的理解 synchronize主要是解决多个线程之间访问资源的同步性 synchronize可以保证被它修饰的方法或者代码块只能被一个线程执行 synchronize在jdk6之前被称之为重量级锁，因为在6之前，synchronize是依靠操作系统的mutex lock进行实现的，这就相当于我们对线程的操作，都需要依靠操作系统来配合完成，会花费很长的时间成本。在6之后对synchronize进行了很大的优化，这就让原始的synchronize不再是重量级锁了，因为6之后加入了很多的新锁，比如首先是无锁状态，然后还有偏向锁，轻量级锁最后会变成重量级锁 自己是怎么使用synchronize？ 修饰实例方法：对当前对象的实例进行加锁 修饰静态方法：对当前类进行加锁 修饰代码块：给指定的对象加锁，给对象/类加锁 说说jdk6之后，对synchronize做了那些优化优化之后的synchronize，现在锁的状态主要有四种：无锁状态-&gt;偏向锁状态-&gt;轻量级锁状态-&gt;重量级锁状态，随着竞争的激烈，锁会逐渐升级，且锁只能升级成越来越重的锁，而不会降级，这种策略提高获取锁和释放锁的效率。 锁升级的过程？锁的状态：无锁-&gt;偏向锁-&gt;轻量级锁-&gt;重量级锁 偏向锁 当一个线程访问同步块的时候，会在对象头和栈针中记录锁偏向的线程ID，然后下一次进来的时候就不用通过CAS操作来加锁和解锁，只需要简单的验证一下MarkWord里面指向的是否为当前线程的偏向锁，如果成功就获取锁，如果失败则需要再测试一下MarkWord中偏向锁的标志位是否为1，如果是则尝试使用Cas将对象头的偏向锁指向当前线程，如果不是则尝试CAS竞争锁。 synchronize和reentrantlock的区别 两者都是可重入锁：可重入锁的意思就是当线程获取了一个对象的锁之后，锁没有被释放，但是还是可以再次获取这个锁，如果是不可重入锁那么就会造成死锁的状态，因为每个线程获取锁的时候，计数器会加1，而线程释放锁的时候计数器就-1也就是0才能释放锁。 synchronize是依赖JVM实现的，而reentrantlock是依赖于API实现的（jdk层面的实现，需要lock、unlock方法配合try/finally块来完成） reentrantlock比synchronize新增了三个功能 等待可中断：中断等待锁的机制，线程可以选择放弃等待改为做其他的事情 可实现公平锁：reentrantlock默认是非公平的，但是可以通过构造方法将它改成公平锁（先等待的线程先获得锁） 可实现选择性通知：一个Lock对象中可以创建多个condition对象，然后线程对象可以指定的注册在某个condition对象中，从而可以有选择性的进行线程的通知。（synchronize中Lock对象只有一个Condition对象，这样所有的线程对象都注册在这一个condition对象中，这样使得如果调用notifyall就会使所有的线程都进行了通知，会造成很大的效率问题） volatile关键字CPU缓存模型为什么要使用CPU高速缓存模型呢？ ​ 用CPU高速缓存模型是为了解决CPU处理速度和内存处理速度不对等的问题。 说说JMM（Java内存模型）volatile关键字除了防止JVM的指令重排，还保证了变量的可见性 并发编程的三个重要特征 原子性：一次或多次操作，要么全部执行成功，要么全部执行失败 可见性：当一个线程对共享变量进行了修改的时候，其他的线程都是可以立即看到修改后的值，volatile保证了共享变量了可见性 有序性：代码在执行的过程中有先后顺序，但是Java编译器在运行期间会进行优化，所以可能导致执行的顺序并不是编写代码时的顺序，volatile关键字可以防止指令重排保证代码执行的有序性 说说synchronize和volatile的区别 它们是互补的存在，而不是对立的存在 volatile是线程同步的轻量级实现，性能比synchronize好，但是volatile只能用于变量，而synchronize可以用于方法和代码块 synchronize可以保证数据的原子性和可见性，但是volatile只能保证数据的可见性 volatile主要解决的是变量在多个线程间的可见性，而synchronize主要解决的是多个线程之间访问资源的同步性 ThreadLocal简介：可以把ThreadLocal简单的比喻成数据盒子，这个数据盒子中存储着每个线程的私有数据，当线程访问这个ThreadLocal的时候，就会得到这个数据的副本。 ThreadLocal内存泄露问题怎么解决在ThreadLocalMap中，是以ThreadLocal为key的弱引用和强引用的value组成，如果ThreadLocal没有被强引用，在垃圾回收的时候会将key清除掉，而value还继续存在，这样就会形成一个键为null的entry对象， 如果不采取任何措施，那么value值将永远不会被Gc，这时候就会产生内存泄露的问题。ThreadLocal已经考虑了这种情况，在调用set、get方法的时候，ExpungeStaleEntry方法会自动清除掉键为null的entry对象。建议：使用完ThreadLocal对象最好是手动调用remove方法 线程池 为什么使用线程池 降低资源的消耗：重复利用已经创建的线程来降低线程创建和销毁带来的消耗 提高响应速度：当任务到达的时候，不用等待线程的创建，直接就可以运行 提高线程的可管理性 实现runnable接口和callable的区别 主要的区别在于：runnable接口不会有返回值或者抛出异常，而callable就有；首先runnable接口是从1.0就有的，而callable接口是从1.5开始的，callable的出现主要解决runnable无法解决的情况。 执行execute方法和submit方法的区别execute方法主要是用于提交没有返回值的任务，而submit主要是提交那些有返回值的任务，有返回值的任务可以通过Future类的get方法来获取返回值，如果调用get方法则会阻塞当前的任务（必须等任务完成） ThreadPoolExecutor构造函数的七大重要参数 corePoolSize：核心线程数，定义了最小可以同时运行的线程数 maximumPoolSize：当队列中存放的任务达到队列的容量的时候，就会将当前线程数变成最大线程数 workQueue：当任务进来的时候，如果线程数达到核心线程数，就会将新任务存储到队列中 keepAliveTime：当线程池中的线程数量大于核心线程数，这时候又没有新的任务提交，多出的线程数不会立即销毁，而是会等待一段时间之后再进行销毁 unit：keepAliveTime的时间单位 threadFactory：executor创建线程会用到 handler:饱和策略 AbortPolicy（拒绝策略）：当有新任务时，直接拒绝执行，并且报拒绝异常 CallerRunsPolicy：这个是调用自己的线程来执行任务，但是这个会延迟任务的提交速度，如果程序能等待这么久，或者你的每一个线程必须执行，那么就可以使用这种策略 DiscordPolicy：不执行，直接抛弃的策略 Discord01destPolicy：丢弃最早未处理的线程策略 创建线程池有哪几种方式？ 可以直接使用构造方法进行创建 可以使用executor的框架工具类executors进行创建 fixedThreadPool：该方法创建了一个固定线程数量的线程池。当任务线程进来之后，如果线程池有空闲，那么就执行该任务，如果线程池没有空闲，那么就会将任务存放到任务队列中 SingleThreadExecutor：该方法创建的是只有一个线程的线程池。若多余任务进来，那么就会将任务暂存到任务队列中，等线程池空闲之后，就将任务队列中的任务取出进行运行，这些任务的执行顺序遵循先进先出 CachedThreadPool：该任务创建了一个可以根据实际情况来改变线程数量的线程池。当多余任务进来时，如果线程池有空闲的线程就会复用空闲的线程，如果没有，那么就会新创建一个线程对当前任务进行执行 线程池原理分析​ 线程池到底是依据什么原理进行运行的呢？ ​ 首先当有任务进来的时候，先判断核心线程数是否满了，如果没满那么就创建线程对任务进行执行，如果满了那么就加入任务队列，然后当任务不断进来的时候，核心线程数满了，这时候要考虑任务队列是否满了，如果任务队列没满，就将新任务先添加到任务队列中，如果满了就将核心线程数变成最大线程数，这时候，如果线程数足够使用，那么就创建线程对任务执行，如果不够用加进任务队列，如果队列也满了，那么就会根据定制的饱和策略进行处理。 JUC中的原子类有哪几种？一共有四类 基本类型 AtomicInteger AtomicLong AtomicBoolean 数组类型 AtomicIntegerArray AtomicLongArray AtomicReferenceArray：引用类型数组原子类 引用类型 AtomicReference：引用类型 AtomicStampedReference：带有版本号的引用类型 AtomicMarkableReference：带有标记的引用类型 对象属性修改类型 AtomicIntegerFieldUpdater：原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicReferenceFieldUpdater：原子更新引用类型字段的更新器 JUC中Atomic原子类的总结什么是AQS？AQS是用来构建锁和同步器的框架 AQS原理分析AQS的核心思想就是，如果共享资源是空闲状态，那么就将当前请求资源的线程设置为有效工作线程。如果当前共享资源是被占有的状态，那么就会将这些获取不到锁的线程都加入到CLH队列中，这个队列有一套阻塞等待以及唤醒锁的机制。CLH队列是一个虚拟双向队列，这个队列遵从先进先出的原则。AQS使用原子操作，实现对值的修改。 AQS对资源共享的方式 Exclusive（独占）：只有一个线程能执行。这里分为两种情况：公平和非公平竞争 Share：可以多个线程同时执行 JVM有哪几种类加载器 简单介绍一下程序计数器程序计数器是线程私有的，每个线程都有一个程序计数器，它是随着线程的创建而创建，随着线程的销毁而销毁 它有两个作用： 字节码解释器通过改变程序计数器的值依次读取指令，从而实现代码的流程控制 在多线程情况下，记录上一次程序执行的地方，保证在上下文切换的时候可以回到上一次线程执行的位置。 简单说一下虚拟机栈和本地方法栈它们俩都属于线程私有的，生命周期和线程也是相同的。 虚拟机栈主要是Java方法的运行，而本地方法栈主要是Native方法服务。虚拟机栈在运行方法的时候，都会有一个栈针对应这个方法，被压入方法栈中，带待所有方法都被执行完成的时候，就会一一弹出。每个栈针中都会存在局部变量表、操作数栈、方法出口、动态连接等信息。栈都会出现OOM和StackOverFlowError错误 简单的说一下堆首先堆在1.8之前分为新生代、老年代和永久代，在1.8的时候就变成了新生代、老年代、元空间。而新生代又分为Eden区，Survivor区，survivor有两个区，一个是from 一个是to区。然后新生代是发生GC的频繁区域，而且它的大小大约占用整个新生区的三分之一。而且GC算法有四种：在新生区用的都是复制算法而在老年区用的都是标记清除和标记压缩算法 经常发生的错误：OOM 内存不足而出现的OOM GC时间太长出现的OOM 简单说一下方法区主要用于存储已经被虚拟机加载的类信息、常量、静态变量，即时编译器编译后的数据 方法区和永久代有什么区别？永久代是方法区的一种实现 为什么元空间替换永久代因为之前的永久代都是需要自定义大小的，内存很受限制，很容易出现内存不足的异常，为了解决这个问题，直接将元空间代替永久带，并且移入本地内存区，这样不管是类的加载还是数据的存放，都只受本地内存的影响，并且很少出现内存异常 空间分配担保什么是空间分配担保呢？？ ​ 首先在发生minorGc之前，虚拟机会检查老年代的剩余空间大小是否大于新生代的空间大小，如果成立那么就会进行一次minorGC，如果不成立那么就会判断虚拟机中的担保参数，如果设置了这个担保参数为ture，那么就会检查历代新生区中晋升对象的平均大小空间，如果小于老年代的剩余空间，那么就允许这次GC进行，如果小于那么就不允许这次的GC进行，而是进行FUllGC 有哪几种垃圾收集器 serial（串行）收集器：这个串行收集器是一个单线程收集器，它的单线程不仅仅在于收集垃圾的时候使用的是一个线程进行的，而是在垃圾回收期间暂停其他所有的线程进行垃圾回收的 优点：因为是单线程的垃圾收集器，它没有线程交互的额外开销，所以它是很高效的。 ParNew收集器：这个收集器就是serial的多线程版本，其他都和serial是一致的 parallelScanvege收集器：这个收集器和ParNew没什么区别，主要的区别是这个收集器更注重于吞吐量的提升，可以自定义参数，最大效率的利用了cpu 类加载过程(https://blog.csdn.net/m0_38075425/article/details/81627349) 加载：加载第一步主要完成下面三件事 通过类的全限定名来获取该类的二进制字节流 将这个字节流所代表的静态存储结构转换成方法区的运行时数据结构 在内存区生成一个代表此类的class对象，作为方法区这个类的各种数据访问入口 连接 验证 文件格式验证：验证字节流是否符合class文件格式的规范（例如：魔数、版本、常量池。。。） 元数据验证：主要是对类的元数据进行语意校验，保证不存在与Java语言规范相悖的元数据信息 字节码验证：是最复杂的一个阶段，主要目的是通过数据流分析和控制流分析，确定程序语意是否合法、符合逻辑 符号引用验证：确保解析动作能够正确执行 准备：这里的准备阶段就是将类的静态变量进行内存分配。这里进行内存分配的时候类变量都是初始值，而不是我们给定的值，只有在后面的初始化之后，才会加载给定的值 解析：这个阶段主要就是将常量池中的符号引用全部转换成直接饮用的过程，也就是得到类、字段、方法在内存中的地址或者偏移量 初始化：虚拟机执行字节码操作 卸载 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被 GC 垃圾回收机制https://blog.csdn.net/seriousboy/article/details/81913799 说说CMS垃圾回收机制 说说G1垃圾回收机制 区别？ JVM性能调优Mysql何为索引？有什么作用索引是一种快速查询和检索的数据结构。常见的索引结构：B树，B+树和hash 索引通俗易懂的解释：其实索引就相当于一本书的目录一样，有了索引我们就可以根据目录快速查询我们想看的数据，不然全面扫描会很慢 索引的优缺点 优点 使用检索可以大大的提升查询速度，这也是创建索引的最主要原因 通过创建唯一索引，可以保证数据库中的每一行数据的唯一性 缺点 创建索引和维护索引需要耗费很多时间。 增删改的时候，如果数据有索引，那么就会在操作的时候索引也需要动态修改，这样会降低sql的效率 索引需要物理空间的存储，这样也会耗费一些资源 问题：索引一定可以提高查询性能吗？ ​ 大多数情况下索引比全盘扫描的效率是更高的，但是如果数据库的数据量不是很大的话，那么查询效率也不会提高太多，反而会增加创建索引维护索引带来的资源消耗。 索引的底层数据结构hash表&amp;b+树因为hash表中存储的数据都是以键值对的格式，所以通过hash算法，我们可以很容易通过键来获取相应的value。但是同样的也存在hash冲突的问题，解决hash冲突的问题就是通过链地址的方法。 问题：既然hash索引的这么快，为什么MySQL不使用hash作为索引的数据结构呢？ 首先是因为hash冲突的原因，但是这不是根本原因 根本原因是：Hash索引不支持顺序和范围查询（例如：我们要对数据库的数据进行排序或者范围查询，那么将是很大的问题） B树&amp;B+树问题：它们俩有什么异同点？ b树的所有节点都存放键也存放data，但是b+树只在叶子节点存放key和data，其他的节点都只存放key。 b树的叶子节点都是独立的，但是b+树的叶子节点都有一条引用链指向相邻的叶子节点 b树的检索对范围内所有的节点的关键字都做二分查找，而b+树则是从根节点到叶子节点依次进行检索 索引类型主键索引数据表中主键使用的就是主键索引 二级索引定义：二级索引称为辅助索引，因为其叶子节点存储的数据是主键，我们可以通过二级索引来定义主键的位置 唯一索引：唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据，但是允许值为null，一张表可以创建多个唯一索引。建立唯一索引一般不是为了查询效率，而是保证数据的唯一性 普通索引：普通索引唯一作用就是提高查询速度，一张表中允许创建多个普通索引，并且允许数据重复和null 前缀索引：前缀索引只适用于字符类型的数据，前缀索引对文本的前几个字符创建索引，相比普通索引它的数据更小 全文索引：全文检索主要是检索大文件数据中的关键字 聚集索引和非聚集索引聚集索引索引结构和数据存放在一起的叫聚集索引，主键索引叫聚集索引 优点聚集索引的查询速度非常快 缺点 依赖有顺序的数据：因为B+树是多路平衡树，如果索引的数据不是有序的，那么在插入的时候就会进行排序，如果这个排序的是整型还好，如果不是整形像是字符串或者UUID之类的，排序比较难，这样在插入和查询到时候效率会大打折扣 更新代价大：因为如果数据修改的时候会影响到索引的修改，因为聚集索引的数据和索引都存在一起，修改的代价很大，所以一般主键索引都不允许修改主键 非聚集索引非聚集索引就是索引的数据和索引是分开存放的 二级索引属于非聚集索引 优点更新代价小于聚集索引，因为它的节点没有存放数据 缺点 依赖有序的数据（和聚集索引一样） 可能会二次查询（最大的缺点）：当找到索引所对应的指针或者主键后，可能会根据指针或者主键再进行一次查询 创建索引需要注意的事项 选择合适的字段创建索引 被频繁更新的字段应该谨慎建索引 尽可能建立联合索引而不是单列索引 避免冗余索引 考虑在字符串类型上使用前缀索引代替普通索引 Innodb和MyiSAM对比 Innodb支持行锁和表锁 MYISAM只支持表锁 Innodb支持事务 MYISAM不支持事务 Innodb支持外键 MYISAM不支持外键 Innodb支持数据恢复 MYISAM不支持系统崩溃后的数据恢复 Mysql Innodb是怎样实现ACID的 Innodb使用redo log 来保证事务的持久性 Innodb使用undo log 来保证事务的原子性 Innodb使用锁机制、MVCC 来保证事务的隔离性 保证了以上的事务性质，才能保证一致性 事务并发事务会存在哪些问题 脏读：当一个事务对数据进行修改的时候，这个数据还没有进行提交，但是另外一个事务又过来了，它查询得到的数据是没有提交的脏数据，这时候就发生了脏读的情况 丢失修改：意思就是当两个事务同时查询了一个数据，然后同时要对这个数据进行修改，但是后面的修改就会覆盖前面的修改，这就叫丢失修改 不可重复读：也比喻是两个事务，第一个事务读取到了数据之后，第二个事务也读取到了数据，这时候第二个事务对数据进行了修改，然后第一个事务发现前后得到的数据不一致，这种情况就是不可重读读 幻读：比喻两个事务，第一个事务读取了几条数据之后，第二个事务对数据进行了增加或者删除，这时候第一个事务发现前后得到的数据不一致，这个就叫幻读 不可重复读和幻读的区别： 不可重复读针对的是修改前后的数据，而幻读针对的是增加或者删除前后的数据 事务隔离级别有哪些？ 读取未提交 读取已提交 可重复读 可串行化 Redis简单介绍一下redis​ redis是c语言开发的数据库，它与别的数据库不同的是它的数据是存在内存中的，所以读写是非常快的，因此它经常用于缓存、分布式锁甚至是消息队列。 Spring框架 spring的模块有七种： IOC 什么是IOC？ IOC（控制反转），是一种设计思想，DI（依赖注入）是IOC的一种实现方法，之前是手动创建对象，由程序自己控制，现在是将对象的创建交给IOC容器来控制 使用IOC有什么好处呢？ 解耦，使对象之间的耦合关系变低 使用单例模式，减少内存开销，提高性能 只用去写Bean的实现，而不用具体去创建Bean的实现 什么是依赖注入？依赖注入是IOC的一种实现方式 依赖注入的方式有哪几种？三种方式： 接口注入 setter方法注入 构造器注入 IOC装配Bean的方式有哪几种？ XML配置文件 Java类 @Configuration 注解 @Autowire @Primary（首选注入bean） @Qualifier（按指定的bean名称进行注入） IOC的作用域 Singleton（单例） prototype（多例）：对象如果有多个状态，那么就是用多例 request session global session IOC的初始化过程 IOC怎么实现对象的创建和依赖管理 首先从加载Bean的配置信息到容器中，然后创建一个Bean定义的注册表，通过这个注册表实例化Bean，然后将实例化的Bean添加到Map缓存区中，供应用程序调用 Spring容器可以简单分成两种 BeanFactory：面向Spring ApplicationContext：面向使用者（主要包含一下两种常用的实现类） ClasspathXmlApplicationContext FileSystemXmlApplicationContext WebApplicationContext（主要用于Web应用） Bean的生命周期 大致可以分为： 实例化Bean：Ioc容器通过获取BeanDefinition对象中的信息进行实例化，实例化对象被包装在BeanWrapper对象中 设置对象属性（DI）：通过BeanWrapper提供的设置属性的接口完成属性依赖注入； 注入Aware接口（BeanFactoryAware， 可以用这个方式来获取其它 Bean，ApplicationContextAware）：Spring会检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给bean BeanPostProcessor：自定义的处理（分前置处理和后置处理） InitializingBean和init-method：执行我们自己定义的初始化方法 使用 destroy：bean的销毁 AOP 什么是AOP？AOP就是面向切面编程，主要使用动态代理的方式实现，将相同逻辑的重复代码横向抽取出来，使用动态代理技术将这些重复代码织入到目标对象方法中，实现和原来一样的功能。 为什么使用AOP？ 减少相同代码的冗余度，提高代码的可维护性 怎么使用AOP？ 配置切面 配置切入点 配置切入表达式 代理能干嘛？ 增强对象的行为，在调用对象方法的时候，拦截方法，对方法进行改造增强 什么是静态代理？什么是动态代理？它们有什么区别? 静态代理：由程序员创建或者工具生成代理类，也就是在程序运行之前，就已经确定了代理类和委托类之间的关系 优点：业务类只需要关注业务逻辑的本身。 缺点 代理对象一个接口只服务于一种类型对象，如果接口方法过多，还需要对每个方法进行代理，多了很多繁琐的业务 如果接口增加了方法，那么下面实现接口的所有代理类都会将这个方法进行重写，增加代码维护难度 动态代理：在程序运行期间，由JVM通过反射等机制动态生成，代理类和委托类是在运行期间确定关系的 优点：可以灵活的实现接口中的方法，而不是像静态代理一样全部都实现 Aop的动态代理有那些方式？ JDK动态代理 Cglib动态代理 AOP默认使用的是JDK动态代理，如果代理对象没有实现的接口那么就使用Cglib进行代理 JDK和Cglib这两种方式有什么区别？ JDK是基于接口进行代理的，而Cglib是基于父类进行代理的，如果被代理的对象没有实现的接口，那么就需要使用Cglib进行代理 怎么选择JDK和Cglib这两种代理，原因是什么？ 如果是单例模式就使用Cglib，如果是多例模式就使用JDK 原因：JDK创建对象的性能比Cglib高，而Cglib生成代理对象的性能比JDK高 Spring中AOP有哪几种实现方式？ 基于注解的方式@AspctJ 基于代理（自定义代码进行实现） 使用XML进行实现（POJO） Spring MVC SpringMVC原理是什么？ 简单原理： 详细原理： 流程讲解： 首先用户发送请求，然后请求进入到DispatcherServlet DispatcherServlet根据请求信息从HandlerMapping中找到对应的handler 找到之后，然后再请求handler适配器进行处理 handler适配器根据传过来的handler进行请求的处理 处理完成之后就会返回一个ModelAndView对象 View resolver会根据返回的ModelAndView中的View信息进行解析，并返回对应的View 然后将返回的Model数据添加进View中，进行视图渲染 最后返回给浏览器 循环依赖什么是循环依赖？怎么解决循环依赖呢？循环依赖就是比如有两个对象A和B，它们之间互相引用形成一个环，这样的情况叫做循环依赖 怎么解决呢？如果是通过构造器方式进行对象注入，那么是无法解决的，会直接报错，如果是setter方法进行注入的，那么就可以解决 为什么通过构造器创建对象就不能解决循环依赖呢？ 因为Spring解决循环依赖是依靠Bean的中间态进行解决的，中间态指的就是，对象已经实例化，但是还没有初始化。而构造器是直接初始化了，所以无法解决。 Spring采用三级缓存的方式进行处理循环依赖问题。 spring中使用了哪些设计模式 工厂模式：spring中的BeanFactory就是简单工厂模式的体现，根据传入唯一的标识来获得bean对象； 单例模式：提供了全局的访问点BeanFactory； 代理模式：AOP功能的原理就使用代理模式（1、JDK动态代理。2、CGLib字节码生成技术代理。） 装饰器模式：依赖注入就需要使用BeanWrapper； 观察者模式：spring中Observer模式常用的地方是listener的实现。如ApplicationListener。 策略模式：Bean的实例化的时候决定采用何种方式初始化bean实例（反射或者CGLIB动态字节码生成） 事务 Spring的事务的特性是什么？ACID Atomicity：原子性 Consistency：一致性 Isolation：隔离性 Isolation：持久性 事务的传播行为有哪些？ 七种行为： 支持当前事务的情况： propagation_requierd：如果当前没有事务，就创建一个事务，如果当前已经存在一个事务，那么就加入到这个事务中 propagation_supports：支持当前事务，如果当前没有事务，就以非事务方法执行 propagation_mandatory：使用当前事务，如果没有就抛出异常 不支持当前事务的情况： propagation_required_new：新建事务，如果当前存在事务，就将当前事务挂起 propagation_not_supported：以非事务方式执行，如果当前存在事务，就将当前事务挂起 propagation_never：以非事务方式执行，如果当前存在事务那么抛出异常 特殊情况： propagation_nested：如果当前存在事务就嵌入到该事务中，如果没有就执行require一样的操作 事务的隔离级别 读未提交 读已提交 可重复读 串行化 如果不设置事务，会出现哪些问题 脏读 不可重复读 幻读 事务的回滚机制 创建事务回滚就需要使用@transactional这个注解，在这个注解中有一个属性是rollbakfor，如果配置了这个属性，那么在运行期间如果报错，就会通过注解找到这儿自定义的回滚机制进行事务回滚 配置事务方式 编程式事务 编程式事务管理是侵入性事务管理，使用TransactionTemplate或者直接使用PlatformTransactionManager，对于编程式事务管理，Spring推荐使用TransactionTemplate。 声明式事务 声明式事务管理建立在AOP之上，其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，执行完目标方法之后根据执行的情况提交或者回滚。编程式事务每次实现都要单独实现，但业务量大功能复杂时，使用编程式事务无疑是痛苦的，而声明式事务不同，声明式事务属于无侵入式，不会影响业务逻辑的实现，只需要在配置文件中做相关的事务规则声明或者通过注解的方式，便可以将事务规则应用到业务逻辑中。显然声明式事务管理要优于编程式事务管理，这正是Spring倡导的非侵入式的编程方式。唯一不足的地方就是声明式事务管理的粒度是方法级别，而编程式事务管理是可以到代码块的，但是可以通过提取方法的方式完成声明式事务管理的配置。 事务超时 为了使一个应用程序很好地执行，它的事务不能运行太长时间。因此，声明式事务的下一个特性就是它的超时。 假设事务的运行时间变得格外的长，由于事务可能涉及对数据库的锁定，所以长时间运行的事务会不必要地占用数据库资源。这时就可以声明一个事务在特定秒数后自动回滚，不必等它自己结束。 由于超时时钟在一个事务启动的时候开始的，因此，只有对于那些具有可能启动一个新事务的传播行为（PROPAGATION_REQUIRES_NEW、PROPAGATION_REQUIRED、ROPAGATION_NESTED）的方法来说，声明事务超时才有意义。 回滚机制 在默认设置下，事务只在出现运行时异常（runtime exception）时回滚，而在出现受检查异常（checked exception）时不回滚（这一行为和EJB中的回滚行为是一致的）。不过，可以声明在出现特定受检查异常时像运行时异常一样回滚。同样，也可以声明一个事务在出现特定的异常时不回滚，即使特定的异常是运行时异常。 事务常用的参数 事物配置中有哪些属性可以配置?以下只是简单的使用参考 事务的传播性：@Transactional(propagation=Propagation.REQUIRED) 事务的隔离级别：@Transactional(isolation = Isolation.READ_UNCOMMITTED) 读取未提交数据(会出现脏读, 不可重复读) 基本不使用 只读：@Transactional(readOnly=true)该属性用于设置当前事务是否为只读事务，设置为true表示只读，false则表示可读写，默认值为false。 事务的超时性：@Transactional(timeout=30) 回滚：指定单一异常类：@Transactional(rollbackFor=RuntimeException.class)指定多个异常类：@Transactional(rollbackFor={RuntimeException.class, Exception.class}) 该属性用于设置需要进行回滚的异常类数组，当方法中抛出指定异常数组中的异常时，则进行事务回滚。 Mybatis框架架构图 什么是JDBCJava DataBase Connection，意思就是使用Java语言操作数据库 什么是ORMORM就是持久层，意思就是将数据存储到硬盘 什么是Mybatis就是优秀的ORM框架 Mybatis和JDBC什么关系 如何防止SQL注入？ #{}或者${}，前者就是相当于一个占位符，后者就是将数据直接拼接到SQL后面，这样就可以防止SQL注入，需要注意的是使用$必须要添加@param这个注解 Netty什么是BIO、NIO、AIO？ BIO：同步阻塞I/O模式，保证读写都是又一个线程来完成的，在不高的并发量下是没有问题的，但是如果是百万级并发量，那么BIO会显得很无力 NIO：同步非阻塞I/O模式，这个相较于传统的BIO就有了很大的提升，主要是它支持面向缓冲，基于通道的I/O实现方式，可以适用于高并发的情景 AIO：异步非阻塞I/O模式，这种模式主要是有回调机制的实现，在应用操作之后就会直接返回给用户信息，然后后台再通知相应的线程完成剩余的事情。 什么是Netty？ Netty是基于NIO的一种客户端服务器的框架 它极大的优化了TCP和UDP套接字服务器的性能，并且安全性也有很高的提升 支持多种协议（FTP、HTTP、SMTP） 面试题 什么是面向对象？ 面向对象就是我们在设计任何东西的时候，都是以角色进行划分的，而不是关心其过程，如果关心其过程那么就是面向过程编程。面向对象易于维护、复用和扩展。 面向对象的三大特性：封装、继承、多态 如何选用集合？ 如果我们要通过键值来获取数据的时候那么就选用map 如果需要排序就选择TreeMap 如果不需要排序就选择HashMap 如果需要保证线程安全就选择ConcurrentHashMap 如果我们只存储元素值的时候就选择Collection集合 如果保证元素唯一就选择Set 不需要就选择Arraylist或者LinkedList 为什么要使用集合？ 因为如果在实际开发中使用数组会有很多的局限性，比如数组一旦定义，那么长度和类型就不能改变，而且存储的类型是可重复、单一的，因此选择集合就能充分解决上述所有的问题，集合是非常灵活多变的。 ArrayList和Vector有什么区别？ 他们底层都是用数组实现的，但是ArrayList线程不安全，而Vector线程安全 ArrayList和LinkedList的区别 它们都是不同步的，也就是它们线程都是不安全的 底层数据结构不同，ArrayList的底层数据结构是数组，而LinkedList底层数据结构是双向链表，在1.6之前是循环链表 插入和删除： ArrayList因为底层数据结构是数组，所以在查找元素的时候很快，但是在插入和删除元素的时候，复杂度为O（n-i）；LinkedList底层数据结构是双向链表，双向链表对于头尾插入和删除都是非常方便的，但是如果对一个指定的位置进行插入和删除，那么时间复杂度就为O（n） 是否支持快速访问 ArrayList支持快速访问，LinedList不支持 内存空间占用 ArrayList的空间浪费，就是List的初始化都会预留一定的空间；而LinkedList的空间浪费是在存储每一个节点的时候，会有多出的空间来存储这个节点的指针。 无序性和不可重复性的含义： 无序性：意思就是存储在底层的数据结构不是按数组索引存储的，而是按照哈希值决定的 不可重复性：不可重复的意思就是指添加元素是按equals进行判断的，并且判断结果为false才进行存储，需要同时重写equals和hashcode方法 比较HashSet、LinkedHashSet、TreeSet的异同 HashSet是Set的实现类，HashSet底层是HashMap，线程不安全，可以存储null值 LinkedHashSet是HashSet的子类，可以按照添加顺序进行遍历 TreeSet底层是红黑树，元素是有序的，排序的方法有自然排序和定制排序 Queue和Deque的区别 Queue： 是单端队列，只能从一端插入元素，另一端删除元素，遵循先进先出原则 Queue扩展了Collection接口因容量问题而导致操作失败后处理问题的方式不同： 操作失败后抛出异常 返回特殊值 Deque Deque是双端队列，可以在两端进行插入和删除 Deque扩展了Queue接口，增加了在队头队尾的增加和删除功能，失败后处理的方式不同，分为两类：抛出异常、返回特殊值 ArrayDeque和LinkdeList的区别：它们都实现了Deque，都具有队列功能，但是有什么区别呢？ ArrayDeque是通过可变长的数组和双指针实现的，而LinkedList是通过链表实现的 ArrayQueue不支持存储null值，而LinkedList支持 ArrayQueue是在1.6引入的，而LinkedList是1.2就已经引入了 ArrayDeque插入时可能就存在扩容，但是均摊后，插入的时间依然为O(1)，LinkedList虽然不存在扩容，但是每插入一个都需要申请新的存储空间，均摊性比较差 HashMap和HashTable的区别 HashMap的线程是不安全的，而HashTable是线程安全的，因为HashTable方法基本都经过了synchronized修饰，如果要保证线程安全，那么可以使用ConcurrentHashMap HashMap的效率比HashTable的效率，而且HashTable现在几乎不用了 HashMap可以存储null值和null键，但是HashTable不行，如果插入则会报空指针异常 如果没有指定容量的默认值，那么HashMap的默认容量是16，而HashTable的容量是11，而且每次扩容HashMap都是原有的两倍，而HashTable是原来的2n+1。 底层数据结构：HashMap在1.8以后有很大的改动，首先是扩容机制，之前的扩容就是数组的扩容，但是1。8之后，首先是判断数组的长度是否＜64，如果＜那么就会先将数组扩容，而不是将链表变成红黑树，如果数组长度大于64，而且链表的长度大于8，那么就会将链表转换成红黑树 HashMap和HashSet的区别 HashMap实现了Map接口，HashSet实现了Set接口 HashMap存储的是键值对，HashSet存储的是对象 HashMap调用put进行添加，HashSet调用add进行添加 HashMap的hashcode是通过键来计算的，HashSet是通过对象进行计算的，如果两个对象的hashCode值一样，那么就会进行equals判断 HashMap和TreeMap的区别 TreeMap主要是多了对集合中元素的排序功能和对元素的搜索功能 HashMap常见的遍历方式有哪几种？ 使用iterator EntrySet进行遍历 使用iterator KeySet进行遍历 使用for each EntrySet进行遍历 使用for each KeySet进行遍历 使用Lambda表达式进行遍历 使用Streams API 单线程方式遍历 使用Streams API多线程方式遍历 性能比较：EntrySet最快，KeySet最慢 删除数据的安全性：用map.remove()是不安全的，用iterator.remove()是安全的， 总结：不管是遍历还是删除数据，建议都使用Iterator的EntrySet来操作 ConcurrentHashMap和HashTable的区别 主要的区别：实现线程的安全的方式不同 ConcurrentHashMap的数据结构1.7是分段数组+链表，1.8是数组+链表+红黑树；HashTable的底层数据结构数组+链表 ConcurrentHashMap在1.7的时候，它使用的锁是分段式锁，也就是将数据分开进行加锁，这样做在高并发下就不会因为抢占一把锁而造成阻塞，从而提高并发的效率；在1.8的时候使用的是synchronized+CAS来实现锁；HashTable使用的加锁机制是synchronized，同一把锁，这样加锁确实可以保证线程的安全，但是效率非常低，当同时访问一个同步方法的时候，可能会造成阻塞或者轮询，效率大打折扣。 ConcurrentHashMap线程安全底层的具体实现 在1.7的时候，因为它的底层数据结构是分段数组+链表进行实现的，这个分段数组相当于HashMap里面的数组，里面包含链表，也就是HashEntry，每个分段数组守护着一个链表，当我们对HashEntry里面的数据进行修改的时候，就必须先获取分段锁。 在1.8的时候，因为它的底层数据结构变成了数组+链表+红黑树，取消了分段锁，使用的是synchronized和CAS对链表的头结点进行加锁，因为只要hash不冲突，那么就不会产生并发。","categories":[{"name":"Java","slug":"Java","permalink":"http://hwm156542114.github.io/categories/Java/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://hwm156542114.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"计算机网络的组成","slug":"计算机网络的组成","date":"2021-11-16T13:16:40.000Z","updated":"2021-12-08T02:08:13.007Z","comments":true,"path":"2021/11/16/ji-suan-ji-wang-luo-de-zu-cheng/","link":"","permalink":"http://hwm156542114.github.io/2021/11/16/ji-suan-ji-wang-luo-de-zu-cheng/","excerpt":"","text":"计算机网络的组成 计算机网络的分类 OSI七层模型 物理层 作用：该层为上层协议提供了一个传输数据的物理媒体 协议：EIA/TIA RS-232、EIA/TIA RS-449、V.35、RJ-45 数据链路层 作用：数据链路层在不可靠的物理介质上提供可靠的传输。该层的作用包括：物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。 协议：ARP、RARP、MAC、SDLC、HDLC、PPP、STP、帧中继等 网络层 作用：网络层负责对子网间的数据包进行路由选择。网络层还可以实现拥塞控制、网际互连等功能。 协议：ARP、RARP、IP、IPX、RIP、OSPF等 传输层 作用：传输层是第一个端到端，即主机到主机的层次。传输层负责将上层数据分段并提供端到端的、可靠的或不可靠的传输。此外，传输层还要处理端到端的差错控制和流量控制问题。 协议：TCP、UDP、SPX等。 会话层 作用：会话层管理主机之间的会话进程，即负责建立、管理、终止进程之间的会话。会话层还利用在数据中插入校验点来实现数据的同步。 协议：RPC、SQL、NFS 、X WINDOWS、ASP 表示层 作用：表示层对上层数据或信息进行变换以保证一个主机应用层信息可以被另一个主机的应用程序理解。表示层的数据转换包括数据的加密、压缩、格式转换等。 协议：ASCII、PICT、TIFF、JPEG、 MIDI、MPEG 应用层 作用：应用层为操作系统或网络应用程序提供访问网络服务的接口。 协议：万维网、HTTP、FTP、SMTP 你知道的协议TCP/UDP TCP/UDP是什么？TCP — Transmission Control Protocol，传输控制协议。UDP — User Data Protocol，用户数据报协议。 TCP/UDP的区别（优缺点）？ ​ (1)、TCP是面向连接的，UDP是面向无连接的。TCP在通信之前必须通过三次握手机制与对方建立连接，而UDP通信不必与对方建立连接，不管对方的状态就直接把数据发送给对方(2)、TCP连接过程耗时，UDP不耗时(3)、TCP连接过程中出现的延时增加了被攻击的可能，安全性不高，而UDP不需要连接，安全性较高(4)、TCP是可靠的，保证数据传输的正确性，不易丢包;UDP是不可靠的，易丢包(5)、tcp传输速率较慢，实时性差，udp传输速率较快。tcp建立连接需要耗时，并且tcp首部信息太多，每次传输的有用信息较少，实时性差。(6)、tcp是流模式，udp是数据包模式。tcp只要不超过缓冲区的大小就可以连续发送数据到缓冲区上，接收端只要缓冲区上有数据就可以读取，可以一次读取多个数据包，而udp一次只能读取一个数据包，数据包之间独立 TCP三次握手过程STEP 1：主机A通过向主机B发送一个含有同步序列号的标志位的数据段给主机B，向主机B请求建立连接,通过这个数据段，主机A告诉主机B两件事:我想要和你通信；你可以用哪个序列号作为起始数据段来回应我。STEP 2：主机B收到主机A的请求后,用一个带有确认应答(ACK)和同步序列号(SYN)标志位的数据段响应主机A，也告诉主机A两件事：我已经收到你的请求了，你可以传输数据了；你要用哪佧序列号作为起始数据段来回应我。STEP 3：主机A收到这个数据段后，再发送一个确认应答，确认已收到主机B的数据段：”我已收到回复，我现在要开始传输实际数据了。这样3次握手就完成了，主机A和主机B就可以传输数据了。 为什么需要三次握手？ 我们假设client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。 本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。 假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。 所以，采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。 TCP四次挥手过程当 TCP 数据段的传输结束时，双方都需要发送 FIN 段和 ACK 段来终止 TCP 会话。这个方式叫做四次挥手，详细过程如下： 主机 A 想要终止连接，发送序列号为 p 的段，FIN 置位，表示 FIN 管理段。 主机 B 收到主机 A 发送的 FIN 段后，发送 ACK 段，确认号为 p + 1 ，同时关闭连接。 主机 B 发送序列号为 q的段，FIN 置位，通知连接关闭。 主机 A 收到主机 B 发送的 FIN 段后，发送 ACK 段，确认号为 q + 1 ，同时关闭连接。TCP 连接至此结束。 注意此时需要注意的是，TCP建立连接要进行3次握手，而断开连接要进行4次。 名词解释ACK：TCP报头的控制位之一，对数据进行确认，确认由目的端发出，用它来告诉发送端这个序列号之前的数据段都收到了。比如，确认号为X，则表示前X-1个数据段都收到了，只有当ACK=1时，确认号才有效，当ACK=0时，确认号无效，这时会要求重传数据，保证数据的完整性。SYN：同步序列号，TCP建立连接时将这个位置1。FIN ：发送端完成发送任务位，当TCP完成数据传输需要断开时，提出断开连接的一方将这位置1。 为什么连接的时候是三次握手，关闭的时候却是四次握手？ 建立连接时因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。所以建立连接只需要三次握手。 由于TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议，TCP是全双工模式。这就意味着，关闭连接时，当Client端发出FIN报文段时，只是表示Client端告诉Server端数据已经发送完毕了。当Server端收到FIN报文并返回ACK报文段，表示它已经知道Client端没有数据发送了，但是Server端还是可以发送数据到Client端的，所以Server很可能并不会立即关闭SOCKET，直到Server端把数据也发送完毕。当Server端也发送了FIN报文段时，这个时候就表示Server端也没有数据要发送了，就会告诉Client端，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。 为什么要等待2MSL？ MSL：报文段最大生存时间，它是任何报文段被丢弃前在网络内的最长时间。有以下两个原因： 第一点：保证TCP协议的全双工连接能够可靠关闭：由于IP协议的不可靠性或者是其它网络原因，导致了Server端没有收到Client端的ACK报文，那么Server端就会在超时之后重新发送FIN，如果此时Client端的连接已经关闭处于CLOESD状态，那么重发的FIN就找不到对应的连接了，从而导致连接错乱，所以，Client端发送完最后的ACK不能直接进入CLOSED状态，而要保持TIME_WAIT，当再次收到FIN的收，能够保证对方收到ACK，最后正确关闭连接。 第二点：保证这次连接的重复数据段从网络中消失如果Client端发送最后的ACK直接进入CLOSED状态，然后又再向Server端发起一个新连接，这时不能保证新连接的与刚关闭的连接的端口号是不同的，也就是新连接和老连接的端口号可能一样了，那么就可能出现问题：如果前一次的连接某些数据滞留在网络中，这些延迟数据在建立新连接后到达Client端，由于新老连接的端口号和IP都一样，TCP协议就认为延迟数据是属于新连接的，新连接就会接收到脏数据，这样就会导致数据包混乱。所以TCP连接需要在TIME_WAIT状态等待2倍MSL，才能保证本次连接的所有数据在网络中消失。 TCP可靠性的四大手段(1)、顺序编号：tcp在传输文件的时候，会将文件拆分为多个tcp数据包，每个装满的数据包大小大约在1k左右，tcp协议为保证可靠传输，会将这些数据包顺序编号 ​ (2)、确认机制：当数据包成功的被发送方发送给接收方，接收方会根据tcp协议反馈给发送方一个成功接收的ACK信号，信号中包含了当前包的序号 (3)、超时重传：当发送方发送数据包给接收方时，会为每一个数据包设置一个定时器，当在设定的时间内，发送方仍没有收到接收方的ACK信号，会再次发送该数据包，直到收到接收方的ACK信号或者连接已断开 ​ (4)、校验信息：tcp首部校验信息较多，udp首部校验信息较少。 流控制 接收端处于高负荷状态时，可能无法处理接收的数据，并丢弃数据，就会触发重发机制，导致网络流量无端浪费。 为了防止这种情况，TCP 提供一种机制可以让发送端根据接收端的实际接收能力控制发送的数据量，这就是流控制。它的具体操作是，接收端主机通知发送端主机自己可以接收数据的大小，于是发送端会发送不超过这个限度的数据。这个限度的大小就是窗口大小。 TCP 头部中有一个字段用来通知窗口大小。接收主机将缓冲区大小放入这个字段发送给接收端。当接收端的缓存不足或处理能力有限时，窗口大小的值会降低一半，从而控制数据发送量。也就是说，发送端主机会根据接收端主机的指示，对发送数据的量进行控制，也就形成了一个完整的 TCP 流控制。 若接收端要求窗口大小为 0 ，表示接收端已经接收全部数据，或者接收端应用程序没有时间读取数据，要求暂停发送。 如果窗口更新的报文丢失，可能会导致无法继续通信。为避免这个问题，发送端主机会时不时的发送一个叫窗口探测的数据段，此数据段仅含一个字节以获取最新的窗口大小信息。 拥塞控制 有了 TCP 的窗口控制，收发主机之间不再以一个数据段为单位发送确认应答，也能够连续发送大量数据包。在网络出现拥堵时，如果突然发送一个较大量的数据，有可能会导致整个网络瘫痪。 为了防止这个问题出现，在通信开始时，就会通过一个叫慢启动的算法得出的数值，对发送数据量进行控制。 为了在发送端调节发送数据的量，需要使用拥塞窗口。在慢启动时，将拥塞窗口的大小设置为 1 MSS 发送数据，之后每收到一次确认应答（ ACK ）,拥塞窗口的值就加 1 。在发送数据包时，将拥塞窗口的大小与接收端主机通知的窗口大小做比较，选择它们当中较小的值发送数据。这样可以有效减少通信开始时连续发包导致网络拥堵，还可以避免网络拥塞的发生。 TCP粘包，拆包问题我们都知道TCP属于传输层的协议，传输层除了有TCP协议外还有UDP协议。那么UDP是否会发生粘包或拆包的现象呢？答案是不会。UDP是基于报文发送的，从UDP的帧结构可以看出，在UDP首部采用了16bit来指示UDP数据报文的长度，因此在应用层能很好的将不同的数据报文区分开，从而避免粘包和拆包的问题。而TCP是基于字节流的，虽然应用层和TCP传输层之间的数据交互是大小不等的数据块，但是TCP把这些数据块仅仅看成一连串无结构的字节流，没有边界；另外从TCP的帧结构也可以看出，在TCP的首部没有表示数据长度的字段，基于上面两点，在使用TCP传输数据时，才有粘包或者拆包现象发生的可能。 粘包、拆包发生原因 1、要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包。 2、待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。 3、要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包。 4、接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。 粘包、拆包解决办法 通过以上分析，我们清楚了粘包或拆包发生的原因，那么如何解决这个问题呢？解决问题的关键在于如何给每个数据包添加边界信息，常用的方法有如下几个： 1、发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。 2、发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。 3、可以在数据包之间设置边界，如添加特殊符号，这样，接收端通过这个边界就可以将不同的数据包拆分开。 如何区分通信的区别？网络通信中通常采用 5 个信息来识别一个通信。它们是源 IP 地址、目的 IP 地址、协议号、源端口号、目的端口号。只要其中一项不同，就会认为是不同的通信。 ARP的工作原理如下（RARP工作原理与之相反） 首先，每台主机都会在自己的ARP缓冲区 (ARP Cache)中建立一个 ARP列表，以表示IP地址和MAC地址的对应关系。 当源主机需要将一个数据包要发送到目的主机时，会首先检查自己 ARP列表中是否存在该 IP地址对应的MAC地址，如果有﹐就直接将数据包发送到这个MAC地址；如果没有，就向本地网段发起一个ARP请求的广播包，查询此目的主机对应的MAC地址。此ARP请求数据包里包括源主机的IP地址、硬件地址、以及目的主机的IP地址。 网络中所有的主机收到这个ARP请求后，会检查数据包中的目的IP是否和自己的IP地址一致。如果不相同就忽略此数据包；如果相同，该主机首先将发送端的MAC地址和IP地址添加到自己的ARP列表中，如果ARP表中已经存在该IP的信息，则将其覆盖，然后给源主机发送一个 ARP响应数据包，告诉对方自己是它需要查找的MAC地址； 源主机收到这个ARP响应数据包后，将得到的目的主机的IP地址和MAC地址添加到自己的ARP列表中，并利用此信息开始数据的传输。如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。 在浏览器输入URL回车之后发生了什么？http1.0、http1.1、http2.0的区别","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://hwm156542114.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络基本概念","slug":"计算机网络基本概念","permalink":"http://hwm156542114.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"}]}],"categories":[{"name":"redis","slug":"redis","permalink":"http://hwm156542114.github.io/categories/redis/"},{"name":"算法","slug":"算法","permalink":"http://hwm156542114.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"Java","slug":"Java","permalink":"http://hwm156542114.github.io/categories/Java/"},{"name":"学Java","slug":"学Java","permalink":"http://hwm156542114.github.io/categories/%E5%AD%A6Java/"},{"name":"MySQL","slug":"MySQL","permalink":"http://hwm156542114.github.io/categories/MySQL/"},{"name":"设计模式","slug":"设计模式","permalink":"http://hwm156542114.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"项目","slug":"项目","permalink":"http://hwm156542114.github.io/categories/%E9%A1%B9%E7%9B%AE/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://hwm156542114.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"redis简单介绍","slug":"redis简单介绍","permalink":"http://hwm156542114.github.io/tags/redis%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"},{"name":"每日算法","slug":"每日算法","permalink":"http://hwm156542114.github.io/tags/%E6%AF%8F%E6%97%A5%E7%AE%97%E6%B3%95/"},{"name":"AQS源码","slug":"AQS源码","permalink":"http://hwm156542114.github.io/tags/AQS%E6%BA%90%E7%A0%81/"},{"name":"spring基础","slug":"spring基础","permalink":"http://hwm156542114.github.io/tags/spring%E5%9F%BA%E7%A1%80/"},{"name":"springcloud","slug":"springcloud","permalink":"http://hwm156542114.github.io/tags/springcloud/"},{"name":"ArrayList源码","slug":"ArrayList源码","permalink":"http://hwm156542114.github.io/tags/ArrayList%E6%BA%90%E7%A0%81/"},{"name":"spring源码解读","slug":"spring源码解读","permalink":"http://hwm156542114.github.io/tags/spring%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"},{"name":"MySQL优化","slug":"MySQL优化","permalink":"http://hwm156542114.github.io/tags/MySQL%E4%BC%98%E5%8C%96/"},{"name":"单例模式","slug":"单例模式","permalink":"http://hwm156542114.github.io/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"name":"电商项目","slug":"电商项目","permalink":"http://hwm156542114.github.io/tags/%E7%94%B5%E5%95%86%E9%A1%B9%E7%9B%AE/"},{"name":"面试","slug":"面试","permalink":"http://hwm156542114.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"计算机网络基本概念","slug":"计算机网络基本概念","permalink":"http://hwm156542114.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"}]}